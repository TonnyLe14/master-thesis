{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from xgboost import XGBRegressor\n",
    "from tensorflow import keras\n",
    "from scipy.interpolate import interpn\n",
    "from tqdm import tqdm\n",
    "from joblib import dump, load\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 25 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/openpyxl/worksheet/_read_only.py:79: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  for idx, row in parser.parse():\n"
     ]
    }
   ],
   "source": [
    "from load_data import process_steel_data\n",
    "\n",
    "full_path = 'data/'\n",
    "path = 'data/MDC_Data_Descriptions_MeCoMeP-r-value.xlsx'\n",
    "correlation_rate = 0.2\n",
    "dvl_line = 1\n",
    "\n",
    "df = process_steel_data(full_path, path, correlation_rate, dvl_line, model_output=False)\n",
    "df = pd.get_dummies(df, columns=['steel_family'], prefix='steel').drop(['steel_grade'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(df, binary_prefix='steel_'):\n",
    "\n",
    "    # Identify binary columns\n",
    "    binary_columns = [col for col in df.columns if col.startswith(binary_prefix)]\n",
    "    \n",
    "    # Identify columns to scale (non-binary columns)\n",
    "    columns_to_scale = [col for col in df.columns if col not in binary_columns + ['r_value']]\n",
    "    \n",
    "    # Scale numerical features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(df[columns_to_scale])\n",
    "    \n",
    "    # Create new dataframe with scaled data\n",
    "    scaled_df = pd.DataFrame(scaled_data, columns=columns_to_scale)\n",
    "    \n",
    "    # Add back binary columns\n",
    "    for col in binary_columns:\n",
    "        scaled_df[col] = df[col].values\n",
    "    \n",
    "    # Add target variable if present\n",
    "    if 'r_value' in df.columns:\n",
    "        scaled_df['r_value'] = df['r_value'].values\n",
    "    \n",
    "    return scaled_df, scaler\n",
    "\n",
    "def train_model_with_cv_gridsearch(df, model, param_grid=None, n_splits=5, random_state=42, use_grid_search=True, model_params=None):\n",
    "    \"\"\"\n",
    "    Train a model with optional grid search and cross-validation\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Input dataframe\n",
    "    model : estimator object\n",
    "        Machine learning model to train\n",
    "    param_grid : dict, optional\n",
    "        Parameter grid for grid search (used if use_grid_search=True)\n",
    "    n_splits : int, optional\n",
    "        Number of cross-validation splits (default: 5)\n",
    "    random_state : int, optional\n",
    "        Random state for reproducibility (default: 42)\n",
    "    use_grid_search : bool, optional\n",
    "        Whether to perform grid search (default: True)\n",
    "    model_params : dict, optional\n",
    "        Direct model parameters to use if use_grid_search=False\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict containing model results and performance metrics including tol90\n",
    "    \"\"\"\n",
    "    # Prepare X and y\n",
    "    X = df.drop(['r_value'], axis=1)\n",
    "    y = df['r_value']\n",
    "    \n",
    "    # Initialize cross-validation\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # Initialize metrics storage\n",
    "    cv_scores = {\n",
    "        'mae': [],\n",
    "        'mse': [],\n",
    "        'r2': [],\n",
    "        'tol90': []  # Add tol90 metric\n",
    "    }\n",
    "    \n",
    "    # Determine model parameters\n",
    "    if use_grid_search:\n",
    "        if param_grid is None:\n",
    "            raise ValueError(\"param_grid must be provided when use_grid_search is True\")\n",
    "        \n",
    "        # Initialize GridSearchCV\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=model,\n",
    "            param_grid=param_grid,\n",
    "            cv=n_splits,\n",
    "            scoring='neg_mean_absolute_error',\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Fit GridSearchCV\n",
    "        print(\"Performing GridSearch...\")\n",
    "        grid_search.fit(X, y)\n",
    "        print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
    "        best_model = grid_search.best_estimator_\n",
    "    else:\n",
    "        # Use directly specified parameters or default model\n",
    "        if model_params:\n",
    "            best_model = type(model)(**model_params)\n",
    "        else:\n",
    "            best_model = model\n",
    "        \n",
    "        grid_search = None\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    print(\"\\nPerforming cross-validation...\")\n",
    "    pbar = tqdm(enumerate(kf.split(X), 1),\n",
    "                total=n_splits,\n",
    "                desc=\"Cross-validation\",\n",
    "                leave=True)\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in pbar:\n",
    "        # Split data\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        # Train model\n",
    "        best_model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = best_model.predict(X_val)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "        mse = mean_squared_error(y_val, y_pred)\n",
    "        r2 = r2_score(y_val, y_pred)\n",
    "        \n",
    "        # Calculate tol90 (90th percentile of absolute errors)\n",
    "        abs_errors = np.abs(y_val - y_pred)\n",
    "        tol90 = np.percentile(abs_errors, 90)\n",
    "        \n",
    "        cv_scores['mae'].append(mae)\n",
    "        cv_scores['mse'].append(mse)\n",
    "        cv_scores['r2'].append(r2)\n",
    "        cv_scores['tol90'].append(tol90)\n",
    "        \n",
    "        # Update progress bar description\n",
    "        pbar.set_description(\n",
    "            f\"Fold {fold} - MAE: {mae:.4f}, MSE: {mse:.4f}, R2: {r2:.4f}, TOL90: {tol90:.4f}\"\n",
    "        )\n",
    "    \n",
    "    # Prepare results\n",
    "    results = {\n",
    "        'model': best_model,\n",
    "        'best_params': grid_search.best_params_ if use_grid_search else model_params or {},\n",
    "        'avg_mae': np.mean(cv_scores['mae']),\n",
    "        'std_mae': np.std(cv_scores['mae']),\n",
    "        'avg_mse': np.mean(cv_scores['mse']),\n",
    "        'std_mse': np.std(cv_scores['mse']),\n",
    "        'avg_r2': np.mean(cv_scores['r2']),\n",
    "        'std_r2': np.std(cv_scores['r2']),\n",
    "        'avg_tol90': np.mean(cv_scores['tol90']),  # Add average tol90\n",
    "        'std_tol90': np.std(cv_scores['tol90']),   # Add std of tol90\n",
    "        'cv_scores': cv_scores,\n",
    "        'grid_search_results': grid_search.cv_results_ if use_grid_search else None\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def report_cv_results(results):\n",
    "    print(\"\\nCross-Validation Results:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Best Parameters: {results['best_params']}\")\n",
    "    print(f\"Average MAE: {results['avg_mae']:.4f} ± {results['std_mae']:.4f}\")\n",
    "    print(f\"Average MSE: {results['avg_mse']:.4f} ± {results['std_mse']:.4f}\")\n",
    "    print(f\"Average R2: {results['avg_r2']:.4f} ± {results['std_r2']:.4f}\")\n",
    "    print(f\"Average TOL90: {results['avg_tol90']:.4f} ± {results['std_tol90']:.4f}\")\n",
    "\n",
    "def report_cv_results(results):\n",
    "    print(\"\\nCross-Validation Results:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Best Parameters: {results['best_params']}\")\n",
    "    print(f\"Average MAE: {results['avg_mae']:.4f} ± {results['std_mae']:.4f}\")\n",
    "    print(f\"Average MSE: {results['avg_mse']:.4f} ± {results['std_mse']:.4f}\")\n",
    "    print(f\"Average R2: {results['avg_r2']:.4f} ± {results['std_r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_scaled_df, scaler = scale_data(train_df)\n",
    "binary_columns = [col for col in test_df.columns if col.startswith('steel_')]\n",
    "columns_to_scale = [col for col in test_df.columns if col not in binary_columns + ['r_value']]\n",
    "scaled_test_data = scaler.transform(test_df[columns_to_scale])\n",
    "test_scaled_df = pd.DataFrame(scaled_test_data, columns=columns_to_scale)\n",
    "for col in binary_columns:\n",
    "    test_scaled_df[col] = test_df[col].values\n",
    "if 'r_value' in test_df.columns:\n",
    "    test_scaled_df['r_value'] = test_df['r_value'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing GridSearch...\n",
      "\n",
      "Best parameters: {'n_estimators': 350}\n",
      "\n",
      "Performing cross-validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 - MAE: 0.0941, MSE: 0.0180, R2: 0.9468, TOL90: 0.2190: 100%|██████████| 5/5 [04:18<00:00, 51.79s/it]\n"
     ]
    }
   ],
   "source": [
    "rfr = RandomForestRegressor(random_state=42)\n",
    "rfr_param_grid = {\n",
    "    'n_estimators': [350]\n",
    "}\n",
    "\n",
    "rfr_results = train_model_with_cv_gridsearch(\n",
    "    df=train_scaled_df,\n",
    "    model=rfr,\n",
    "    param_grid=rfr_param_grid,\n",
    "    n_splits=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing GridSearch...\n",
      "\n",
      "Best parameters: {'eta': 0.1, 'lambda': 1, 'max_depth': 8}\n",
      "\n",
      "Performing cross-validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 - MAE: 0.0969, MSE: 0.0189, R2: 0.9443, TOL90: 0.2258: 100%|██████████| 5/5 [00:02<00:00,  1.81it/s]\n"
     ]
    }
   ],
   "source": [
    "xgb_model = XGBRegressor(random_state=42)\n",
    "\n",
    "xgb_param_grid = {\n",
    "    'eta': [0.01, 0.05, 0.1, 0.2, 0.3, 0.4],\n",
    "    'lambda': [0, 0.01, 0.1, 1, 10, 50],\n",
    "    'max_depth': [3, 4, 5, 6, 7, 8]\n",
    "}\n",
    "\n",
    "xgb_results = train_model_with_cv_gridsearch(\n",
    "    df=train_scaled_df,\n",
    "    model=xgb_model,\n",
    "    param_grid=xgb_param_grid,\n",
    "    n_splits=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing cross-validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cross-validation:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 - MAE: 0.1037, MSE: 0.0204, R2: 0.9399, TOL90: 0.2299: 100%|██████████| 5/5 [28:29<00:00, 341.91s/it]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.gaussian_process.kernels import Matern, WhiteKernel\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "\n",
    "results_without_grid = train_model_with_cv_gridsearch(\n",
    "    df=train_scaled_df, \n",
    "    model=GaussianProcessRegressor(), \n",
    "    use_grid_search=False,\n",
    "    model_params={'kernel': 1**2 * Matern(length_scale=1, nu=1.5) + WhiteKernel(noise_level=1)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing GridSearch...\n",
      "\n",
      "Best parameters: {'leaf_size': 20, 'n_neighbors': 9, 'weights': 'distance'}\n",
      "\n",
      "Performing cross-validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 - MAE: 0.1075, MSE: 0.0224, R2: 0.9338, TOL90: 0.2421: 100%|██████████| 5/5 [00:00<00:00, 12.02it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "knn_model = KNeighborsRegressor()\n",
    "\n",
    "knn_param_grid = {\n",
    "    'n_neighbors': list(range(2, 15)),\n",
    "    'leaf_size': [20, 30, 40, 50],\n",
    "    'weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "knn_results = train_model_with_cv_gridsearch(\n",
    "    df=train_scaled_df,\n",
    "    model=knn_model,\n",
    "    param_grid=knn_param_grid\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_saved_model_architecture(saved_model_path, df, target_column='r_value', n_splits=5, \n",
    "                               epochs=100, batch_size=32, random_state=42):\n",
    "\n",
    "    # Load saved model to get architecture and parameters\n",
    "    base_model = tf.keras.models.load_model(saved_model_path)\n",
    "    \n",
    "    # Get learning rate from saved model and convert to Python float\n",
    "    learning_rate = float(base_model.optimizer.learning_rate.numpy())\n",
    "    \n",
    "    # Prepare data\n",
    "    X = df.drop([target_column], axis=1)\n",
    "    y = df[target_column]\n",
    "    \n",
    "    # Initialize KFold\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # Storage for CV metrics\n",
    "    cv_scores = {\n",
    "        'mae': [],\n",
    "        'mse': [],\n",
    "        'r2': [],\n",
    "        'tol90': []  # Add tol90 metric\n",
    "    }\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    print(\"\\nPerforming cross-validation...\")\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X), 1):\n",
    "        print(f\"\\nFold {fold}/{n_splits}\")\n",
    "        \n",
    "        # Clear previous model from memory\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        # Convert to float32\n",
    "        X_train = np.array(X_train, dtype=np.float32)\n",
    "        y_train = np.array(y_train, dtype=np.float32)\n",
    "        X_val = np.array(X_val, dtype=np.float32)\n",
    "        y_val = np.array(y_val, dtype=np.float32)\n",
    "        \n",
    "        # Create new model with same architecture\n",
    "        model = tf.keras.models.clone_model(base_model)\n",
    "        \n",
    "        # Compile with same optimizer type and learning rate\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='mean_absolute_error',\n",
    "            metrics=['mae', 'mse']\n",
    "        )\n",
    "        \n",
    "        # Early stopping\n",
    "        early_stopping = keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            min_delta=1e-4\n",
    "        )\n",
    "        \n",
    "        # Create TF datasets\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "        train_dataset = (train_dataset\n",
    "            .batch(batch_size, drop_remainder=True)\n",
    "            .repeat())\n",
    "        \n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "        val_dataset = val_dataset.batch(batch_size)\n",
    "        \n",
    "        steps_per_epoch = len(X_train) // batch_size\n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            train_dataset,\n",
    "            validation_data=val_dataset,\n",
    "            epochs=epochs,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_val, verbose=0)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "        mse = mean_squared_error(y_val, y_pred)\n",
    "        r2 = r2_score(y_val, y_pred)\n",
    "        \n",
    "        # Calculate tol90 (90th percentile of absolute errors)\n",
    "        abs_errors = np.abs(y_val - y_pred)\n",
    "        tol90 = np.percentile(abs_errors, 90)\n",
    "        \n",
    "        cv_scores['mae'].append(mae)\n",
    "        cv_scores['mse'].append(mse)\n",
    "        cv_scores['r2'].append(r2)\n",
    "        cv_scores['tol90'].append(tol90)\n",
    "        \n",
    "        print(f\"Fold {fold} - MAE: {mae:.4f}, MSE: {mse:.4f}, R2: {r2:.4f}, TOL90: {tol90:.4f}\")\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    results = {\n",
    "        'avg_mae': np.mean(cv_scores['mae']),\n",
    "        'std_mae': np.std(cv_scores['mae']),\n",
    "        'avg_mse': np.mean(cv_scores['mse']),\n",
    "        'std_mse': np.std(cv_scores['mse']),\n",
    "        'avg_r2': np.mean(cv_scores['r2']),\n",
    "        'std_r2': np.std(cv_scores['r2']),\n",
    "        'avg_tol90': np.mean(cv_scores['tol90']),\n",
    "        'std_tol90': np.std(cv_scores['tol90']),\n",
    "        'cv_scores': cv_scores\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteelPropertiesANN:\n",
    "    def __init__(self, input_dim, target_column):\n",
    "        self.input_dim = input_dim\n",
    "        self.target_column = target_column\n",
    "        self.best_model = None\n",
    "        self.best_params = None\n",
    "        self.best_score = float('inf')\n",
    "\n",
    "    def build_model(self, config):\n",
    "        hidden_layers = config['layers']\n",
    "        learning_rate = config['learning_rate']\n",
    "        l2_strength = config['l2_regularization']\n",
    "        \n",
    "        model = keras.Sequential()\n",
    "        model.add(keras.layers.Input(shape=(self.input_dim,)))\n",
    "        \n",
    "        for units, activation in hidden_layers:\n",
    "            model.add(keras.layers.Dense(\n",
    "                units=units,\n",
    "                activation=activation,\n",
    "                kernel_regularizer=keras.regularizers.l2(l2_strength)\n",
    "            ))\n",
    "        \n",
    "        model.add(keras.layers.Dense(1))\n",
    "        \n",
    "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "            initial_learning_rate=learning_rate,\n",
    "            decay_steps=100,\n",
    "            decay_rate=0.9,\n",
    "            staircase=True\n",
    "        )\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=lr_schedule, clipnorm=1.0),\n",
    "            loss='mean_absolute_error',\n",
    "            metrics=['mae', 'mse']\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def generate_grid_configs(self, \n",
    "        layer_options=[(64, 'relu'), (128, 'relu'), (256, 'relu')],\n",
    "        layer_depths=[2, 3, 4],\n",
    "        learning_rates=[1e-2, 1e-3, 1e-4],\n",
    "        l2_regularization=[1e-3, 1e-4, 1e-5],\n",
    "        batch_sizes=[16, 32, 64]\n",
    "    ):\n",
    "        from itertools import product\n",
    "        grid_configs = []\n",
    "        \n",
    "        for depth in layer_depths:\n",
    "            for lr in learning_rates:\n",
    "                for l2_reg in l2_regularization:\n",
    "                    for batch_size in batch_sizes:\n",
    "                        layer_combinations = list(product(layer_options, repeat=depth))\n",
    "                        for layers in layer_combinations:\n",
    "                            config = {\n",
    "                                'layers': layers,\n",
    "                                'learning_rate': lr,\n",
    "                                'l2_regularization': l2_reg,\n",
    "                                'batch_size': batch_size\n",
    "                            }\n",
    "                            grid_configs.append(config)\n",
    "        \n",
    "        return grid_configs\n",
    "\n",
    "    def grid_search(self, train_scaled_df, grid_configs=None, epochs=100, max_configs=None):\n",
    "        # Split training data into training and validation sets\n",
    "        train_data, val_data = train_test_split(train_scaled_df, test_size=0.2, random_state=42)\n",
    "    \n",
    "        if grid_configs is None:\n",
    "            grid_configs = self.generate_grid_configs()\n",
    "        \n",
    "        X_train = train_data.drop([self.target_column], axis=1)\n",
    "        y_train = train_data[self.target_column]\n",
    "        X_val = val_data.drop([self.target_column], axis=1)\n",
    "        y_val = val_data[self.target_column]\n",
    "        \n",
    "        if max_configs:\n",
    "            grid_configs = grid_configs[:max_configs]\n",
    "        \n",
    "        results = []\n",
    "        for config in tqdm(grid_configs, desc=\"Training models\"):\n",
    "            tf.keras.backend.clear_session()\n",
    "            model = self.build_model(config)\n",
    "            batch_size = min(config['batch_size'], len(X_train))\n",
    "            \n",
    "            early_stopping = keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss', \n",
    "                patience=10, \n",
    "                restore_best_weights=True,\n",
    "                min_delta=1e-4\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                X_train = np.array(X_train, dtype=np.float32)\n",
    "                y_train = np.array(y_train, dtype=np.float32)\n",
    "                X_val = np.array(X_val, dtype=np.float32)\n",
    "                y_val = np.array(y_val, dtype=np.float32)\n",
    "\n",
    "                train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "                train_dataset = (train_dataset\n",
    "                    .batch(batch_size, drop_remainder=True)\n",
    "                    .repeat())\n",
    "                \n",
    "                val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "                val_dataset = val_dataset.batch(batch_size)\n",
    "                \n",
    "                steps_per_epoch = len(X_train) // batch_size\n",
    "                \n",
    "                history = model.fit(\n",
    "                    train_dataset,\n",
    "                    validation_data=val_dataset,\n",
    "                    epochs=epochs,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    callbacks=[early_stopping],\n",
    "                    verbose=0\n",
    "                )\n",
    "                \n",
    "                val_loss = model.evaluate(X_val, y_val, verbose=0)[0]\n",
    "                \n",
    "                result_entry = config.copy()\n",
    "                result_entry.update({'val_loss': val_loss})\n",
    "                results.append(result_entry)\n",
    "                \n",
    "                if val_loss < self.best_score:\n",
    "                    self.best_score = val_loss\n",
    "                    self.best_model = model\n",
    "                    self.best_params = config\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error with config {config}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return self.best_model, self.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.3760 - mae: 0.3589 - mse: 0.3522 - val_loss: 0.1554 - val_mae: 0.1387 - val_mse: 0.0360\n",
      "Epoch 2/100\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1555 - mae: 0.1389 - mse: 0.0340 - val_loss: 0.1448 - val_mae: 0.1286 - val_mse: 0.0308\n",
      "Epoch 3/100\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 880us/step - loss: 0.1407 - mae: 0.1246 - mse: 0.0277 - val_loss: 0.1367 - val_mae: 0.1210 - val_mse: 0.0282\n",
      "Epoch 4/100\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 821us/step - loss: 0.1380 - mae: 0.1223 - mse: 0.0269 - val_loss: 0.1348 - val_mae: 0.1195 - val_mse: 0.0270\n",
      "Epoch 5/100\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 819us/step - loss: 0.1311 - mae: 0.1159 - mse: 0.0243 - val_loss: 0.1326 - val_mae: 0.1176 - val_mse: 0.0261\n",
      "Epoch 6/100\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 820us/step - loss: 0.1263 - mae: 0.1114 - mse: 0.0232 - val_loss: 0.1306 - val_mae: 0.1159 - val_mse: 0.0255\n",
      "Epoch 7/100\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1258 - mae: 0.1112 - mse: 0.0230 - val_loss: 0.1346 - val_mae: 0.1202 - val_mse: 0.0274\n",
      "Epoch 8/100\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 840us/step - loss: 0.1232 - mae: 0.1089 - mse: 0.0222 - val_loss: 0.1384 - val_mae: 0.1244 - val_mse: 0.0294\n",
      "Epoch 9/100\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 812us/step - loss: 0.1215 - mae: 0.1076 - mse: 0.0222 - val_loss: 0.1306 - val_mae: 0.1168 - val_mse: 0.0264\n",
      "Epoch 10/100\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 821us/step - loss: 0.1208 - mae: 0.1071 - mse: 0.0220 - val_loss: 0.1309 - val_mae: 0.1174 - val_mse: 0.0263\n",
      "Epoch 11/100\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 798us/step - loss: 0.1180 - mae: 0.1045 - mse: 0.0209 - val_loss: 0.1271 - val_mae: 0.1137 - val_mse: 0.0249\n",
      "Epoch 12/100\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 810us/step - loss: 0.1157 - mae: 0.1024 - mse: 0.0205 - val_loss: 0.1283 - val_mae: 0.1152 - val_mse: 0.0248\n",
      "Epoch 13/100\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 817us/step - loss: 0.1157 - mae: 0.1026 - mse: 0.0203 - val_loss: 0.1257 - val_mae: 0.1128 - val_mse: 0.0243\n",
      "Epoch 14/100\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 827us/step - loss: 0.1128 - mae: 0.1000 - mse: 0.0195 - val_loss: 0.1234 - val_mae: 0.1106 - val_mse: 0.0235\n",
      "Epoch 15/100\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 807us/step - loss: 0.1117 - mae: 0.0990 - mse: 0.0193 - val_loss: 0.1329 - val_mae: 0.1204 - val_mse: 0.0270\n",
      "Epoch 16/100\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 807us/step - loss: 0.1135 - mae: 0.1009 - mse: 0.0200 - val_loss: 0.1325 - val_mae: 0.1201 - val_mse: 0.0264\n",
      "Epoch 17/100\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 799us/step - loss: 0.1123 - mae: 0.0999 - mse: 0.0194 - val_loss: 0.1316 - val_mae: 0.1193 - val_mse: 0.0261\n",
      "Epoch 18/100\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 929us/step - loss: 0.1112 - mae: 0.0989 - mse: 0.0193 - val_loss: 0.1284 - val_mae: 0.1162 - val_mse: 0.0249\n",
      "Epoch 19/100\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - loss: 0.1128 - mae: 0.1006 - mse: 0.0199 - val_loss: 0.1286 - val_mae: 0.1166 - val_mse: 0.0249\n",
      "Epoch 20/100\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 807us/step - loss: 0.1123 - mae: 0.1003 - mse: 0.0202 - val_loss: 0.1247 - val_mae: 0.1128 - val_mse: 0.0241\n",
      "Epoch 21/100\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 827us/step - loss: 0.1107 - mae: 0.0988 - mse: 0.0196 - val_loss: 0.1256 - val_mae: 0.1138 - val_mse: 0.0242\n",
      "Epoch 22/100\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 810us/step - loss: 0.1102 - mae: 0.0984 - mse: 0.0195 - val_loss: 0.1300 - val_mae: 0.1184 - val_mse: 0.0262\n",
      "Epoch 23/100\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 811us/step - loss: 0.1083 - mae: 0.0966 - mse: 0.0189 - val_loss: 0.1285 - val_mae: 0.1170 - val_mse: 0.0248\n",
      "Epoch 24/100\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 811us/step - loss: 0.1106 - mae: 0.0991 - mse: 0.0196 - val_loss: 0.1287 - val_mae: 0.1172 - val_mse: 0.0247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: [0.12338341772556305, 0.1106322780251503, 0.02351290173828602]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Assuming you have the best_params from grid search\n",
    "best_params = {\n",
    "    'layers': ((256, 'relu'), (64, 'relu')),\n",
    "    'learning_rate': 0.001,\n",
    "    'l2_regularization': 0.0001,\n",
    "    'batch_size': 64\n",
    "}\n",
    "\n",
    "train_data, val_data = train_test_split(train_scaled_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Instantiate the ANN class\n",
    "ann = SteelPropertiesANN(input_dim=train_scaled_df.drop(['r_value'], axis=1).shape[1], target_column='r_value')\n",
    "\n",
    "# Build the model using best_params\n",
    "best_model = ann.build_model(config=best_params)\n",
    "\n",
    "# Prepare data for training\n",
    "X_train = train_data.drop(['r_value'], axis=1)\n",
    "y_train = train_data['r_value']\n",
    "\n",
    "X_val = val_data.drop(['r_value'], axis=1)\n",
    "y_val = val_data['r_value']\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X_train = np.array(X_train, dtype=np.float32)\n",
    "y_train = np.array(y_train, dtype=np.float32)\n",
    "X_val = np.array(X_val, dtype=np.float32)\n",
    "y_val = np.array(y_val, dtype=np.float32)\n",
    "\n",
    "# Create datasets\n",
    "batch_size = best_params['batch_size']\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_size).repeat()\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(batch_size)\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=10, restore_best_weights=True, min_delta=1e-4\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "steps_per_epoch = len(X_train) // batch_size\n",
    "history = best_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=100,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model on validation data\n",
    "val_loss = best_model.evaluate(X_val, y_val, verbose=0)\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "\n",
    "# Save the model\n",
    "best_model.save('best_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing cross-validation...\n",
      "\n",
      "Fold 1/5\n",
      "Epoch 1/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 994us/step - loss: 0.2652 - mae: 0.2482 - mse: 0.1450 - val_loss: 0.1579 - val_mae: 0.1418 - val_mse: 0.0351\n",
      "Epoch 2/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 929us/step - loss: 0.1544 - mae: 0.1385 - mse: 0.0335 - val_loss: 0.1519 - val_mae: 0.1367 - val_mse: 0.0325\n",
      "Epoch 3/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 863us/step - loss: 0.1411 - mae: 0.1262 - mse: 0.0286 - val_loss: 0.1424 - val_mae: 0.1281 - val_mse: 0.0309\n",
      "Epoch 4/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 0.1381 - mae: 0.1240 - mse: 0.0278 - val_loss: 0.1316 - val_mae: 0.1180 - val_mse: 0.0257\n",
      "Epoch 5/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 0.1300 - mae: 0.1166 - mse: 0.0251 - val_loss: 0.1336 - val_mae: 0.1206 - val_mse: 0.0255\n",
      "Epoch 6/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 0.1297 - mae: 0.1168 - mse: 0.0250 - val_loss: 0.1412 - val_mae: 0.1287 - val_mse: 0.0301\n",
      "Epoch 7/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 715us/step - loss: 0.1260 - mae: 0.1136 - mse: 0.0243 - val_loss: 0.1376 - val_mae: 0.1257 - val_mse: 0.0289\n",
      "Epoch 8/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - loss: 0.1259 - mae: 0.1140 - mse: 0.0244 - val_loss: 0.1380 - val_mae: 0.1264 - val_mse: 0.0278\n",
      "Epoch 9/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step - loss: 0.1217 - mae: 0.1103 - mse: 0.0232 - val_loss: 0.1272 - val_mae: 0.1160 - val_mse: 0.0247\n",
      "Epoch 10/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 0.1205 - mae: 0.1094 - mse: 0.0229 - val_loss: 0.1298 - val_mae: 0.1189 - val_mse: 0.0251\n",
      "Epoch 11/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 0.1183 - mae: 0.1075 - mse: 0.0224 - val_loss: 0.1301 - val_mae: 0.1195 - val_mse: 0.0261\n",
      "Epoch 12/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - loss: 0.1174 - mae: 0.1069 - mse: 0.0223 - val_loss: 0.1363 - val_mae: 0.1260 - val_mse: 0.0288\n",
      "Epoch 13/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 0.1174 - mae: 0.1072 - mse: 0.0225 - val_loss: 0.1336 - val_mae: 0.1236 - val_mse: 0.0276\n",
      "Epoch 14/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 0.1169 - mae: 0.1069 - mse: 0.0222 - val_loss: 0.1329 - val_mae: 0.1231 - val_mse: 0.0277\n",
      "Epoch 15/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 760us/step - loss: 0.1153 - mae: 0.1056 - mse: 0.0220 - val_loss: 0.1250 - val_mae: 0.1154 - val_mse: 0.0247\n",
      "Epoch 16/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 0.1161 - mae: 0.1065 - mse: 0.0223 - val_loss: 0.1275 - val_mae: 0.1181 - val_mse: 0.0247\n",
      "Epoch 17/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 0.1180 - mae: 0.1086 - mse: 0.0230 - val_loss: 0.1296 - val_mae: 0.1203 - val_mse: 0.0260\n",
      "Epoch 18/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 867us/step - loss: 0.1135 - mae: 0.1043 - mse: 0.0215 - val_loss: 0.1187 - val_mae: 0.1096 - val_mse: 0.0224\n",
      "Epoch 19/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 0.1123 - mae: 0.1031 - mse: 0.0211 - val_loss: 0.1302 - val_mae: 0.1212 - val_mse: 0.0268\n",
      "Epoch 20/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 0.1121 - mae: 0.1031 - mse: 0.0216 - val_loss: 0.1267 - val_mae: 0.1178 - val_mse: 0.0257\n",
      "Epoch 21/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 0.1132 - mae: 0.1043 - mse: 0.0218 - val_loss: 0.1255 - val_mae: 0.1167 - val_mse: 0.0249\n",
      "Epoch 22/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 0.1120 - mae: 0.1033 - mse: 0.0213 - val_loss: 0.1211 - val_mae: 0.1124 - val_mse: 0.0237\n",
      "Epoch 23/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 0.1117 - mae: 0.1030 - mse: 0.0213 - val_loss: 0.1246 - val_mae: 0.1159 - val_mse: 0.0251\n",
      "Epoch 24/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 0.1108 - mae: 0.1022 - mse: 0.0209 - val_loss: 0.1276 - val_mae: 0.1190 - val_mse: 0.0264\n",
      "Epoch 25/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 0.1095 - mae: 0.1010 - mse: 0.0207 - val_loss: 0.1248 - val_mae: 0.1163 - val_mse: 0.0253\n",
      "Epoch 26/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 0.1106 - mae: 0.1022 - mse: 0.0210 - val_loss: 0.1224 - val_mae: 0.1139 - val_mse: 0.0246\n",
      "Epoch 27/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 0.1082 - mae: 0.0997 - mse: 0.0203 - val_loss: 0.1228 - val_mae: 0.1143 - val_mse: 0.0245\n",
      "Epoch 28/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 0.1074 - mae: 0.0990 - mse: 0.0200 - val_loss: 0.1234 - val_mae: 0.1151 - val_mse: 0.0249\n",
      "Fold 1 - MAE: 0.1096, MSE: 0.0224, R2: 0.9324, TOL90: 1.3217\n",
      "\n",
      "Fold 2/5\n",
      "Epoch 1/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 983us/step - loss: 0.2860 - mae: 0.2690 - mse: 0.1806 - val_loss: 0.1508 - val_mae: 0.1345 - val_mse: 0.0317\n",
      "Epoch 2/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710us/step - loss: 0.1550 - mae: 0.1389 - mse: 0.0344 - val_loss: 0.1400 - val_mae: 0.1245 - val_mse: 0.0280\n",
      "Epoch 3/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 727us/step - loss: 0.1458 - mae: 0.1305 - mse: 0.0302 - val_loss: 0.1312 - val_mae: 0.1166 - val_mse: 0.0251\n",
      "Epoch 4/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 822us/step - loss: 0.1379 - mae: 0.1234 - mse: 0.0277 - val_loss: 0.1295 - val_mae: 0.1155 - val_mse: 0.0239\n",
      "Epoch 5/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 0.1320 - mae: 0.1182 - mse: 0.0257 - val_loss: 0.1290 - val_mae: 0.1157 - val_mse: 0.0237\n",
      "Epoch 6/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 0.1287 - mae: 0.1155 - mse: 0.0246 - val_loss: 0.1257 - val_mae: 0.1129 - val_mse: 0.0228\n",
      "Epoch 7/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 0.1244 - mae: 0.1117 - mse: 0.0237 - val_loss: 0.1221 - val_mae: 0.1098 - val_mse: 0.0219\n",
      "Epoch 8/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 0.1232 - mae: 0.1110 - mse: 0.0235 - val_loss: 0.1210 - val_mae: 0.1091 - val_mse: 0.0222\n",
      "Epoch 9/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 0.1219 - mae: 0.1101 - mse: 0.0233 - val_loss: 0.1202 - val_mae: 0.1086 - val_mse: 0.0221\n",
      "Epoch 10/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 727us/step - loss: 0.1199 - mae: 0.1085 - mse: 0.0229 - val_loss: 0.1218 - val_mae: 0.1106 - val_mse: 0.0231\n",
      "Epoch 11/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 0.1194 - mae: 0.1083 - mse: 0.0228 - val_loss: 0.1232 - val_mae: 0.1123 - val_mse: 0.0240\n",
      "Epoch 12/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 0.1178 - mae: 0.1070 - mse: 0.0224 - val_loss: 0.1187 - val_mae: 0.1080 - val_mse: 0.0225\n",
      "Epoch 13/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 0.1170 - mae: 0.1064 - mse: 0.0223 - val_loss: 0.1181 - val_mae: 0.1077 - val_mse: 0.0222\n",
      "Epoch 14/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 0.1165 - mae: 0.1061 - mse: 0.0222 - val_loss: 0.1181 - val_mae: 0.1078 - val_mse: 0.0222\n",
      "Epoch 15/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 0.1160 - mae: 0.1058 - mse: 0.0220 - val_loss: 0.1211 - val_mae: 0.1110 - val_mse: 0.0233\n",
      "Epoch 16/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 0.1126 - mae: 0.1026 - mse: 0.0211 - val_loss: 0.1164 - val_mae: 0.1065 - val_mse: 0.0220\n",
      "Epoch 17/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 0.1119 - mae: 0.1020 - mse: 0.0209 - val_loss: 0.1216 - val_mae: 0.1119 - val_mse: 0.0236\n",
      "Epoch 18/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 841us/step - loss: 0.1127 - mae: 0.1030 - mse: 0.0212 - val_loss: 0.1225 - val_mae: 0.1130 - val_mse: 0.0245\n",
      "Epoch 19/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 0.1115 - mae: 0.1020 - mse: 0.0210 - val_loss: 0.1216 - val_mae: 0.1121 - val_mse: 0.0239\n",
      "Epoch 20/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 0.1115 - mae: 0.1021 - mse: 0.0211 - val_loss: 0.1205 - val_mae: 0.1112 - val_mse: 0.0233\n",
      "Epoch 21/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 0.1107 - mae: 0.1014 - mse: 0.0210 - val_loss: 0.1186 - val_mae: 0.1094 - val_mse: 0.0230\n",
      "Epoch 22/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 0.1096 - mae: 0.1004 - mse: 0.0205 - val_loss: 0.1179 - val_mae: 0.1088 - val_mse: 0.0225\n",
      "Epoch 23/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 0.1104 - mae: 0.1014 - mse: 0.0209 - val_loss: 0.1204 - val_mae: 0.1114 - val_mse: 0.0229\n",
      "Epoch 24/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 0.1084 - mae: 0.0994 - mse: 0.0203 - val_loss: 0.1194 - val_mae: 0.1105 - val_mse: 0.0226\n",
      "Epoch 25/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 723us/step - loss: 0.1075 - mae: 0.0986 - mse: 0.0200 - val_loss: 0.1212 - val_mae: 0.1124 - val_mse: 0.0234\n",
      "Epoch 26/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 0.1073 - mae: 0.0984 - mse: 0.0199 - val_loss: 0.1170 - val_mae: 0.1083 - val_mse: 0.0224\n",
      "Fold 2 - MAE: 0.1065, MSE: 0.0220, R2: 0.9349, TOL90: 1.3193\n",
      "\n",
      "Fold 3/5\n",
      "Epoch 1/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 984us/step - loss: 0.3131 - mae: 0.2960 - mse: 0.2239 - val_loss: 0.1564 - val_mae: 0.1401 - val_mse: 0.0335\n",
      "Epoch 2/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 0.1589 - mae: 0.1428 - mse: 0.0359 - val_loss: 0.1435 - val_mae: 0.1281 - val_mse: 0.0277\n",
      "Epoch 3/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 0.1431 - mae: 0.1279 - mse: 0.0293 - val_loss: 0.1368 - val_mae: 0.1222 - val_mse: 0.0256\n",
      "Epoch 4/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 0.1368 - mae: 0.1224 - mse: 0.0275 - val_loss: 0.1363 - val_mae: 0.1224 - val_mse: 0.0251\n",
      "Epoch 5/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 819us/step - loss: 0.1323 - mae: 0.1186 - mse: 0.0257 - val_loss: 0.1326 - val_mae: 0.1193 - val_mse: 0.0248\n",
      "Epoch 6/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - loss: 0.1269 - mae: 0.1138 - mse: 0.0245 - val_loss: 0.1328 - val_mae: 0.1200 - val_mse: 0.0242\n",
      "Epoch 7/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 0.1269 - mae: 0.1142 - mse: 0.0246 - val_loss: 0.1339 - val_mae: 0.1216 - val_mse: 0.0249\n",
      "Epoch 8/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 0.1252 - mae: 0.1131 - mse: 0.0240 - val_loss: 0.1285 - val_mae: 0.1166 - val_mse: 0.0234\n",
      "Epoch 9/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 0.1225 - mae: 0.1108 - mse: 0.0235 - val_loss: 0.1284 - val_mae: 0.1170 - val_mse: 0.0234\n",
      "Epoch 10/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 0.1225 - mae: 0.1112 - mse: 0.0238 - val_loss: 0.1266 - val_mae: 0.1155 - val_mse: 0.0230\n",
      "Epoch 11/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 0.1200 - mae: 0.1090 - mse: 0.0232 - val_loss: 0.1242 - val_mae: 0.1134 - val_mse: 0.0225\n",
      "Epoch 12/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 0.1165 - mae: 0.1058 - mse: 0.0222 - val_loss: 0.1260 - val_mae: 0.1155 - val_mse: 0.0239\n",
      "Epoch 13/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 0.1183 - mae: 0.1079 - mse: 0.0229 - val_loss: 0.1230 - val_mae: 0.1128 - val_mse: 0.0227\n",
      "Epoch 14/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 728us/step - loss: 0.1143 - mae: 0.1041 - mse: 0.0215 - val_loss: 0.1202 - val_mae: 0.1101 - val_mse: 0.0217\n",
      "Epoch 15/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 0.1150 - mae: 0.1050 - mse: 0.0219 - val_loss: 0.1214 - val_mae: 0.1115 - val_mse: 0.0223\n",
      "Epoch 16/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 0.1145 - mae: 0.1047 - mse: 0.0217 - val_loss: 0.1186 - val_mae: 0.1090 - val_mse: 0.0219\n",
      "Epoch 17/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 826us/step - loss: 0.1135 - mae: 0.1039 - mse: 0.0216 - val_loss: 0.1188 - val_mae: 0.1093 - val_mse: 0.0220\n",
      "Epoch 18/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 0.1139 - mae: 0.1045 - mse: 0.0218 - val_loss: 0.1222 - val_mae: 0.1128 - val_mse: 0.0230\n",
      "Epoch 19/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 0.1138 - mae: 0.1045 - mse: 0.0218 - val_loss: 0.1166 - val_mae: 0.1074 - val_mse: 0.0211\n",
      "Epoch 20/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 0.1120 - mae: 0.1029 - mse: 0.0212 - val_loss: 0.1187 - val_mae: 0.1096 - val_mse: 0.0221\n",
      "Epoch 21/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 928us/step - loss: 0.1107 - mae: 0.1016 - mse: 0.0210 - val_loss: 0.1215 - val_mae: 0.1125 - val_mse: 0.0231\n",
      "Epoch 22/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 0.1099 - mae: 0.1009 - mse: 0.0207 - val_loss: 0.1163 - val_mae: 0.1074 - val_mse: 0.0212\n",
      "Epoch 23/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 0.1110 - mae: 0.1021 - mse: 0.0212 - val_loss: 0.1200 - val_mae: 0.1111 - val_mse: 0.0223\n",
      "Epoch 24/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 0.1068 - mae: 0.0979 - mse: 0.0198 - val_loss: 0.1178 - val_mae: 0.1090 - val_mse: 0.0216\n",
      "Epoch 25/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 0.1081 - mae: 0.0993 - mse: 0.0201 - val_loss: 0.1171 - val_mae: 0.1084 - val_mse: 0.0215\n",
      "Epoch 26/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 0.1105 - mae: 0.1018 - mse: 0.0212 - val_loss: 0.1162 - val_mae: 0.1075 - val_mse: 0.0217\n",
      "Epoch 27/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 0.1084 - mae: 0.0997 - mse: 0.0205 - val_loss: 0.1178 - val_mae: 0.1092 - val_mse: 0.0215\n",
      "Epoch 28/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 862us/step - loss: 0.1058 - mae: 0.0972 - mse: 0.0196 - val_loss: 0.1201 - val_mae: 0.1115 - val_mse: 0.0226\n",
      "Epoch 29/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774us/step - loss: 0.1070 - mae: 0.0984 - mse: 0.0200 - val_loss: 0.1215 - val_mae: 0.1129 - val_mse: 0.0230\n",
      "Epoch 30/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 877us/step - loss: 0.1073 - mae: 0.0988 - mse: 0.0203 - val_loss: 0.1192 - val_mae: 0.1107 - val_mse: 0.0218\n",
      "Epoch 31/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 799us/step - loss: 0.1065 - mae: 0.0980 - mse: 0.0199 - val_loss: 0.1183 - val_mae: 0.1098 - val_mse: 0.0219\n",
      "Epoch 32/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 0.1053 - mae: 0.0968 - mse: 0.0196 - val_loss: 0.1209 - val_mae: 0.1125 - val_mse: 0.0225\n",
      "Epoch 33/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 0.1069 - mae: 0.0984 - mse: 0.0200 - val_loss: 0.1204 - val_mae: 0.1120 - val_mse: 0.0224\n",
      "Epoch 34/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 0.1050 - mae: 0.0966 - mse: 0.0196 - val_loss: 0.1200 - val_mae: 0.1116 - val_mse: 0.0221\n",
      "Epoch 35/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 0.1041 - mae: 0.0957 - mse: 0.0192 - val_loss: 0.1204 - val_mae: 0.1120 - val_mse: 0.0222\n",
      "Epoch 36/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 0.1045 - mae: 0.0962 - mse: 0.0194 - val_loss: 0.1207 - val_mae: 0.1124 - val_mse: 0.0223\n",
      "Fold 3 - MAE: 0.1075, MSE: 0.0217, R2: 0.9357, TOL90: 1.3668\n",
      "\n",
      "Fold 4/5\n",
      "Epoch 1/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 996us/step - loss: 0.2651 - mae: 0.2482 - mse: 0.1422 - val_loss: 0.1924 - val_mae: 0.1762 - val_mse: 0.0525\n",
      "Epoch 2/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 0.1568 - mae: 0.1407 - mse: 0.0345 - val_loss: 0.1618 - val_mae: 0.1462 - val_mse: 0.0376\n",
      "Epoch 3/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 0.1408 - mae: 0.1255 - mse: 0.0277 - val_loss: 0.1610 - val_mae: 0.1462 - val_mse: 0.0362\n",
      "Epoch 4/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 0.1346 - mae: 0.1199 - mse: 0.0257 - val_loss: 0.1441 - val_mae: 0.1298 - val_mse: 0.0298\n",
      "Epoch 5/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 0.1314 - mae: 0.1173 - mse: 0.0246 - val_loss: 0.1424 - val_mae: 0.1286 - val_mse: 0.0294\n",
      "Epoch 6/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 0.1255 - mae: 0.1119 - mse: 0.0229 - val_loss: 0.1452 - val_mae: 0.1319 - val_mse: 0.0309\n",
      "Epoch 7/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 740us/step - loss: 0.1242 - mae: 0.1111 - mse: 0.0230 - val_loss: 0.1432 - val_mae: 0.1304 - val_mse: 0.0294\n",
      "Epoch 8/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 0.1212 - mae: 0.1084 - mse: 0.0220 - val_loss: 0.1369 - val_mae: 0.1244 - val_mse: 0.0278\n",
      "Epoch 9/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 0.1207 - mae: 0.1083 - mse: 0.0222 - val_loss: 0.1336 - val_mae: 0.1214 - val_mse: 0.0276\n",
      "Epoch 10/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 0.1201 - mae: 0.1081 - mse: 0.0223 - val_loss: 0.1338 - val_mae: 0.1220 - val_mse: 0.0279\n",
      "Epoch 11/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 0.1186 - mae: 0.1068 - mse: 0.0219 - val_loss: 0.1363 - val_mae: 0.1247 - val_mse: 0.0283\n",
      "Epoch 12/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 821us/step - loss: 0.1181 - mae: 0.1067 - mse: 0.0218 - val_loss: 0.1410 - val_mae: 0.1298 - val_mse: 0.0315\n",
      "Epoch 13/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 0.1175 - mae: 0.1063 - mse: 0.0215 - val_loss: 0.1356 - val_mae: 0.1246 - val_mse: 0.0301\n",
      "Epoch 14/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 730us/step - loss: 0.1137 - mae: 0.1027 - mse: 0.0207 - val_loss: 0.1256 - val_mae: 0.1147 - val_mse: 0.0261\n",
      "Epoch 15/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 0.1137 - mae: 0.1029 - mse: 0.0206 - val_loss: 0.1379 - val_mae: 0.1272 - val_mse: 0.0303\n",
      "Epoch 16/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 0.1137 - mae: 0.1030 - mse: 0.0205 - val_loss: 0.1342 - val_mae: 0.1237 - val_mse: 0.0296\n",
      "Epoch 17/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 0.1126 - mae: 0.1021 - mse: 0.0205 - val_loss: 0.1303 - val_mae: 0.1200 - val_mse: 0.0282\n",
      "Epoch 18/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 0.1113 - mae: 0.1010 - mse: 0.0201 - val_loss: 0.1358 - val_mae: 0.1256 - val_mse: 0.0309\n",
      "Epoch 19/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 0.1115 - mae: 0.1014 - mse: 0.0204 - val_loss: 0.1329 - val_mae: 0.1229 - val_mse: 0.0295\n",
      "Epoch 20/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 0.1103 - mae: 0.1004 - mse: 0.0203 - val_loss: 0.1375 - val_mae: 0.1277 - val_mse: 0.0311\n",
      "Epoch 21/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 736us/step - loss: 0.1106 - mae: 0.1008 - mse: 0.0202 - val_loss: 0.1350 - val_mae: 0.1253 - val_mse: 0.0302\n",
      "Epoch 22/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 0.1101 - mae: 0.1004 - mse: 0.0199 - val_loss: 0.1362 - val_mae: 0.1267 - val_mse: 0.0318\n",
      "Epoch 23/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 821us/step - loss: 0.1110 - mae: 0.1014 - mse: 0.0205 - val_loss: 0.1332 - val_mae: 0.1237 - val_mse: 0.0296\n",
      "Epoch 24/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 0.1077 - mae: 0.0983 - mse: 0.0195 - val_loss: 0.1381 - val_mae: 0.1287 - val_mse: 0.0322\n",
      "Fold 4 - MAE: 0.1147, MSE: 0.0261, R2: 0.9230, TOL90: 1.3526\n",
      "\n",
      "Fold 5/5\n",
      "Epoch 1/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 992us/step - loss: 0.2904 - mae: 0.2736 - mse: 0.1874 - val_loss: 0.1700 - val_mae: 0.1540 - val_mse: 0.0388\n",
      "Epoch 2/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 0.1576 - mae: 0.1418 - mse: 0.0350 - val_loss: 0.1513 - val_mae: 0.1362 - val_mse: 0.0316\n",
      "Epoch 3/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 0.1454 - mae: 0.1305 - mse: 0.0302 - val_loss: 0.1570 - val_mae: 0.1427 - val_mse: 0.0332\n",
      "Epoch 4/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755us/step - loss: 0.1361 - mae: 0.1220 - mse: 0.0270 - val_loss: 0.1463 - val_mae: 0.1327 - val_mse: 0.0293\n",
      "Epoch 5/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 723us/step - loss: 0.1345 - mae: 0.1210 - mse: 0.0269 - val_loss: 0.1346 - val_mae: 0.1215 - val_mse: 0.0262\n",
      "Epoch 6/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 0.1294 - mae: 0.1165 - mse: 0.0254 - val_loss: 0.1361 - val_mae: 0.1235 - val_mse: 0.0267\n",
      "Epoch 7/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 0.1269 - mae: 0.1145 - mse: 0.0248 - val_loss: 0.1299 - val_mae: 0.1178 - val_mse: 0.0242\n",
      "Epoch 8/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 0.1265 - mae: 0.1146 - mse: 0.0247 - val_loss: 0.1255 - val_mae: 0.1138 - val_mse: 0.0235\n",
      "Epoch 9/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 817us/step - loss: 0.1228 - mae: 0.1112 - mse: 0.0236 - val_loss: 0.1344 - val_mae: 0.1230 - val_mse: 0.0264\n",
      "Epoch 10/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674us/step - loss: 0.1229 - mae: 0.1116 - mse: 0.0239 - val_loss: 0.1350 - val_mae: 0.1240 - val_mse: 0.0278\n",
      "Epoch 11/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 729us/step - loss: 0.1222 - mae: 0.1112 - mse: 0.0236 - val_loss: 0.1241 - val_mae: 0.1133 - val_mse: 0.0246\n",
      "Epoch 12/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 0.1194 - mae: 0.1087 - mse: 0.0229 - val_loss: 0.1283 - val_mae: 0.1177 - val_mse: 0.0258\n",
      "Epoch 13/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 0.1190 - mae: 0.1085 - mse: 0.0227 - val_loss: 0.1282 - val_mae: 0.1179 - val_mse: 0.0263\n",
      "Epoch 14/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 0.1179 - mae: 0.1077 - mse: 0.0225 - val_loss: 0.1223 - val_mae: 0.1122 - val_mse: 0.0233\n",
      "Epoch 15/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 0.1154 - mae: 0.1054 - mse: 0.0216 - val_loss: 0.1232 - val_mae: 0.1132 - val_mse: 0.0245\n",
      "Epoch 16/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 0.1169 - mae: 0.1071 - mse: 0.0225 - val_loss: 0.1292 - val_mae: 0.1195 - val_mse: 0.0273\n",
      "Epoch 17/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 0.1177 - mae: 0.1080 - mse: 0.0227 - val_loss: 0.1229 - val_mae: 0.1133 - val_mse: 0.0232\n",
      "Epoch 18/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 0.1139 - mae: 0.1043 - mse: 0.0215 - val_loss: 0.1215 - val_mae: 0.1120 - val_mse: 0.0239\n",
      "Epoch 19/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 844us/step - loss: 0.1126 - mae: 0.1032 - mse: 0.0211 - val_loss: 0.1244 - val_mae: 0.1150 - val_mse: 0.0250\n",
      "Epoch 20/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 0.1120 - mae: 0.1026 - mse: 0.0212 - val_loss: 0.1227 - val_mae: 0.1134 - val_mse: 0.0242\n",
      "Epoch 21/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 0.1129 - mae: 0.1037 - mse: 0.0213 - val_loss: 0.1218 - val_mae: 0.1126 - val_mse: 0.0240\n",
      "Epoch 22/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 0.1123 - mae: 0.1032 - mse: 0.0211 - val_loss: 0.1163 - val_mae: 0.1073 - val_mse: 0.0219\n",
      "Epoch 23/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 0.1116 - mae: 0.1026 - mse: 0.0209 - val_loss: 0.1238 - val_mae: 0.1148 - val_mse: 0.0248\n",
      "Epoch 24/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 0.1101 - mae: 0.1012 - mse: 0.0206 - val_loss: 0.1222 - val_mae: 0.1134 - val_mse: 0.0238\n",
      "Epoch 25/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 0.1108 - mae: 0.1020 - mse: 0.0207 - val_loss: 0.1214 - val_mae: 0.1126 - val_mse: 0.0243\n",
      "Epoch 26/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 726us/step - loss: 0.1098 - mae: 0.1011 - mse: 0.0206 - val_loss: 0.1230 - val_mae: 0.1143 - val_mse: 0.0239\n",
      "Epoch 27/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 0.1091 - mae: 0.1004 - mse: 0.0204 - val_loss: 0.1178 - val_mae: 0.1091 - val_mse: 0.0229\n",
      "Epoch 28/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 0.1083 - mae: 0.0996 - mse: 0.0201 - val_loss: 0.1237 - val_mae: 0.1151 - val_mse: 0.0244\n",
      "Epoch 29/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 823us/step - loss: 0.1092 - mae: 0.1006 - mse: 0.0205 - val_loss: 0.1248 - val_mae: 0.1163 - val_mse: 0.0247\n",
      "Epoch 30/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 673us/step - loss: 0.1100 - mae: 0.1015 - mse: 0.0208 - val_loss: 0.1156 - val_mae: 0.1072 - val_mse: 0.0219\n",
      "Epoch 31/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 0.1089 - mae: 0.1005 - mse: 0.0205 - val_loss: 0.1187 - val_mae: 0.1103 - val_mse: 0.0226\n",
      "Epoch 32/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 0.1057 - mae: 0.0973 - mse: 0.0195 - val_loss: 0.1191 - val_mae: 0.1107 - val_mse: 0.0226\n",
      "Epoch 33/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 0.1075 - mae: 0.0992 - mse: 0.0201 - val_loss: 0.1180 - val_mae: 0.1097 - val_mse: 0.0225\n",
      "Epoch 34/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724us/step - loss: 0.1056 - mae: 0.0973 - mse: 0.0195 - val_loss: 0.1200 - val_mae: 0.1117 - val_mse: 0.0230\n",
      "Epoch 35/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 0.1057 - mae: 0.0974 - mse: 0.0195 - val_loss: 0.1169 - val_mae: 0.1087 - val_mse: 0.0223\n",
      "Epoch 36/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 0.1057 - mae: 0.0975 - mse: 0.0198 - val_loss: 0.1191 - val_mae: 0.1109 - val_mse: 0.0229\n",
      "Epoch 37/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 0.1049 - mae: 0.0967 - mse: 0.0193 - val_loss: 0.1191 - val_mae: 0.1109 - val_mse: 0.0229\n",
      "Epoch 38/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 810us/step - loss: 0.1049 - mae: 0.0967 - mse: 0.0194 - val_loss: 0.1187 - val_mae: 0.1105 - val_mse: 0.0230\n",
      "Epoch 39/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 0.1050 - mae: 0.0968 - mse: 0.0195 - val_loss: 0.1172 - val_mae: 0.1090 - val_mse: 0.0223\n",
      "Epoch 40/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 0.1054 - mae: 0.0972 - mse: 0.0194 - val_loss: 0.1182 - val_mae: 0.1101 - val_mse: 0.0224\n",
      "Fold 5 - MAE: 0.1072, MSE: 0.0219, R2: 0.9353, TOL90: 1.3164\n",
      "\n",
      "Cross-Validation Results:\n",
      "--------------------------------------------------\n",
      "Average MAE: 0.1091 ± 0.0030\n",
      "Average MSE: 0.0228 ± 0.0016\n",
      "Average R2: 0.9323 ± 0.0048\n"
     ]
    }
   ],
   "source": [
    "cv_results = cv_saved_model_architecture(\n",
    "    saved_model_path='best_model.h5',\n",
    "    df=train_scaled_df,\n",
    "    target_column='r_value',\n",
    "    n_splits=5,\n",
    "    epochs=100,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Print CV results\n",
    "print(\"\\nCross-Validation Results:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Average MAE: {cv_results['avg_mae']:.4f} ± {cv_results['std_mae']:.4f}\")\n",
    "print(f\"Average MSE: {cv_results['avg_mse']:.4f} ± {cv_results['std_mse']:.4f}\")\n",
    "print(f\"Average R2: {cv_results['avg_r2']:.4f} ± {cv_results['std_r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'avg_mae': 0.109099224,\n",
       " 'std_mae': 0.003003221,\n",
       " 'avg_mse': 0.022822645,\n",
       " 'std_mse': 0.0016381636,\n",
       " 'avg_r2': 0.9322552306664029,\n",
       " 'std_r2': 0.004790306009933363,\n",
       " 'avg_tol90': 1.3353492259979247,\n",
       " 'std_tol90': 0.020452509554258386,\n",
       " 'cv_scores': {'mae': [0.10957691,\n",
       "   0.10651408,\n",
       "   0.10750166,\n",
       "   0.11474446,\n",
       "   0.10715896],\n",
       "  'mse': [0.022413583, 0.021979269, 0.021708783, 0.026067216, 0.021944378],\n",
       "  'r2': [0.932395797492249,\n",
       "   0.9348832884736349,\n",
       "   0.935731378504229,\n",
       "   0.9229592508228391,\n",
       "   0.9353064380390621],\n",
       "  'tol90': [1.3217065334320068,\n",
       "   1.3192760944366455,\n",
       "   1.3668313026428223,\n",
       "   1.352554440498352,\n",
       "   1.3163777589797974]}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing GridSearch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters: {'C': 1, 'epsilon': 0.01}\n",
      "\n",
      "Performing cross-validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 - MAE: 0.1044, MSE: 0.0216, R2: 0.9362, TOL90: 0.2364: 100%|██████████| 5/5 [00:28<00:00,  5.73s/it]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "svr_param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'epsilon': [0.001, 0.01, 0.1, 0.5, 1, 2]\n",
    "}\n",
    "\n",
    "svr_results = train_model_with_cv_gridsearch(\n",
    "    df=train_scaled_df,\n",
    "    model=SVR(),\n",
    "    param_grid=svr_param_grid\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing GridSearch...\n",
      "\n",
      "Best parameters: {'alpha': 0.001}\n",
      "\n",
      "Performing cross-validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 - MAE: 0.1217, MSE: 0.0266, R2: 0.9216, TOL90: 0.2564: 100%|██████████| 5/5 [00:00<00:00, 39.98it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge_param_grid = {\n",
    "    'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "}\n",
    "\n",
    "ridge_results = train_model_with_cv_gridsearch(\n",
    "    df=train_scaled_df,\n",
    "    model=Ridge(random_state=42),\n",
    "    param_grid=ridge_param_grid\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_evaluate_model(data_df, features_dict, model_path, n_splits=5, batch_size=32):\n",
    "    \"\"\"\n",
    "    Load saved model and perform cross validation\n",
    "    \n",
    "    Args:\n",
    "        data_df: Pandas DataFrame containing the data\n",
    "        features_dict: Dictionary of features by category\n",
    "        model_path: Path to saved model file\n",
    "        n_splits: Number of CV folds\n",
    "        batch_size: Batch size for evaluation\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of metrics averaged across folds, including tol90\n",
    "    \"\"\"\n",
    "    # Load the saved model\n",
    "    model = torch.load(model_path)\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize feature arrays and dimensions\n",
    "    feature_arrays = {}\n",
    "    feature_dims = {}\n",
    "    \n",
    "    # Process each feature category\n",
    "    for category in ['chemical', 'time', 'process', 'model']:\n",
    "        available_features = [col for col in features_dict[category] \n",
    "                            if col in data_df.columns]\n",
    "        \n",
    "        if available_features:\n",
    "            feature_arrays[category] = data_df[available_features].values.astype(np.float32)\n",
    "            feature_dims[category] = len(available_features)\n",
    "        else:\n",
    "            feature_arrays[category] = np.zeros((len(data_df), 0), dtype=np.float32)\n",
    "            feature_dims[category] = 0\n",
    "    \n",
    "    # Prepare targets\n",
    "    targets = data_df['r_value'].values\n",
    "    \n",
    "    # Initialize KFold\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Lists to store metrics for each fold\n",
    "    fold_metrics = {\n",
    "        'mae': [],\n",
    "        'mse': [],\n",
    "        'rmse': [],\n",
    "        'r2': [],\n",
    "        'tol90': []  # Add tol90 to metrics\n",
    "    }\n",
    "    \n",
    "    # Cross validation loop\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(targets)):\n",
    "        # Prepare validation tensors for this fold\n",
    "        val_tensors = {\n",
    "            'chemical': torch.FloatTensor(feature_arrays['chemical'][val_idx]),\n",
    "            'time': torch.FloatTensor(feature_arrays['time'][val_idx]),\n",
    "            'process': torch.FloatTensor(feature_arrays['process'][val_idx]),\n",
    "            'model': torch.FloatTensor(feature_arrays['model'][val_idx])\n",
    "        }\n",
    "        \n",
    "        val_targets = torch.FloatTensor(targets[val_idx])\n",
    "        \n",
    "        # Create validation DataLoader\n",
    "        val_dataset = TensorDataset(\n",
    "            val_tensors['chemical'],\n",
    "            val_tensors['time'],\n",
    "            val_tensors['process'],\n",
    "            val_tensors['model'],\n",
    "            val_targets\n",
    "        )\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "        \n",
    "        # Evaluation for this fold\n",
    "        predictions = []\n",
    "        actuals = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_chem, batch_time, batch_proc, batch_model, batch_targets in val_loader:\n",
    "                outputs = model(batch_chem, batch_time, batch_proc, batch_model)\n",
    "                predictions.extend(outputs.numpy().flatten())\n",
    "                actuals.extend(batch_targets.numpy().flatten())\n",
    "        \n",
    "        # Calculate metrics for this fold\n",
    "        predictions = np.array(predictions)\n",
    "        actuals = np.array(actuals)\n",
    "        \n",
    "        mae = mean_absolute_error(actuals, predictions)\n",
    "        mse = mean_squared_error(actuals, predictions)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(actuals, predictions)\n",
    "        \n",
    "        # Calculate tol90 (90th percentile of absolute errors)\n",
    "        abs_errors = np.abs(actuals - predictions)\n",
    "        tol90 = np.percentile(abs_errors, 90)\n",
    "        \n",
    "        fold_metrics['mae'].append(mae)\n",
    "        fold_metrics['mse'].append(mse)\n",
    "        fold_metrics['rmse'].append(rmse)\n",
    "        fold_metrics['r2'].append(r2)\n",
    "        fold_metrics['tol90'].append(tol90)\n",
    "        \n",
    "        print(f\"Fold {fold+1} - MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}, R2: {r2:.4f}, TOL90: {tol90:.4f}\")\n",
    "    \n",
    "    # Calculate and return average metrics\n",
    "    avg_metrics = {\n",
    "        'mae': np.mean(fold_metrics['mae']),\n",
    "        'mae_std': np.std(fold_metrics['mae']),\n",
    "        'mse': np.mean(fold_metrics['mse']),\n",
    "        'mse_std': np.std(fold_metrics['mse']), \n",
    "        'rmse': np.mean(fold_metrics['rmse']),\n",
    "        'rmse_std': np.std(fold_metrics['rmse']),\n",
    "        'r2': np.mean(fold_metrics['r2']),\n",
    "        'r2_std': np.std(fold_metrics['r2']),\n",
    "        'tol90': np.mean(fold_metrics['tol90']),  # Add average tol90\n",
    "        'tol90_std': np.std(fold_metrics['tol90'])  # Add tol90 standard deviation\n",
    "    }\n",
    "    \n",
    "    print(\"\\nAverage Metrics across folds:\")\n",
    "    print(f\"MAE: {avg_metrics['mae']:.4f} ± {avg_metrics['mae_std']:.4f}\")\n",
    "    print(f\"MSE: {avg_metrics['mse']:.4f} ± {avg_metrics['mse_std']:.4f}\")\n",
    "    print(f\"RMSE: {avg_metrics['rmse']:.4f} ± {avg_metrics['rmse_std']:.4f}\")\n",
    "    print(f\"R2: {avg_metrics['r2']:.4f} ± {avg_metrics['r2_std']:.4f}\")\n",
    "    print(f\"TOL90: {avg_metrics['tol90']:.4f} ± {avg_metrics['tol90_std']:.4f}\")\n",
    "    \n",
    "    return avg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labeling the features for each branch\n",
    "features = [col for col in df.columns if col not in ['r_value', 'steel_family', 'steel_grade']]\n",
    "features_dict = {\n",
    "   'time': [col for col in features if 'time' in col.lower()], \n",
    "   'chemical': ['pct_al', 'pct_b', 'pct_c', 'pct_cr', 'pct_mn', 'pct_n', 'pct_nb', 'pct_si', 'pct_ti', 'pct_v', 'mfia_coil_frac_fer', 'mfia_et1_frac_fer', 'mfia_et2_frac_fer'],\n",
    "   'model': [\"rm\", \"ag\", \"a80\", \"n_value\"]\n",
    "}\n",
    "features_dict['process'] = [col for col in features if col not in features_dict['time'] and col not in features_dict['chemical']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiBranchSteelRegressor(nn.Module):\n",
    "    def __init__(self, chemical_dim, time_dim, process_dim, model_dim, hidden_units=64, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        # Track which branches are active\n",
    "        self.has_chemical = chemical_dim > 0\n",
    "        self.has_time = time_dim > 0\n",
    "        self.has_process = process_dim > 0\n",
    "        self.has_model = model_dim > 0\n",
    "        \n",
    "        # Count active branches\n",
    "        self.active_branches = sum([self.has_chemical, self.has_time, self.has_process, self.has_model])\n",
    "        \n",
    "        # Adjust hidden units for each branch\n",
    "        self.branch_hidden = min(hidden_units, max(16, hidden_units // 2))\n",
    "        \n",
    "        # Creating branch\n",
    "        def create_branch(input_dim):\n",
    "            return nn.Sequential(\n",
    "                nn.Linear(input_dim, self.branch_hidden),\n",
    "                nn.BatchNorm1d(self.branch_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            )\n",
    "        \n",
    "        # Only create branches that have features\n",
    "        if self.has_chemical:\n",
    "            self.chemical_branch = create_branch(chemical_dim)\n",
    "        if self.has_time:\n",
    "            self.time_branch = create_branch(time_dim)\n",
    "        if self.has_process:\n",
    "            self.process_branch = create_branch(process_dim)\n",
    "        if self.has_model:\n",
    "            self.model_branch = create_branch(model_dim)\n",
    "        \n",
    "        # Combined input dimension based on active branches only\n",
    "        combined_dim = self.branch_hidden * self.active_branches\n",
    "        \n",
    "        # Final layers after concatenation\n",
    "        self.final_layers = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, chemical, time, process, model):\n",
    "        features = []\n",
    "        # Only process branches that have features\n",
    "        if self.has_chemical:\n",
    "            if chemical.dim() == 1:\n",
    "                chemical = chemical.unsqueeze(0)\n",
    "            features.append(self.chemical_branch(chemical))\n",
    "        \n",
    "        if self.has_time:\n",
    "            if time.dim() == 1:\n",
    "                time = time.unsqueeze(0)\n",
    "            features.append(self.time_branch(time))\n",
    "        \n",
    "        if self.has_process:\n",
    "            if process.dim() == 1:\n",
    "                process = process.unsqueeze(0)\n",
    "            features.append(self.process_branch(process))\n",
    "        \n",
    "        if self.has_model:\n",
    "            if model.dim() == 1:\n",
    "                model = model.unsqueeze(0)\n",
    "            features.append(self.model_branch(model))\n",
    "        \n",
    "        # Concatenate only active features\n",
    "        combined = torch.cat(features, dim=1) if len(features) > 1 else features[0]\n",
    "        return self.final_layers(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_regular(df, features_dict, num_epochs, hyperparameters, use_l2=False):\n",
    "    batch_size = hyperparameters['batch_size']\n",
    "    \n",
    "    # Initialize feature arrays and dimensions\n",
    "    feature_arrays = {}\n",
    "    feature_dims = {}\n",
    "    \n",
    "    # Process each feature category\n",
    "    for category in ['chemical', 'time', 'process', 'model']:\n",
    "        available_features = [col for col in features_dict[category] \n",
    "                            if col in df.columns]\n",
    "        \n",
    "        if available_features:\n",
    "            feature_arrays[category] = df[available_features].values.astype(np.float32)\n",
    "            feature_dims[category] = len(available_features)\n",
    "        else:\n",
    "            feature_arrays[category] = np.zeros((len(df), 0), dtype=np.float32)\n",
    "            feature_dims[category] = 0\n",
    "    \n",
    "    # Prepare targets\n",
    "    targets = df['r_value'].values\n",
    "    \n",
    "    # Split data\n",
    "    split_data = train_test_split(\n",
    "        feature_arrays['chemical'],\n",
    "        feature_arrays['time'],\n",
    "        feature_arrays['process'],\n",
    "        feature_arrays['model'],\n",
    "        targets,\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    (X_train_chem, X_test_chem, X_train_time, X_test_time, \n",
    "     X_train_proc, X_test_proc, X_train_model, X_test_model, \n",
    "     y_train, y_test) = split_data\n",
    "    \n",
    "    # Convert to tensors\n",
    "    train_tensors = {\n",
    "        'chemical': torch.FloatTensor(X_train_chem),\n",
    "        'time': torch.FloatTensor(X_train_time),\n",
    "        'process': torch.FloatTensor(X_train_proc),\n",
    "        'model': torch.FloatTensor(X_train_model)\n",
    "    }\n",
    "    \n",
    "    test_tensors = {\n",
    "        'chemical': torch.FloatTensor(X_test_chem),\n",
    "        'time': torch.FloatTensor(X_test_time),\n",
    "        'process': torch.FloatTensor(X_test_proc),\n",
    "        'model': torch.FloatTensor(X_test_model)\n",
    "    }\n",
    "    \n",
    "    y_train_tensor = torch.FloatTensor(y_train)\n",
    "    y_test_tensor = torch.FloatTensor(y_test)\n",
    "    \n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(\n",
    "        train_tensors['chemical'],\n",
    "        train_tensors['time'],\n",
    "        train_tensors['process'],\n",
    "        train_tensors['model'],\n",
    "        y_train_tensor\n",
    "    )\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = MultiBranchSteelRegressor(\n",
    "        chemical_dim=feature_dims['chemical'],\n",
    "        time_dim=feature_dims['time'],\n",
    "        process_dim=feature_dims['process'],\n",
    "        model_dim=feature_dims['model'],\n",
    "        hidden_units=hyperparameters['hidden_units'],\n",
    "        dropout_rate=hyperparameters['dropout_rate']\n",
    "    )\n",
    "    \n",
    "    if use_l2:\n",
    "        weight_decay = 0.001\n",
    "    else:\n",
    "        weight_decay = 0.0\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), weight_decay=weight_decay)\n",
    "    criterion = nn.L1Loss()\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch_chem, batch_time, batch_proc, batch_model, batch_targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_chem, batch_time, batch_proc, batch_model)\n",
    "            loss = criterion(outputs, batch_targets.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(\n",
    "            test_tensors['chemical'],\n",
    "            test_tensors['time'],\n",
    "            test_tensors['process'],\n",
    "            test_tensors['model']\n",
    "        )\n",
    "        test_loss = criterion(y_pred, y_test_tensor.unsqueeze(1)).item()\n",
    "        y_pred_np = y_pred.numpy().flatten()\n",
    "        r2 = r2_score(y_test, y_pred_np)\n",
    "        mae = mean_absolute_error(y_test, y_pred_np)\n",
    "        mse = mean_squared_error(y_test, y_pred_np)\n",
    "        \n",
    "        metrics = {\n",
    "            'r2_score': r2,\n",
    "            'mae': mae,\n",
    "            'mse': mse,\n",
    "            'test_loss': test_loss\n",
    "        }\n",
    "        print(f\"Evaluation - Test Loss: {test_loss:.4f}, R2: {r2:.4f}\")\n",
    "    \n",
    "    return model, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.01, 1e-3],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'hidden_units': [64, 128, 256],\n",
    "    'dropout_rate': [0, 0.2]\n",
    "}\n",
    "grid = ParameterGrid(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:   0%|          | 0/54 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating hyperparameters: {'batch_size': 16, 'dropout_rate': 0, 'hidden_units': 64, 'learning_rate': 0.1}\n",
      "Epoch [10/100], Loss: 0.1243\n",
      "Epoch [20/100], Loss: 0.1143\n",
      "Epoch [30/100], Loss: 0.1085\n",
      "Epoch [40/100], Loss: 0.1073\n",
      "Epoch [50/100], Loss: 0.1052\n",
      "Epoch [60/100], Loss: 0.1029\n",
      "Epoch [70/100], Loss: 0.1026\n",
      "Epoch [80/100], Loss: 0.1008\n",
      "Epoch [90/100], Loss: 0.0996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:   2%|▏         | 1/54 [04:20<3:49:50, 260.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.0991\n",
      "Evaluation - Test Loss: 0.1047, R2: 0.9341\n",
      "Evaluating hyperparameters: {'batch_size': 16, 'dropout_rate': 0, 'hidden_units': 64, 'learning_rate': 0.01}\n",
      "Epoch [10/100], Loss: 0.1231\n",
      "Epoch [20/100], Loss: 0.1148\n",
      "Epoch [30/100], Loss: 0.1100\n",
      "Epoch [40/100], Loss: 0.1080\n",
      "Epoch [50/100], Loss: 0.1048\n",
      "Epoch [60/100], Loss: 0.1030\n",
      "Epoch [70/100], Loss: 0.1015\n",
      "Epoch [80/100], Loss: 0.1001\n",
      "Epoch [90/100], Loss: 0.1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:   4%|▎         | 2/54 [08:27<3:38:49, 252.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.0999\n",
      "Evaluation - Test Loss: 0.1021, R2: 0.9362\n",
      "Evaluating hyperparameters: {'batch_size': 16, 'dropout_rate': 0, 'hidden_units': 64, 'learning_rate': 0.001}\n",
      "Epoch [10/100], Loss: 0.1238\n",
      "Epoch [20/100], Loss: 0.1142\n",
      "Epoch [30/100], Loss: 0.1084\n",
      "Epoch [40/100], Loss: 0.1069\n",
      "Epoch [50/100], Loss: 0.1049\n",
      "Epoch [60/100], Loss: 0.1041\n",
      "Epoch [70/100], Loss: 0.1025\n",
      "Epoch [80/100], Loss: 0.1010\n",
      "Epoch [90/100], Loss: 0.0998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:   6%|▌         | 3/54 [11:38<3:10:46, 224.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.1004\n",
      "Evaluation - Test Loss: 0.1017, R2: 0.9368\n",
      "Evaluating hyperparameters: {'batch_size': 16, 'dropout_rate': 0, 'hidden_units': 128, 'learning_rate': 0.1}\n",
      "Epoch [10/100], Loss: 0.1223\n",
      "Epoch [20/100], Loss: 0.1129\n",
      "Epoch [30/100], Loss: 0.1081\n",
      "Epoch [40/100], Loss: 0.1055\n",
      "Epoch [50/100], Loss: 0.1036\n",
      "Epoch [60/100], Loss: 0.1026\n",
      "Epoch [70/100], Loss: 0.0996\n",
      "Epoch [80/100], Loss: 0.0990\n",
      "Epoch [90/100], Loss: 0.0981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:   7%|▋         | 4/54 [14:59<2:59:18, 215.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.0961\n",
      "Evaluation - Test Loss: 0.1012, R2: 0.9374\n",
      "Evaluating hyperparameters: {'batch_size': 16, 'dropout_rate': 0, 'hidden_units': 128, 'learning_rate': 0.01}\n",
      "Epoch [10/100], Loss: 0.1201\n",
      "Epoch [20/100], Loss: 0.1114\n",
      "Epoch [30/100], Loss: 0.1085\n",
      "Epoch [40/100], Loss: 0.1063\n",
      "Epoch [50/100], Loss: 0.1027\n",
      "Epoch [60/100], Loss: 0.1017\n",
      "Epoch [70/100], Loss: 0.1003\n",
      "Epoch [80/100], Loss: 0.0988\n",
      "Epoch [90/100], Loss: 0.0970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:   9%|▉         | 5/54 [18:16<2:50:28, 208.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.0965\n",
      "Evaluation - Test Loss: 0.1030, R2: 0.9366\n",
      "Evaluating hyperparameters: {'batch_size': 16, 'dropout_rate': 0, 'hidden_units': 128, 'learning_rate': 0.001}\n",
      "Epoch [10/100], Loss: 0.1237\n",
      "Epoch [20/100], Loss: 0.1127\n",
      "Epoch [30/100], Loss: 0.1089\n",
      "Epoch [40/100], Loss: 0.1061\n",
      "Epoch [50/100], Loss: 0.1033\n",
      "Epoch [60/100], Loss: 0.1012\n",
      "Epoch [70/100], Loss: 0.1009\n",
      "Epoch [80/100], Loss: 0.0987\n",
      "Epoch [90/100], Loss: 0.0987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  11%|█         | 6/54 [21:32<2:43:30, 204.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.0972\n",
      "Evaluation - Test Loss: 0.1033, R2: 0.9362\n",
      "Evaluating hyperparameters: {'batch_size': 16, 'dropout_rate': 0, 'hidden_units': 256, 'learning_rate': 0.1}\n",
      "Epoch [10/100], Loss: 0.1234\n",
      "Epoch [20/100], Loss: 0.1147\n",
      "Epoch [30/100], Loss: 0.1085\n",
      "Epoch [40/100], Loss: 0.1049\n",
      "Epoch [50/100], Loss: 0.1018\n",
      "Epoch [60/100], Loss: 0.0997\n",
      "Epoch [70/100], Loss: 0.0988\n",
      "Epoch [80/100], Loss: 0.0977\n",
      "Epoch [90/100], Loss: 0.0966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  13%|█▎        | 7/54 [24:56<2:39:57, 204.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.0961\n",
      "Evaluation - Test Loss: 0.1042, R2: 0.9332\n",
      "Evaluating hyperparameters: {'batch_size': 16, 'dropout_rate': 0, 'hidden_units': 256, 'learning_rate': 0.01}\n",
      "Epoch [10/100], Loss: 0.1224\n",
      "Epoch [20/100], Loss: 0.1120\n",
      "Epoch [30/100], Loss: 0.1078\n",
      "Epoch [40/100], Loss: 0.1052\n",
      "Epoch [50/100], Loss: 0.1031\n",
      "Epoch [60/100], Loss: 0.1003\n",
      "Epoch [70/100], Loss: 0.0985\n",
      "Epoch [80/100], Loss: 0.0978\n",
      "Epoch [90/100], Loss: 0.0970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  15%|█▍        | 8/54 [28:22<2:37:03, 204.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.0945\n",
      "Evaluation - Test Loss: 0.1023, R2: 0.9359\n",
      "Evaluating hyperparameters: {'batch_size': 16, 'dropout_rate': 0, 'hidden_units': 256, 'learning_rate': 0.001}\n",
      "Epoch [10/100], Loss: 0.1231\n",
      "Epoch [20/100], Loss: 0.1122\n",
      "Epoch [30/100], Loss: 0.1077\n",
      "Epoch [40/100], Loss: 0.1043\n",
      "Epoch [50/100], Loss: 0.1020\n",
      "Epoch [60/100], Loss: 0.1003\n",
      "Epoch [70/100], Loss: 0.0989\n",
      "Epoch [80/100], Loss: 0.0972\n",
      "Epoch [90/100], Loss: 0.0964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  17%|█▋        | 9/54 [32:04<2:37:38, 210.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.0945\n",
      "Evaluation - Test Loss: 0.1040, R2: 0.9333\n",
      "Evaluating hyperparameters: {'batch_size': 16, 'dropout_rate': 0.2, 'hidden_units': 64, 'learning_rate': 0.1}\n",
      "Epoch [10/100], Loss: 0.1628\n",
      "Epoch [20/100], Loss: 0.1495\n",
      "Epoch [30/100], Loss: 0.1413\n",
      "Epoch [40/100], Loss: 0.1396\n",
      "Epoch [50/100], Loss: 0.1380\n",
      "Epoch [60/100], Loss: 0.1351\n",
      "Epoch [70/100], Loss: 0.1330\n",
      "Epoch [80/100], Loss: 0.1322\n",
      "Epoch [90/100], Loss: 0.1337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  19%|█▊        | 10/54 [35:28<2:32:38, 208.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.1321\n",
      "Evaluation - Test Loss: 0.1193, R2: 0.9218\n",
      "Evaluating hyperparameters: {'batch_size': 16, 'dropout_rate': 0.2, 'hidden_units': 64, 'learning_rate': 0.01}\n",
      "Epoch [10/100], Loss: 0.1625\n",
      "Epoch [20/100], Loss: 0.1459\n",
      "Epoch [30/100], Loss: 0.1422\n",
      "Epoch [40/100], Loss: 0.1393\n",
      "Epoch [50/100], Loss: 0.1390\n",
      "Epoch [60/100], Loss: 0.1352\n",
      "Epoch [70/100], Loss: 0.1350\n",
      "Epoch [80/100], Loss: 0.1345\n",
      "Epoch [90/100], Loss: 0.1342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  20%|██        | 11/54 [38:52<2:28:17, 206.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.1320\n",
      "Evaluation - Test Loss: 0.1077, R2: 0.9324\n",
      "Evaluating hyperparameters: {'batch_size': 16, 'dropout_rate': 0.2, 'hidden_units': 64, 'learning_rate': 0.001}\n",
      "Epoch [10/100], Loss: 0.1586\n",
      "Epoch [20/100], Loss: 0.1466\n",
      "Epoch [30/100], Loss: 0.1445\n",
      "Epoch [40/100], Loss: 0.1388\n",
      "Epoch [50/100], Loss: 0.1386\n",
      "Epoch [60/100], Loss: 0.1356\n",
      "Epoch [70/100], Loss: 0.1347\n",
      "Epoch [80/100], Loss: 0.1365\n",
      "Epoch [90/100], Loss: 0.1357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  22%|██▏       | 12/54 [42:15<2:23:59, 205.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.1316\n",
      "Evaluation - Test Loss: 0.1172, R2: 0.9227\n",
      "Evaluating hyperparameters: {'batch_size': 16, 'dropout_rate': 0.2, 'hidden_units': 128, 'learning_rate': 0.1}\n",
      "Epoch [10/100], Loss: 0.1567\n",
      "Epoch [20/100], Loss: 0.1454\n",
      "Epoch [30/100], Loss: 0.1418\n",
      "Epoch [40/100], Loss: 0.1401\n",
      "Epoch [50/100], Loss: 0.1382\n",
      "Epoch [60/100], Loss: 0.1356\n",
      "Epoch [70/100], Loss: 0.1321\n",
      "Epoch [80/100], Loss: 0.1337\n",
      "Epoch [90/100], Loss: 0.1346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  24%|██▍       | 13/54 [45:43<2:21:00, 206.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.1317\n",
      "Evaluation - Test Loss: 0.1182, R2: 0.9202\n",
      "Evaluating hyperparameters: {'batch_size': 16, 'dropout_rate': 0.2, 'hidden_units': 128, 'learning_rate': 0.01}\n",
      "Epoch [10/100], Loss: 0.1581\n",
      "Epoch [20/100], Loss: 0.1470\n",
      "Epoch [30/100], Loss: 0.1407\n",
      "Epoch [40/100], Loss: 0.1386\n",
      "Epoch [50/100], Loss: 0.1369\n",
      "Epoch [60/100], Loss: 0.1331\n",
      "Epoch [70/100], Loss: 0.1338\n",
      "Epoch [80/100], Loss: 0.1335\n",
      "Epoch [90/100], Loss: 0.1326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  26%|██▌       | 14/54 [49:11<2:17:55, 206.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.1310\n",
      "Evaluation - Test Loss: 0.1146, R2: 0.9240\n",
      "Evaluating hyperparameters: {'batch_size': 16, 'dropout_rate': 0.2, 'hidden_units': 128, 'learning_rate': 0.001}\n",
      "Epoch [10/100], Loss: 0.1603\n",
      "Epoch [20/100], Loss: 0.1449\n",
      "Epoch [30/100], Loss: 0.1379\n",
      "Epoch [40/100], Loss: 0.1355\n",
      "Epoch [50/100], Loss: 0.1364\n",
      "Epoch [60/100], Loss: 0.1343\n",
      "Epoch [70/100], Loss: 0.1358\n",
      "Epoch [80/100], Loss: 0.1333\n",
      "Epoch [90/100], Loss: 0.1307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  28%|██▊       | 15/54 [52:39<2:14:47, 207.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.1312\n",
      "Evaluation - Test Loss: 0.1162, R2: 0.9245\n",
      "Evaluating hyperparameters: {'batch_size': 16, 'dropout_rate': 0.2, 'hidden_units': 256, 'learning_rate': 0.1}\n",
      "Epoch [10/100], Loss: 0.1566\n",
      "Epoch [20/100], Loss: 0.1420\n",
      "Epoch [30/100], Loss: 0.1374\n",
      "Epoch [40/100], Loss: 0.1377\n",
      "Epoch [50/100], Loss: 0.1343\n",
      "Epoch [60/100], Loss: 0.1322\n",
      "Epoch [70/100], Loss: 0.1336\n",
      "Epoch [80/100], Loss: 0.1334\n",
      "Epoch [90/100], Loss: 0.1316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  30%|██▉       | 16/54 [56:14<2:12:48, 209.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.1315\n",
      "Evaluation - Test Loss: 0.1057, R2: 0.9356\n",
      "Evaluating hyperparameters: {'batch_size': 16, 'dropout_rate': 0.2, 'hidden_units': 256, 'learning_rate': 0.01}\n",
      "Epoch [10/100], Loss: 0.1578\n",
      "Epoch [20/100], Loss: 0.1440\n",
      "Epoch [30/100], Loss: 0.1367\n",
      "Epoch [40/100], Loss: 0.1353\n",
      "Epoch [50/100], Loss: 0.1324\n",
      "Epoch [60/100], Loss: 0.1299\n",
      "Epoch [70/100], Loss: 0.1295\n",
      "Epoch [80/100], Loss: 0.1302\n",
      "Epoch [90/100], Loss: 0.1276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  31%|███▏      | 17/54 [59:51<2:10:33, 211.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.1273\n",
      "Evaluation - Test Loss: 0.1141, R2: 0.9256\n",
      "Evaluating hyperparameters: {'batch_size': 16, 'dropout_rate': 0.2, 'hidden_units': 256, 'learning_rate': 0.001}\n",
      "Epoch [10/100], Loss: 0.1550\n",
      "Epoch [20/100], Loss: 0.1441\n",
      "Epoch [30/100], Loss: 0.1377\n",
      "Epoch [40/100], Loss: 0.1369\n",
      "Epoch [50/100], Loss: 0.1322\n",
      "Epoch [60/100], Loss: 0.1323\n",
      "Epoch [70/100], Loss: 0.1293\n",
      "Epoch [80/100], Loss: 0.1279\n",
      "Epoch [90/100], Loss: 0.1281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  33%|███▎      | 18/54 [1:03:26<2:07:43, 212.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.1286\n",
      "Evaluation - Test Loss: 0.1164, R2: 0.9243\n",
      "Evaluating hyperparameters: {'batch_size': 32, 'dropout_rate': 0, 'hidden_units': 64, 'learning_rate': 0.1}\n",
      "Epoch [10/100], Loss: 0.1186\n",
      "Epoch [20/100], Loss: 0.1118\n",
      "Epoch [30/100], Loss: 0.1059\n",
      "Epoch [40/100], Loss: 0.1031\n",
      "Epoch [50/100], Loss: 0.0995\n",
      "Epoch [60/100], Loss: 0.0988\n",
      "Epoch [70/100], Loss: 0.0971\n",
      "Epoch [80/100], Loss: 0.0962\n",
      "Epoch [90/100], Loss: 0.0932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  35%|███▌      | 19/54 [1:05:13<1:45:35, 181.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.0932\n",
      "Evaluation - Test Loss: 0.1045, R2: 0.9347\n",
      "Evaluating hyperparameters: {'batch_size': 32, 'dropout_rate': 0, 'hidden_units': 64, 'learning_rate': 0.01}\n",
      "Epoch [10/100], Loss: 0.1190\n",
      "Epoch [20/100], Loss: 0.1107\n",
      "Epoch [30/100], Loss: 0.1058\n",
      "Epoch [40/100], Loss: 0.1018\n",
      "Epoch [50/100], Loss: 0.1003\n",
      "Epoch [60/100], Loss: 0.0983\n",
      "Epoch [70/100], Loss: 0.0966\n",
      "Epoch [80/100], Loss: 0.0946\n",
      "Epoch [90/100], Loss: 0.0932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  37%|███▋      | 20/54 [1:06:58<1:29:37, 158.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.0912\n",
      "Evaluation - Test Loss: 0.1029, R2: 0.9330\n",
      "Evaluating hyperparameters: {'batch_size': 32, 'dropout_rate': 0, 'hidden_units': 64, 'learning_rate': 0.001}\n",
      "Epoch [10/100], Loss: 0.1195\n",
      "Epoch [20/100], Loss: 0.1094\n",
      "Epoch [30/100], Loss: 0.1055\n",
      "Epoch [40/100], Loss: 0.1028\n",
      "Epoch [50/100], Loss: 0.1007\n",
      "Epoch [60/100], Loss: 0.0970\n",
      "Epoch [70/100], Loss: 0.0972\n",
      "Epoch [80/100], Loss: 0.0951\n",
      "Epoch [90/100], Loss: 0.0928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  39%|███▉      | 21/54 [1:08:45<1:18:31, 142.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.0929\n",
      "Evaluation - Test Loss: 0.1068, R2: 0.9335\n",
      "Evaluating hyperparameters: {'batch_size': 32, 'dropout_rate': 0, 'hidden_units': 128, 'learning_rate': 0.1}\n",
      "Epoch [10/100], Loss: 0.1152\n",
      "Epoch [20/100], Loss: 0.1083\n",
      "Epoch [30/100], Loss: 0.1039\n",
      "Epoch [40/100], Loss: 0.1004\n",
      "Epoch [50/100], Loss: 0.0983\n",
      "Epoch [60/100], Loss: 0.0957\n",
      "Epoch [70/100], Loss: 0.0925\n",
      "Epoch [80/100], Loss: 0.0911\n",
      "Epoch [90/100], Loss: 0.0909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  41%|████      | 22/54 [1:10:32<1:10:26, 132.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.0884\n",
      "Evaluation - Test Loss: 0.1034, R2: 0.9338\n",
      "Evaluating hyperparameters: {'batch_size': 32, 'dropout_rate': 0, 'hidden_units': 128, 'learning_rate': 0.01}\n",
      "Epoch [10/100], Loss: 0.1165\n",
      "Epoch [20/100], Loss: 0.1068\n",
      "Epoch [30/100], Loss: 0.1020\n",
      "Epoch [40/100], Loss: 0.0992\n",
      "Epoch [50/100], Loss: 0.0976\n",
      "Epoch [60/100], Loss: 0.0949\n",
      "Epoch [70/100], Loss: 0.0930\n",
      "Epoch [80/100], Loss: 0.0904\n",
      "Epoch [90/100], Loss: 0.0895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  43%|████▎     | 23/54 [1:12:20<1:04:26, 124.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.0866\n",
      "Evaluation - Test Loss: 0.1081, R2: 0.9297\n",
      "Evaluating hyperparameters: {'batch_size': 32, 'dropout_rate': 0, 'hidden_units': 128, 'learning_rate': 0.001}\n",
      "Epoch [10/100], Loss: 0.1176\n",
      "Epoch [20/100], Loss: 0.1103\n",
      "Epoch [30/100], Loss: 0.1048\n",
      "Epoch [40/100], Loss: 0.0995\n",
      "Epoch [50/100], Loss: 0.0982\n",
      "Epoch [60/100], Loss: 0.0954\n",
      "Epoch [70/100], Loss: 0.0937\n",
      "Epoch [80/100], Loss: 0.0917\n",
      "Epoch [90/100], Loss: 0.0900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  44%|████▍     | 24/54 [1:14:07<59:47, 119.59s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.0886\n",
      "Evaluation - Test Loss: 0.1051, R2: 0.9325\n",
      "Evaluating hyperparameters: {'batch_size': 32, 'dropout_rate': 0, 'hidden_units': 256, 'learning_rate': 0.1}\n",
      "Epoch [10/100], Loss: 0.1171\n",
      "Epoch [20/100], Loss: 0.1081\n",
      "Epoch [30/100], Loss: 0.1032\n",
      "Epoch [40/100], Loss: 0.0999\n",
      "Epoch [50/100], Loss: 0.0968\n",
      "Epoch [60/100], Loss: 0.0932\n",
      "Epoch [70/100], Loss: 0.0906\n",
      "Epoch [80/100], Loss: 0.0889\n",
      "Epoch [90/100], Loss: 0.0872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  46%|████▋     | 25/54 [1:16:01<56:54, 117.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.0852\n",
      "Evaluation - Test Loss: 0.1043, R2: 0.9342\n",
      "Evaluating hyperparameters: {'batch_size': 32, 'dropout_rate': 0, 'hidden_units': 256, 'learning_rate': 0.01}\n",
      "Epoch [10/100], Loss: 0.1152\n",
      "Epoch [20/100], Loss: 0.1084\n",
      "Epoch [30/100], Loss: 0.1033\n",
      "Epoch [40/100], Loss: 0.0988\n",
      "Epoch [50/100], Loss: 0.0961\n",
      "Epoch [60/100], Loss: 0.0938\n",
      "Epoch [70/100], Loss: 0.0916\n",
      "Epoch [80/100], Loss: 0.0888\n",
      "Epoch [90/100], Loss: 0.0887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  48%|████▊     | 26/54 [1:17:53<54:12, 116.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.0861\n",
      "Evaluation - Test Loss: 0.1070, R2: 0.9310\n",
      "Evaluating hyperparameters: {'batch_size': 32, 'dropout_rate': 0, 'hidden_units': 256, 'learning_rate': 0.001}\n",
      "Epoch [10/100], Loss: 0.1156\n",
      "Epoch [20/100], Loss: 0.1078\n",
      "Epoch [30/100], Loss: 0.1025\n",
      "Epoch [40/100], Loss: 0.0999\n",
      "Epoch [50/100], Loss: 0.0966\n",
      "Epoch [60/100], Loss: 0.0944\n",
      "Epoch [70/100], Loss: 0.0909\n",
      "Epoch [80/100], Loss: 0.0900\n",
      "Epoch [90/100], Loss: 0.0893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  50%|█████     | 27/54 [1:19:45<51:42, 114.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.0873\n",
      "Evaluation - Test Loss: 0.1047, R2: 0.9342\n",
      "Evaluating hyperparameters: {'batch_size': 32, 'dropout_rate': 0.2, 'hidden_units': 64, 'learning_rate': 0.1}\n",
      "Epoch [10/100], Loss: 0.1638\n",
      "Epoch [20/100], Loss: 0.1457\n",
      "Epoch [30/100], Loss: 0.1366\n",
      "Epoch [40/100], Loss: 0.1356\n",
      "Epoch [50/100], Loss: 0.1333\n",
      "Epoch [60/100], Loss: 0.1305\n",
      "Epoch [70/100], Loss: 0.1297\n",
      "Epoch [80/100], Loss: 0.1298\n",
      "Epoch [90/100], Loss: 0.1275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  52%|█████▏    | 28/54 [1:21:37<49:23, 113.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.1268\n",
      "Evaluation - Test Loss: 0.1087, R2: 0.9300\n",
      "Evaluating hyperparameters: {'batch_size': 32, 'dropout_rate': 0.2, 'hidden_units': 64, 'learning_rate': 0.01}\n",
      "Epoch [10/100], Loss: 0.1632\n",
      "Epoch [20/100], Loss: 0.1434\n",
      "Epoch [30/100], Loss: 0.1354\n",
      "Epoch [40/100], Loss: 0.1361\n",
      "Epoch [50/100], Loss: 0.1316\n",
      "Epoch [60/100], Loss: 0.1304\n",
      "Epoch [70/100], Loss: 0.1284\n",
      "Epoch [80/100], Loss: 0.1273\n",
      "Epoch [90/100], Loss: 0.1252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  54%|█████▎    | 29/54 [1:23:28<47:10, 113.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.1260\n",
      "Evaluation - Test Loss: 0.1049, R2: 0.9351\n",
      "Evaluating hyperparameters: {'batch_size': 32, 'dropout_rate': 0.2, 'hidden_units': 64, 'learning_rate': 0.001}\n",
      "Epoch [10/100], Loss: 0.1619\n",
      "Epoch [20/100], Loss: 0.1421\n",
      "Epoch [30/100], Loss: 0.1354\n",
      "Epoch [40/100], Loss: 0.1329\n",
      "Epoch [50/100], Loss: 0.1304\n",
      "Epoch [60/100], Loss: 0.1284\n",
      "Epoch [70/100], Loss: 0.1285\n",
      "Epoch [80/100], Loss: 0.1260\n",
      "Epoch [90/100], Loss: 0.1245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  56%|█████▌    | 30/54 [1:25:19<45:01, 112.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.1239\n",
      "Evaluation - Test Loss: 0.1209, R2: 0.9154\n",
      "Evaluating hyperparameters: {'batch_size': 32, 'dropout_rate': 0.2, 'hidden_units': 128, 'learning_rate': 0.1}\n",
      "Epoch [10/100], Loss: 0.1546\n",
      "Epoch [20/100], Loss: 0.1375\n",
      "Epoch [30/100], Loss: 0.1341\n",
      "Epoch [40/100], Loss: 0.1292\n",
      "Epoch [50/100], Loss: 0.1281\n",
      "Epoch [60/100], Loss: 0.1277\n",
      "Epoch [70/100], Loss: 0.1243\n",
      "Epoch [80/100], Loss: 0.1238\n",
      "Epoch [90/100], Loss: 0.1231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  57%|█████▋    | 31/54 [1:27:14<43:22, 113.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.1199\n",
      "Evaluation - Test Loss: 0.1093, R2: 0.9302\n",
      "Evaluating hyperparameters: {'batch_size': 32, 'dropout_rate': 0.2, 'hidden_units': 128, 'learning_rate': 0.01}\n",
      "Epoch [10/100], Loss: 0.1584\n",
      "Epoch [20/100], Loss: 0.1403\n",
      "Epoch [30/100], Loss: 0.1355\n",
      "Epoch [40/100], Loss: 0.1292\n",
      "Epoch [50/100], Loss: 0.1266\n",
      "Epoch [60/100], Loss: 0.1259\n",
      "Epoch [70/100], Loss: 0.1241\n",
      "Epoch [80/100], Loss: 0.1220\n",
      "Epoch [90/100], Loss: 0.1234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  59%|█████▉    | 32/54 [1:29:09<41:38, 113.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.1208\n",
      "Evaluation - Test Loss: 0.1058, R2: 0.9344\n",
      "Evaluating hyperparameters: {'batch_size': 32, 'dropout_rate': 0.2, 'hidden_units': 128, 'learning_rate': 0.001}\n",
      "Epoch [10/100], Loss: 0.1551\n",
      "Epoch [20/100], Loss: 0.1373\n",
      "Epoch [30/100], Loss: 0.1330\n",
      "Epoch [40/100], Loss: 0.1319\n",
      "Epoch [50/100], Loss: 0.1280\n",
      "Epoch [60/100], Loss: 0.1262\n",
      "Epoch [70/100], Loss: 0.1252\n",
      "Epoch [80/100], Loss: 0.1232\n",
      "Epoch [90/100], Loss: 0.1216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  61%|██████    | 33/54 [1:31:03<39:48, 113.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.1229\n",
      "Evaluation - Test Loss: 0.1064, R2: 0.9326\n",
      "Evaluating hyperparameters: {'batch_size': 32, 'dropout_rate': 0.2, 'hidden_units': 256, 'learning_rate': 0.1}\n",
      "Epoch [10/100], Loss: 0.1573\n",
      "Epoch [20/100], Loss: 0.1396\n",
      "Epoch [30/100], Loss: 0.1317\n",
      "Epoch [40/100], Loss: 0.1262\n",
      "Epoch [50/100], Loss: 0.1273\n",
      "Epoch [60/100], Loss: 0.1260\n",
      "Epoch [70/100], Loss: 0.1227\n",
      "Epoch [80/100], Loss: 0.1235\n",
      "Epoch [90/100], Loss: 0.1200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  63%|██████▎   | 34/54 [1:33:02<38:30, 115.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.1197\n",
      "Evaluation - Test Loss: 0.1033, R2: 0.9353\n",
      "Evaluating hyperparameters: {'batch_size': 32, 'dropout_rate': 0.2, 'hidden_units': 256, 'learning_rate': 0.01}\n",
      "Epoch [10/100], Loss: 0.1554\n",
      "Epoch [20/100], Loss: 0.1386\n",
      "Epoch [30/100], Loss: 0.1318\n",
      "Epoch [40/100], Loss: 0.1286\n",
      "Epoch [50/100], Loss: 0.1243\n",
      "Epoch [60/100], Loss: 0.1253\n",
      "Epoch [70/100], Loss: 0.1228\n",
      "Epoch [80/100], Loss: 0.1215\n",
      "Epoch [90/100], Loss: 0.1215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  65%|██████▍   | 35/54 [1:35:02<36:57, 116.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.1198\n",
      "Evaluation - Test Loss: 0.1078, R2: 0.9316\n",
      "Evaluating hyperparameters: {'batch_size': 32, 'dropout_rate': 0.2, 'hidden_units': 256, 'learning_rate': 0.001}\n",
      "Epoch [10/100], Loss: 0.1503\n",
      "Epoch [20/100], Loss: 0.1371\n",
      "Epoch [30/100], Loss: 0.1317\n",
      "Epoch [40/100], Loss: 0.1290\n",
      "Epoch [50/100], Loss: 0.1256\n",
      "Epoch [60/100], Loss: 0.1225\n",
      "Epoch [70/100], Loss: 0.1219\n",
      "Epoch [80/100], Loss: 0.1190\n",
      "Epoch [90/100], Loss: 0.1177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  67%|██████▋   | 36/54 [1:37:02<35:18, 117.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.1184\n",
      "Evaluation - Test Loss: 0.1013, R2: 0.9381\n",
      "Evaluating hyperparameters: {'batch_size': 64, 'dropout_rate': 0, 'hidden_units': 64, 'learning_rate': 0.1}\n",
      "Epoch [10/100], Loss: 0.1122\n",
      "Epoch [20/100], Loss: 0.1061\n",
      "Epoch [30/100], Loss: 0.1020\n",
      "Epoch [40/100], Loss: 0.0998\n",
      "Epoch [50/100], Loss: 0.0955\n",
      "Epoch [60/100], Loss: 0.0945\n",
      "Epoch [70/100], Loss: 0.0918\n",
      "Epoch [80/100], Loss: 0.0893\n",
      "Epoch [90/100], Loss: 0.0881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  69%|██████▊   | 37/54 [1:38:04<28:36, 100.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.0859\n",
      "Evaluation - Test Loss: 0.1053, R2: 0.9326\n",
      "Evaluating hyperparameters: {'batch_size': 64, 'dropout_rate': 0, 'hidden_units': 64, 'learning_rate': 0.01}\n",
      "Epoch [10/100], Loss: 0.1130\n",
      "Epoch [20/100], Loss: 0.1047\n",
      "Epoch [30/100], Loss: 0.1023\n",
      "Epoch [40/100], Loss: 0.0987\n",
      "Epoch [50/100], Loss: 0.0961\n",
      "Epoch [60/100], Loss: 0.0949\n",
      "Epoch [70/100], Loss: 0.0920\n",
      "Epoch [80/100], Loss: 0.0908\n",
      "Epoch [90/100], Loss: 0.0898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  70%|███████   | 38/54 [1:39:07<23:52, 89.51s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.0871\n",
      "Evaluation - Test Loss: 0.1168, R2: 0.9192\n",
      "Evaluating hyperparameters: {'batch_size': 64, 'dropout_rate': 0, 'hidden_units': 64, 'learning_rate': 0.001}\n",
      "Epoch [10/100], Loss: 0.1136\n",
      "Epoch [20/100], Loss: 0.1064\n",
      "Epoch [30/100], Loss: 0.1010\n",
      "Epoch [40/100], Loss: 0.0973\n",
      "Epoch [50/100], Loss: 0.0946\n",
      "Epoch [60/100], Loss: 0.0918\n",
      "Epoch [70/100], Loss: 0.0908\n",
      "Epoch [80/100], Loss: 0.0891\n",
      "Epoch [90/100], Loss: 0.0874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  72%|███████▏  | 39/54 [1:40:08<20:17, 81.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.0862\n",
      "Evaluation - Test Loss: 0.1074, R2: 0.9299\n",
      "Evaluating hyperparameters: {'batch_size': 64, 'dropout_rate': 0, 'hidden_units': 128, 'learning_rate': 0.1}\n",
      "Epoch [10/100], Loss: 0.1116\n",
      "Epoch [20/100], Loss: 0.1037\n",
      "Epoch [30/100], Loss: 0.1006\n",
      "Epoch [40/100], Loss: 0.0957\n",
      "Epoch [50/100], Loss: 0.0911\n",
      "Epoch [60/100], Loss: 0.0899\n",
      "Epoch [70/100], Loss: 0.0880\n",
      "Epoch [80/100], Loss: 0.0857\n",
      "Epoch [90/100], Loss: 0.0824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  74%|███████▍  | 40/54 [1:41:12<17:41, 75.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.0820\n",
      "Evaluation - Test Loss: 0.1058, R2: 0.9321\n",
      "Evaluating hyperparameters: {'batch_size': 64, 'dropout_rate': 0, 'hidden_units': 128, 'learning_rate': 0.01}\n",
      "Epoch [10/100], Loss: 0.1106\n",
      "Epoch [20/100], Loss: 0.1035\n",
      "Epoch [30/100], Loss: 0.0988\n",
      "Epoch [40/100], Loss: 0.0963\n",
      "Epoch [50/100], Loss: 0.0929\n",
      "Epoch [60/100], Loss: 0.0897\n",
      "Epoch [70/100], Loss: 0.0888\n",
      "Epoch [80/100], Loss: 0.0856\n",
      "Epoch [90/100], Loss: 0.0833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  76%|███████▌  | 41/54 [1:42:16<15:39, 72.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.0823\n",
      "Evaluation - Test Loss: 0.1059, R2: 0.9326\n",
      "Evaluating hyperparameters: {'batch_size': 64, 'dropout_rate': 0, 'hidden_units': 128, 'learning_rate': 0.001}\n",
      "Epoch [10/100], Loss: 0.1125\n",
      "Epoch [20/100], Loss: 0.1045\n",
      "Epoch [30/100], Loss: 0.1014\n",
      "Epoch [40/100], Loss: 0.0967\n",
      "Epoch [50/100], Loss: 0.0923\n",
      "Epoch [60/100], Loss: 0.0905\n",
      "Epoch [70/100], Loss: 0.0889\n",
      "Epoch [80/100], Loss: 0.0864\n",
      "Epoch [90/100], Loss: 0.0835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  78%|███████▊  | 42/54 [1:43:19<13:56, 69.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.0825\n",
      "Evaluation - Test Loss: 0.1063, R2: 0.9302\n",
      "Evaluating hyperparameters: {'batch_size': 64, 'dropout_rate': 0, 'hidden_units': 256, 'learning_rate': 0.1}\n",
      "Epoch [10/100], Loss: 0.1129\n",
      "Epoch [20/100], Loss: 0.1030\n",
      "Epoch [30/100], Loss: 0.0998\n",
      "Epoch [40/100], Loss: 0.0961\n",
      "Epoch [50/100], Loss: 0.0908\n",
      "Epoch [60/100], Loss: 0.0885\n",
      "Epoch [70/100], Loss: 0.0858\n",
      "Epoch [80/100], Loss: 0.0853\n",
      "Epoch [90/100], Loss: 0.0810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  80%|███████▉  | 43/54 [1:44:27<12:38, 68.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.0805\n",
      "Evaluation - Test Loss: 0.1067, R2: 0.9306\n",
      "Evaluating hyperparameters: {'batch_size': 64, 'dropout_rate': 0, 'hidden_units': 256, 'learning_rate': 0.01}\n",
      "Epoch [10/100], Loss: 0.1127\n",
      "Epoch [20/100], Loss: 0.1061\n",
      "Epoch [30/100], Loss: 0.0989\n",
      "Epoch [40/100], Loss: 0.0962\n",
      "Epoch [50/100], Loss: 0.0917\n",
      "Epoch [60/100], Loss: 0.0887\n",
      "Epoch [70/100], Loss: 0.0857\n",
      "Epoch [80/100], Loss: 0.0836\n",
      "Epoch [90/100], Loss: 0.0797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  81%|████████▏ | 44/54 [1:45:34<11:24, 68.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.0777\n",
      "Evaluation - Test Loss: 0.1080, R2: 0.9300\n",
      "Evaluating hyperparameters: {'batch_size': 64, 'dropout_rate': 0, 'hidden_units': 256, 'learning_rate': 0.001}\n",
      "Epoch [10/100], Loss: 0.1097\n",
      "Epoch [20/100], Loss: 0.1023\n",
      "Epoch [30/100], Loss: 0.1000\n",
      "Epoch [40/100], Loss: 0.0958\n",
      "Epoch [50/100], Loss: 0.0922\n",
      "Epoch [60/100], Loss: 0.0898\n",
      "Epoch [70/100], Loss: 0.0870\n",
      "Epoch [80/100], Loss: 0.0858\n",
      "Epoch [90/100], Loss: 0.0821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  83%|████████▎ | 45/54 [1:46:41<10:11, 67.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.0795\n",
      "Evaluation - Test Loss: 0.1072, R2: 0.9318\n",
      "Evaluating hyperparameters: {'batch_size': 64, 'dropout_rate': 0.2, 'hidden_units': 64, 'learning_rate': 0.1}\n",
      "Epoch [10/100], Loss: 0.1611\n",
      "Epoch [20/100], Loss: 0.1467\n",
      "Epoch [30/100], Loss: 0.1391\n",
      "Epoch [40/100], Loss: 0.1311\n",
      "Epoch [50/100], Loss: 0.1291\n",
      "Epoch [60/100], Loss: 0.1261\n",
      "Epoch [70/100], Loss: 0.1240\n",
      "Epoch [80/100], Loss: 0.1231\n",
      "Epoch [90/100], Loss: 0.1244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  85%|████████▌ | 46/54 [1:47:46<08:58, 67.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.1226\n",
      "Evaluation - Test Loss: 0.1040, R2: 0.9350\n",
      "Evaluating hyperparameters: {'batch_size': 64, 'dropout_rate': 0.2, 'hidden_units': 64, 'learning_rate': 0.01}\n",
      "Epoch [10/100], Loss: 0.1611\n",
      "Epoch [20/100], Loss: 0.1458\n",
      "Epoch [30/100], Loss: 0.1390\n",
      "Epoch [40/100], Loss: 0.1309\n",
      "Epoch [50/100], Loss: 0.1258\n",
      "Epoch [60/100], Loss: 0.1264\n",
      "Epoch [70/100], Loss: 0.1259\n",
      "Epoch [80/100], Loss: 0.1239\n",
      "Epoch [90/100], Loss: 0.1225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  87%|████████▋ | 47/54 [1:48:52<07:47, 66.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.1235\n",
      "Evaluation - Test Loss: 0.1067, R2: 0.9324\n",
      "Evaluating hyperparameters: {'batch_size': 64, 'dropout_rate': 0.2, 'hidden_units': 64, 'learning_rate': 0.001}\n",
      "Epoch [10/100], Loss: 0.1651\n",
      "Epoch [20/100], Loss: 0.1447\n",
      "Epoch [30/100], Loss: 0.1366\n",
      "Epoch [40/100], Loss: 0.1315\n",
      "Epoch [50/100], Loss: 0.1282\n",
      "Epoch [60/100], Loss: 0.1275\n",
      "Epoch [70/100], Loss: 0.1244\n",
      "Epoch [80/100], Loss: 0.1232\n",
      "Epoch [90/100], Loss: 0.1242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  89%|████████▉ | 48/54 [1:49:57<06:38, 66.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.1215\n",
      "Evaluation - Test Loss: 0.1045, R2: 0.9352\n",
      "Evaluating hyperparameters: {'batch_size': 64, 'dropout_rate': 0.2, 'hidden_units': 128, 'learning_rate': 0.1}\n",
      "Epoch [10/100], Loss: 0.1611\n",
      "Epoch [20/100], Loss: 0.1438\n",
      "Epoch [30/100], Loss: 0.1351\n",
      "Epoch [40/100], Loss: 0.1302\n",
      "Epoch [50/100], Loss: 0.1257\n",
      "Epoch [60/100], Loss: 0.1252\n",
      "Epoch [70/100], Loss: 0.1217\n",
      "Epoch [80/100], Loss: 0.1206\n",
      "Epoch [90/100], Loss: 0.1208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  91%|█████████ | 49/54 [1:51:05<05:33, 66.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.1197\n",
      "Evaluation - Test Loss: 0.1040, R2: 0.9358\n",
      "Evaluating hyperparameters: {'batch_size': 64, 'dropout_rate': 0.2, 'hidden_units': 128, 'learning_rate': 0.01}\n",
      "Epoch [10/100], Loss: 0.1642\n",
      "Epoch [20/100], Loss: 0.1450\n",
      "Epoch [30/100], Loss: 0.1365\n",
      "Epoch [40/100], Loss: 0.1287\n",
      "Epoch [50/100], Loss: 0.1264\n",
      "Epoch [60/100], Loss: 0.1221\n",
      "Epoch [70/100], Loss: 0.1259\n",
      "Epoch [80/100], Loss: 0.1220\n",
      "Epoch [90/100], Loss: 0.1215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  93%|█████████▎| 50/54 [1:52:12<04:27, 66.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.1204\n",
      "Evaluation - Test Loss: 0.1026, R2: 0.9375\n",
      "Evaluating hyperparameters: {'batch_size': 64, 'dropout_rate': 0.2, 'hidden_units': 128, 'learning_rate': 0.001}\n",
      "Epoch [10/100], Loss: 0.1634\n",
      "Epoch [20/100], Loss: 0.1467\n",
      "Epoch [30/100], Loss: 0.1361\n",
      "Epoch [40/100], Loss: 0.1314\n",
      "Epoch [50/100], Loss: 0.1237\n",
      "Epoch [60/100], Loss: 0.1233\n",
      "Epoch [70/100], Loss: 0.1215\n",
      "Epoch [80/100], Loss: 0.1205\n",
      "Epoch [90/100], Loss: 0.1195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  94%|█████████▍| 51/54 [1:53:21<03:22, 67.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.1183\n",
      "Evaluation - Test Loss: 0.1047, R2: 0.9349\n",
      "Evaluating hyperparameters: {'batch_size': 64, 'dropout_rate': 0.2, 'hidden_units': 256, 'learning_rate': 0.1}\n",
      "Epoch [10/100], Loss: 0.1599\n",
      "Epoch [20/100], Loss: 0.1404\n",
      "Epoch [30/100], Loss: 0.1330\n",
      "Epoch [40/100], Loss: 0.1282\n",
      "Epoch [50/100], Loss: 0.1253\n",
      "Epoch [60/100], Loss: 0.1240\n",
      "Epoch [70/100], Loss: 0.1222\n",
      "Epoch [80/100], Loss: 0.1202\n",
      "Epoch [90/100], Loss: 0.1188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  96%|█████████▋| 52/54 [1:54:33<02:17, 68.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.1182\n",
      "Evaluation - Test Loss: 0.1032, R2: 0.9361\n",
      "Evaluating hyperparameters: {'batch_size': 64, 'dropout_rate': 0.2, 'hidden_units': 256, 'learning_rate': 0.01}\n",
      "Epoch [10/100], Loss: 0.1623\n",
      "Epoch [20/100], Loss: 0.1399\n",
      "Epoch [30/100], Loss: 0.1297\n",
      "Epoch [40/100], Loss: 0.1229\n",
      "Epoch [50/100], Loss: 0.1219\n",
      "Epoch [60/100], Loss: 0.1185\n",
      "Epoch [70/100], Loss: 0.1179\n",
      "Epoch [80/100], Loss: 0.1168\n",
      "Epoch [90/100], Loss: 0.1173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  98%|█████████▊| 53/54 [1:55:44<01:09, 69.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.1134\n",
      "Evaluation - Test Loss: 0.1032, R2: 0.9357\n",
      "Evaluating hyperparameters: {'batch_size': 64, 'dropout_rate': 0.2, 'hidden_units': 256, 'learning_rate': 0.001}\n",
      "Epoch [10/100], Loss: 0.1546\n",
      "Epoch [20/100], Loss: 0.1345\n",
      "Epoch [30/100], Loss: 0.1274\n",
      "Epoch [40/100], Loss: 0.1219\n",
      "Epoch [50/100], Loss: 0.1225\n",
      "Epoch [60/100], Loss: 0.1176\n",
      "Epoch [70/100], Loss: 0.1153\n",
      "Epoch [80/100], Loss: 0.1146\n",
      "Epoch [90/100], Loss: 0.1142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress: 100%|██████████| 54/54 [1:56:56<00:00, 129.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.1136\n",
      "Evaluation - Test Loss: 0.1046, R2: 0.9351\n",
      "Best parameters found: {'batch_size': 16, 'dropout_rate': 0, 'hidden_units': 128, 'learning_rate': 0.1}\n",
      "Best MAE: 0.1012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# num_epochs = 100\n",
    "# best_params = None\n",
    "# best_results = {'mae': float('inf')}\n",
    "\n",
    "# for params in tqdm(grid, desc=\"Grid Search Progress\", leave=True):\n",
    "#     print(f\"Evaluating hyperparameters: {params}\")\n",
    "    \n",
    "#     model, metrics = train_model_regular(train_scaled_df, features_dict, num_epochs, params)\n",
    "#     mae = metrics['mae']\n",
    "    \n",
    "#     if best_params is None or mae < best_results['mae']:\n",
    "#         best_results = {\n",
    "#             'mae': mae,\n",
    "#             'metrics': metrics\n",
    "#         }\n",
    "#         best_params = params\n",
    "\n",
    "# print(f\"Best parameters found: {best_params}\")\n",
    "# print(f\"Best MAE: {best_results['mae']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50], Loss: 0.1228\n",
      "Epoch [20/50], Loss: 0.1116\n",
      "Epoch [30/50], Loss: 0.1070\n",
      "Epoch [40/50], Loss: 0.1042\n",
      "Epoch [50/50], Loss: 0.1029\n",
      "Evaluation - Test Loss: 0.1054, R2: 0.9339\n"
     ]
    }
   ],
   "source": [
    "best_params_bmlp = {'batch_size': 16, 'dropout_rate': 0, 'hidden_units': 128, 'learning_rate': 0.1}\n",
    "\n",
    "best_mlp = train_model_regular(train_scaled_df, features_dict, 100, best_params_bmlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_mlp[0], 'bmlp_baseline.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - MAE: 0.1043, MSE: 0.0213, RMSE: 0.1459, R2: 0.9358, TOL90: 0.2363\n",
      "Fold 2 - MAE: 0.0877, MSE: 0.0166, RMSE: 0.1287, R2: 0.9509, TOL90: 0.2100\n",
      "Fold 3 - MAE: 0.0892, MSE: 0.0164, RMSE: 0.1281, R2: 0.9514, TOL90: 0.2175\n",
      "Fold 4 - MAE: 0.0950, MSE: 0.0198, RMSE: 0.1407, R2: 0.9415, TOL90: 0.2226\n",
      "Fold 5 - MAE: 0.0886, MSE: 0.0166, RMSE: 0.1287, R2: 0.9511, TOL90: 0.2133\n",
      "\n",
      "Average Metrics across folds:\n",
      "MAE: 0.0930 ± 0.0062\n",
      "MSE: 0.0181 ± 0.0020\n",
      "RMSE: 0.1345 ± 0.0074\n",
      "R2: 0.9461 ± 0.0064\n",
      "TOL90: 0.2199 ± 0.0092\n"
     ]
    }
   ],
   "source": [
    "# Perform cross validation\n",
    "metrics = load_and_evaluate_model(\n",
    "    data_df=train_scaled_df,\n",
    "    features_dict=features_dict,\n",
    "    model_path='bmlp_baseline.pth',\n",
    "    n_splits=5,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tol90(y_true, y_pred):\n",
    "    errors = np.abs(y_true - y_pred)\n",
    "    return np.percentile(errors, 90)\n",
    "\n",
    "def density_scatter(x, y, bins=30, **kwargs):\n",
    "    \"\"\"Create a density scatter plot\"\"\"\n",
    "    # Ensure inputs are 1D arrays\n",
    "    x = np.asarray(x).flatten()\n",
    "    y = np.asarray(y).flatten()\n",
    "    data, x_e, y_e = np.histogram2d(x, y, bins=bins)\n",
    "    z = interpn((0.5*(x_e[1:] + x_e[:-1]), 0.5*(y_e[1:] + y_e[:-1])),\n",
    "                data.T/data.max(),\n",
    "                np.vstack([x,y]).T,\n",
    "                method=\"splinef2d\",\n",
    "                bounds_error=False)\n",
    "    z[np.where(np.isnan(z))] = 0.0\n",
    "    idx = z.argsort()\n",
    "    x, y, z = x[idx], y[idx], z[idx]\n",
    "    plt.scatter(x, y, c=z, **kwargs)\n",
    "\n",
    "def plot_predicted_vs_actual(model, df, features_dict=None, model_type=None, target_column='r_value', title=None, figsize=(10, 8)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Get actual values\n",
    "    actual = df[target_column].values\n",
    "    \n",
    "    # Get predictions based on model type\n",
    "    if model_type == 'pytorch_mlp':\n",
    "        # Prepare feature arrays for PyTorch MLP\n",
    "        feature_arrays = {}\n",
    "        for category in features_dict.keys():\n",
    "            available_features = [col for col in features_dict[category] \n",
    "                                if col in df.columns]\n",
    "            if available_features:\n",
    "                scaler = StandardScaler()\n",
    "                feature_arrays[category] = scaler.fit_transform(df[available_features].values)\n",
    "            else:\n",
    "                feature_arrays[category] = np.zeros((len(df), 0))\n",
    "        \n",
    "        # Convert features to tensors\n",
    "        input_tensors = {\n",
    "            category: torch.FloatTensor(arr) for category, arr in feature_arrays.items()\n",
    "        }\n",
    "        \n",
    "        # Make predictions\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            predicted = model(**input_tensors).numpy().flatten()\n",
    "            \n",
    "    elif model_type == 'keras':\n",
    "        X = df.drop([target_column], axis=1).values\n",
    "        X = X.astype('float32')\n",
    "        predicted = model.predict(X).flatten()\n",
    "        \n",
    "    else:  # sklearn\n",
    "        X = df.drop([target_column], axis=1)\n",
    "        predicted = model['model'].predict(X)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    r2 = r2_score(actual, predicted)\n",
    "    mse = mean_squared_error(actual, predicted)\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    tol90_value = tol90(actual, predicted)\n",
    "    \n",
    "    # Create density scatter plot\n",
    "    density_scatter(actual, predicted, bins=30, alpha=0.6)\n",
    "    \n",
    "    # Plot perfect prediction line\n",
    "    min_val = min(actual.min(), predicted.min())\n",
    "    max_val = max(actual.max(), predicted.max())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "    \n",
    "    # Plot tolerance lines\n",
    "    plt.plot([min_val, max_val], [min_val + tol90_value, max_val + tol90_value],\n",
    "             'g--', lw=1.5, label=f'+tol90 ({tol90_value:.4f})')\n",
    "    plt.plot([min_val, max_val], [min_val - tol90_value, max_val - tol90_value],\n",
    "             'b--', lw=1.5, label=f'-tol90 ({tol90_value:.4f})')\n",
    "    \n",
    "    # Set plot limits with buffer\n",
    "    buffer = 0.2\n",
    "    x_min = actual.min() - (actual.max() - actual.min()) * buffer\n",
    "    x_max = actual.max() + (actual.max() - actual.min()) * buffer\n",
    "    y_min = predicted.min() - (predicted.max() - predicted.min()) * buffer\n",
    "    y_max = predicted.max() + (predicted.max() - predicted.min()) * buffer\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    \n",
    "    # Annotate with metrics\n",
    "    plt.text(0.05, 0.95,\n",
    "             f\"R²: {r2:.4f}\\n\" +\n",
    "             f\"MSE: {mse:.4f}\\n\" +\n",
    "             f\"MAE: {mae:.4f}\\n\" +\n",
    "             f\"tol90: {tol90_value:.4f}\",\n",
    "             transform=plt.gca().transAxes,\n",
    "             verticalalignment='top',\n",
    "             bbox=dict(facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # Labeling\n",
    "    plt.xlabel('Actual Value')\n",
    "    plt.ylabel('Predicted Value')\n",
    "    if title is None:\n",
    "        title = f'Predicted vs Actual {target_column}'\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.axis('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'R2': r2,\n",
    "        'MSE': mse,\n",
    "        'MAE': mae,\n",
    "        'tol90': tol90_value\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predicted_vs_actual(rfr_results['model'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
