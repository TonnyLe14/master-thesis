{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from xgboost import XGBRegressor\n",
    "from tensorflow import keras\n",
    "from scipy.interpolate import interpn\n",
    "from tqdm import tqdm\n",
    "from joblib import dump, load\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 25 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/openpyxl/worksheet/_read_only.py:79: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  for idx, row in parser.parse():\n"
     ]
    }
   ],
   "source": [
    "from load_data import process_steel_data\n",
    "\n",
    "full_path = 'data/'\n",
    "path = 'data/MDC_Data_Descriptions_MeCoMeP-r-value.xlsx'\n",
    "correlation_rate = 0.2\n",
    "dvl_line = 1\n",
    "\n",
    "df = process_steel_data(full_path, path, correlation_rate, dvl_line, model_output=False)\n",
    "df = pd.get_dummies(df, columns=['steel_family'], prefix='steel').drop(['steel_grade'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(df, binary_prefix='steel_'):\n",
    "\n",
    "    # Identify binary columns\n",
    "    binary_columns = [col for col in df.columns if col.startswith(binary_prefix)]\n",
    "    \n",
    "    # Identify columns to scale (non-binary columns)\n",
    "    columns_to_scale = [col for col in df.columns if col not in binary_columns + ['r_value']]\n",
    "    \n",
    "    # Scale numerical features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(df[columns_to_scale])\n",
    "    \n",
    "    # Create new dataframe with scaled data\n",
    "    scaled_df = pd.DataFrame(scaled_data, columns=columns_to_scale)\n",
    "    \n",
    "    # Add back binary columns\n",
    "    for col in binary_columns:\n",
    "        scaled_df[col] = df[col].values\n",
    "    \n",
    "    # Add target variable if present\n",
    "    if 'r_value' in df.columns:\n",
    "        scaled_df['r_value'] = df['r_value'].values\n",
    "    \n",
    "    return scaled_df, scaler\n",
    "\n",
    "def train_model_with_cv_gridsearch(df, model, param_grid=None, n_splits=5, random_state=42, use_grid_search=True, model_params=None):\n",
    "    \"\"\"\n",
    "    Train a model with optional grid search and cross-validation\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Input dataframe\n",
    "    model : estimator object\n",
    "        Machine learning model to train\n",
    "    param_grid : dict, optional\n",
    "        Parameter grid for grid search (used if use_grid_search=True)\n",
    "    n_splits : int, optional\n",
    "        Number of cross-validation splits (default: 5)\n",
    "    random_state : int, optional\n",
    "        Random state for reproducibility (default: 42)\n",
    "    use_grid_search : bool, optional\n",
    "        Whether to perform grid search (default: True)\n",
    "    model_params : dict, optional\n",
    "        Direct model parameters to use if use_grid_search=False\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict containing model results and performance metrics\n",
    "    \"\"\"\n",
    "    # Prepare X and y\n",
    "    X = df.drop(['r_value'], axis=1)\n",
    "    y = df['r_value']\n",
    "    \n",
    "    # Initialize cross-validation\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # Initialize metrics storage\n",
    "    cv_scores = {\n",
    "        'mae': [],\n",
    "        'mse': [],\n",
    "        'r2': []\n",
    "    }\n",
    "    \n",
    "    # Determine model parameters\n",
    "    if use_grid_search:\n",
    "        if param_grid is None:\n",
    "            raise ValueError(\"param_grid must be provided when use_grid_search is True\")\n",
    "        \n",
    "        # Initialize GridSearchCV\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=model,\n",
    "            param_grid=param_grid,\n",
    "            cv=n_splits,\n",
    "            scoring='neg_mean_absolute_error',\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Fit GridSearchCV\n",
    "        print(\"Performing GridSearch...\")\n",
    "        grid_search.fit(X, y)\n",
    "        print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
    "        best_model = grid_search.best_estimator_\n",
    "    else:\n",
    "        # Use directly specified parameters or default model\n",
    "        if model_params:\n",
    "            best_model = type(model)(**model_params)\n",
    "        else:\n",
    "            best_model = model\n",
    "        \n",
    "        grid_search = None\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    print(\"\\nPerforming cross-validation...\")\n",
    "    pbar = tqdm(enumerate(kf.split(X), 1),\n",
    "                total=n_splits,\n",
    "                desc=\"Cross-validation\",\n",
    "                leave=True)\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in pbar:\n",
    "        # Split data\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        # Train model\n",
    "        best_model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = best_model.predict(X_val)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "        mse = mean_squared_error(y_val, y_pred)\n",
    "        r2 = r2_score(y_val, y_pred)\n",
    "        \n",
    "        cv_scores['mae'].append(mae)\n",
    "        cv_scores['mse'].append(mse)\n",
    "        cv_scores['r2'].append(r2)\n",
    "        \n",
    "        # Update progress bar description\n",
    "        pbar.set_description(\n",
    "            f\"Fold {fold} - MAE: {mae:.4f}, MSE: {mse:.4f}, R2: {r2:.4f}\"\n",
    "        )\n",
    "    \n",
    "    # Prepare results\n",
    "    results = {\n",
    "        'model': best_model,\n",
    "        'best_params': grid_search.best_params_ if use_grid_search else model_params or {},\n",
    "        'avg_mae': np.mean(cv_scores['mae']),\n",
    "        'std_mae': np.std(cv_scores['mae']),\n",
    "        'avg_mse': np.mean(cv_scores['mse']),\n",
    "        'std_mse': np.std(cv_scores['mse']),\n",
    "        'avg_r2': np.mean(cv_scores['r2']),\n",
    "        'std_r2': np.std(cv_scores['r2']),\n",
    "        'cv_scores': cv_scores,\n",
    "        'grid_search_results': grid_search.cv_results_ if use_grid_search else None\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def report_cv_results(results):\n",
    "    print(\"\\nCross-Validation Results:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Best Parameters: {results['best_params']}\")\n",
    "    print(f\"Average MAE: {results['avg_mae']:.4f} ± {results['std_mae']:.4f}\")\n",
    "    print(f\"Average MSE: {results['avg_mse']:.4f} ± {results['std_mse']:.4f}\")\n",
    "    print(f\"Average R2: {results['avg_r2']:.4f} ± {results['std_r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_scaled_df, scaler = scale_data(train_df)\n",
    "binary_columns = [col for col in test_df.columns if col.startswith('steel_')]\n",
    "columns_to_scale = [col for col in test_df.columns if col not in binary_columns + ['r_value']]\n",
    "scaled_test_data = scaler.transform(test_df[columns_to_scale])\n",
    "test_scaled_df = pd.DataFrame(scaled_test_data, columns=columns_to_scale)\n",
    "for col in binary_columns:\n",
    "    test_scaled_df[col] = test_df[col].values\n",
    "if 'r_value' in test_df.columns:\n",
    "    test_scaled_df['r_value'] = test_df['r_value'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing GridSearch...\n",
      "\n",
      "Best parameters: {'n_estimators': 350}\n",
      "\n",
      "Performing cross-validation with best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 - MAE: 0.0941, MSE: 0.0180, R2: 0.9468: 100%|██████████| 5/5 [04:20<00:00, 52.16s/it]\n"
     ]
    }
   ],
   "source": [
    "rfr = RandomForestRegressor(random_state=42)\n",
    "rfr_param_grid = {\n",
    "    'n_estimators': [350]\n",
    "}\n",
    "\n",
    "rfr_results = train_model_with_cv_gridsearch(\n",
    "    df=train_scaled_df,\n",
    "    model=rfr,\n",
    "    param_grid=rfr_param_grid,\n",
    "    n_splits=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing GridSearch...\n",
      "\n",
      "Best parameters: {'eta': 0.1, 'lambda': 1, 'max_depth': 8}\n",
      "\n",
      "Performing cross-validation with best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 - MAE: 0.0969, MSE: 0.0189, R2: 0.9443: 100%|██████████| 5/5 [00:02<00:00,  1.99it/s]\n"
     ]
    }
   ],
   "source": [
    "xgb_model = XGBRegressor(random_state=42)\n",
    "\n",
    "xgb_param_grid = {\n",
    "    'eta': [0.01, 0.05, 0.1, 0.2, 0.3, 0.4],\n",
    "    'lambda': [0, 0.01, 0.1, 1, 10, 50],\n",
    "    'max_depth': [3, 4, 5, 6, 7, 8]\n",
    "}\n",
    "\n",
    "xgb_results = train_model_with_cv_gridsearch(\n",
    "    df=train_scaled_df,\n",
    "    model=xgb_model,\n",
    "    param_grid=xgb_param_grid,\n",
    "    n_splits=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing cross-validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 - MAE: 0.1037, MSE: 0.0204, R2: 0.9399: 100%|██████████| 5/5 [27:16<00:00, 327.35s/it]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.gaussian_process.kernels import Matern, WhiteKernel\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "\n",
    "results_without_grid = train_model_with_cv_gridsearch(\n",
    "    df=train_scaled_df, \n",
    "    model=GaussianProcessRegressor(), \n",
    "    use_grid_search=False,\n",
    "    model_params={'kernel': 1**2 * Matern(length_scale=1, nu=1.5) + WhiteKernel(noise_level=1)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing GridSearch...\n",
      "\n",
      "Best parameters: {'leaf_size': 20, 'n_neighbors': 9, 'weights': 'distance'}\n",
      "\n",
      "Performing cross-validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 - MAE: 0.1075, MSE: 0.0224, R2: 0.9338: 100%|██████████| 5/5 [00:00<00:00, 20.07it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "knn_model = KNeighborsRegressor()\n",
    "\n",
    "knn_param_grid = {\n",
    "    'n_neighbors': list(range(2, 15)),\n",
    "    'leaf_size': [20, 30, 40, 50],\n",
    "    'weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "knn_results = train_model_with_cv_gridsearch(\n",
    "    df=train_scaled_df,\n",
    "    model=knn_model,\n",
    "    param_grid=knn_param_grid\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_saved_model_architecture(saved_model_path, df, target_column='r_value', n_splits=5, \n",
    "                               epochs=100, batch_size=32, random_state=42):\n",
    "    \"\"\"\n",
    "    Perform cross-validation using architecture and parameters from a saved model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    saved_model_path : str\n",
    "        Path to saved .h5 model file\n",
    "    df : pandas DataFrame\n",
    "        Input data\n",
    "    target_column : str\n",
    "        Name of target column\n",
    "    n_splits : int\n",
    "        Number of CV folds\n",
    "    epochs : int\n",
    "        Number of training epochs\n",
    "    batch_size : int\n",
    "        Batch size for training\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict containing CV metrics\n",
    "    \"\"\"\n",
    "    # Load saved model to get architecture and parameters\n",
    "    base_model = tf.keras.models.load_model(saved_model_path)\n",
    "    \n",
    "            # Get learning rate from saved model and convert to Python float\n",
    "    learning_rate = float(base_model.optimizer.learning_rate.numpy())\n",
    "    \n",
    "    # Prepare data\n",
    "    X = df.drop([target_column], axis=1)\n",
    "    y = df[target_column]\n",
    "    \n",
    "    # Initialize KFold\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # Storage for CV metrics\n",
    "    cv_scores = {\n",
    "        'mae': [],\n",
    "        'mse': [],\n",
    "        'r2': []\n",
    "    }\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    print(\"\\nPerforming cross-validation...\")\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X), 1):\n",
    "        print(f\"\\nFold {fold}/{n_splits}\")\n",
    "        \n",
    "        # Clear previous model from memory\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        # Convert to float32\n",
    "        X_train = np.array(X_train, dtype=np.float32)\n",
    "        y_train = np.array(y_train, dtype=np.float32)\n",
    "        X_val = np.array(X_val, dtype=np.float32)\n",
    "        y_val = np.array(y_val, dtype=np.float32)\n",
    "        \n",
    "        # Create new model with same architecture\n",
    "        model = tf.keras.models.clone_model(base_model)\n",
    "        \n",
    "        # Compile with same optimizer type and learning rate\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='mean_absolute_error',\n",
    "            metrics=['mae', 'mse']\n",
    "        )\n",
    "        \n",
    "        # Early stopping\n",
    "        early_stopping = keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            min_delta=1e-4\n",
    "        )\n",
    "        \n",
    "        # Create TF datasets\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "        train_dataset = (train_dataset\n",
    "            .batch(batch_size, drop_remainder=True)\n",
    "            .repeat())\n",
    "        \n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "        val_dataset = val_dataset.batch(batch_size)\n",
    "        \n",
    "        steps_per_epoch = len(X_train) // batch_size\n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            train_dataset,\n",
    "            validation_data=val_dataset,\n",
    "            epochs=epochs,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_val, verbose=0)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "        mse = mean_squared_error(y_val, y_pred)\n",
    "        r2 = r2_score(y_val, y_pred)\n",
    "        \n",
    "        cv_scores['mae'].append(mae)\n",
    "        cv_scores['mse'].append(mse)\n",
    "        cv_scores['r2'].append(r2)\n",
    "        \n",
    "        print(f\"Fold {fold} - MAE: {mae:.4f}, MSE: {mse:.4f}, R2: {r2:.4f}\")\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    results = {\n",
    "        'avg_mae': np.mean(cv_scores['mae']),\n",
    "        'std_mae': np.std(cv_scores['mae']),\n",
    "        'avg_mse': np.mean(cv_scores['mse']),\n",
    "        'std_mse': np.std(cv_scores['mse']),\n",
    "        'avg_r2': np.mean(cv_scores['r2']),\n",
    "        'std_r2': np.std(cv_scores['r2']),\n",
    "        'cv_scores': cv_scores\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteelPropertiesANN:\n",
    "    def __init__(self, input_dim, target_column):\n",
    "        self.input_dim = input_dim\n",
    "        self.target_column = target_column\n",
    "        self.best_model = None\n",
    "        self.best_params = None\n",
    "        self.best_score = float('inf')\n",
    "\n",
    "    def build_model(self, config):\n",
    "        hidden_layers = config['layers']\n",
    "        learning_rate = config['learning_rate']\n",
    "        l2_strength = config['l2_regularization']\n",
    "        \n",
    "        model = keras.Sequential()\n",
    "        model.add(keras.layers.Input(shape=(self.input_dim,)))\n",
    "        \n",
    "        for units, activation in hidden_layers:\n",
    "            model.add(keras.layers.Dense(\n",
    "                units=units,\n",
    "                activation=activation,\n",
    "                kernel_regularizer=keras.regularizers.l2(l2_strength)\n",
    "            ))\n",
    "        \n",
    "        model.add(keras.layers.Dense(1))\n",
    "        \n",
    "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "            initial_learning_rate=learning_rate,\n",
    "            decay_steps=100,\n",
    "            decay_rate=0.9,\n",
    "            staircase=True\n",
    "        )\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=lr_schedule, clipnorm=1.0),\n",
    "            loss='mean_absolute_error',\n",
    "            metrics=['mae', 'mse']\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def generate_grid_configs(self, \n",
    "        layer_options=[(64, 'relu'), (128, 'relu'), (256, 'relu')],\n",
    "        layer_depths=[2, 3, 4],\n",
    "        learning_rates=[1e-2, 1e-3, 1e-4],\n",
    "        l2_regularization=[1e-3, 1e-4, 1e-5],\n",
    "        batch_sizes=[16, 32, 64]\n",
    "    ):\n",
    "        from itertools import product\n",
    "        grid_configs = []\n",
    "        \n",
    "        for depth in layer_depths:\n",
    "            for lr in learning_rates:\n",
    "                for l2_reg in l2_regularization:\n",
    "                    for batch_size in batch_sizes:\n",
    "                        layer_combinations = list(product(layer_options, repeat=depth))\n",
    "                        for layers in layer_combinations:\n",
    "                            config = {\n",
    "                                'layers': layers,\n",
    "                                'learning_rate': lr,\n",
    "                                'l2_regularization': l2_reg,\n",
    "                                'batch_size': batch_size\n",
    "                            }\n",
    "                            grid_configs.append(config)\n",
    "        \n",
    "        return grid_configs\n",
    "\n",
    "    def grid_search(self, train_scaled_df, grid_configs=None, epochs=100, max_configs=None):\n",
    "        # Split training data into training and validation sets\n",
    "        train_data, val_data = train_test_split(train_scaled_df, test_size=0.2, random_state=42)\n",
    "    \n",
    "        if grid_configs is None:\n",
    "            grid_configs = self.generate_grid_configs()\n",
    "        \n",
    "        X_train = train_data.drop([self.target_column], axis=1)\n",
    "        y_train = train_data[self.target_column]\n",
    "        X_val = val_data.drop([self.target_column], axis=1)\n",
    "        y_val = val_data[self.target_column]\n",
    "        \n",
    "        if max_configs:\n",
    "            grid_configs = grid_configs[:max_configs]\n",
    "        \n",
    "        results = []\n",
    "        for config in tqdm(grid_configs, desc=\"Training models\"):\n",
    "            tf.keras.backend.clear_session()\n",
    "            model = self.build_model(config)\n",
    "            batch_size = min(config['batch_size'], len(X_train))\n",
    "            \n",
    "            early_stopping = keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss', \n",
    "                patience=10, \n",
    "                restore_best_weights=True,\n",
    "                min_delta=1e-4\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                X_train = np.array(X_train, dtype=np.float32)\n",
    "                y_train = np.array(y_train, dtype=np.float32)\n",
    "                X_val = np.array(X_val, dtype=np.float32)\n",
    "                y_val = np.array(y_val, dtype=np.float32)\n",
    "\n",
    "                train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "                train_dataset = (train_dataset\n",
    "                    .batch(batch_size, drop_remainder=True)\n",
    "                    .repeat())\n",
    "                \n",
    "                val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "                val_dataset = val_dataset.batch(batch_size)\n",
    "                \n",
    "                steps_per_epoch = len(X_train) // batch_size\n",
    "                \n",
    "                history = model.fit(\n",
    "                    train_dataset,\n",
    "                    validation_data=val_dataset,\n",
    "                    epochs=epochs,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    callbacks=[early_stopping],\n",
    "                    verbose=0\n",
    "                )\n",
    "                \n",
    "                val_loss = model.evaluate(X_val, y_val, verbose=0)[0]\n",
    "                \n",
    "                result_entry = config.copy()\n",
    "                result_entry.update({'val_loss': val_loss})\n",
    "                results.append(result_entry)\n",
    "                \n",
    "                if val_loss < self.best_score:\n",
    "                    self.best_score = val_loss\n",
    "                    self.best_model = model\n",
    "                    self.best_params = config\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error with config {config}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return self.best_model, self.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing cross-validation...\n",
      "\n",
      "Fold 1/5\n",
      "Epoch 1/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 972us/step - loss: 0.3032 - mae: 0.2864 - mse: 0.2022 - val_loss: 0.1535 - val_mae: 0.1375 - val_mse: 0.0332\n",
      "Epoch 2/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 0.1515 - mae: 0.1358 - mse: 0.0322 - val_loss: 0.1416 - val_mae: 0.1265 - val_mse: 0.0284\n",
      "Epoch 3/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 0.1428 - mae: 0.1279 - mse: 0.0292 - val_loss: 0.1391 - val_mae: 0.1247 - val_mse: 0.0276\n",
      "Epoch 4/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 0.1354 - mae: 0.1213 - mse: 0.0267 - val_loss: 0.1378 - val_mae: 0.1241 - val_mse: 0.0269\n",
      "Epoch 5/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 0.1321 - mae: 0.1186 - mse: 0.0260 - val_loss: 0.1313 - val_mae: 0.1183 - val_mse: 0.0252\n",
      "Epoch 6/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 821us/step - loss: 0.1279 - mae: 0.1151 - mse: 0.0246 - val_loss: 0.1287 - val_mae: 0.1162 - val_mse: 0.0239\n",
      "Epoch 7/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 0.1244 - mae: 0.1120 - mse: 0.0236 - val_loss: 0.1327 - val_mae: 0.1207 - val_mse: 0.0259\n",
      "Epoch 8/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 0.1244 - mae: 0.1125 - mse: 0.0239 - val_loss: 0.1279 - val_mae: 0.1163 - val_mse: 0.0242\n",
      "Epoch 9/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 0.1228 - mae: 0.1113 - mse: 0.0236 - val_loss: 0.1325 - val_mae: 0.1213 - val_mse: 0.0261\n",
      "Epoch 10/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 0.1201 - mae: 0.1089 - mse: 0.0230 - val_loss: 0.1308 - val_mae: 0.1199 - val_mse: 0.0259\n",
      "Epoch 11/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 0.1194 - mae: 0.1086 - mse: 0.0227 - val_loss: 0.1258 - val_mae: 0.1152 - val_mse: 0.0240\n",
      "Epoch 12/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 0.1185 - mae: 0.1079 - mse: 0.0225 - val_loss: 0.1225 - val_mae: 0.1121 - val_mse: 0.0232\n",
      "Epoch 13/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774us/step - loss: 0.1165 - mae: 0.1062 - mse: 0.0219 - val_loss: 0.1258 - val_mae: 0.1156 - val_mse: 0.0239\n",
      "Epoch 14/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 737us/step - loss: 0.1153 - mae: 0.1053 - mse: 0.0217 - val_loss: 0.1286 - val_mae: 0.1187 - val_mse: 0.0247\n",
      "Epoch 15/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 0.1158 - mae: 0.1059 - mse: 0.0218 - val_loss: 0.1268 - val_mae: 0.1170 - val_mse: 0.0244\n",
      "Epoch 16/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 0.1150 - mae: 0.1052 - mse: 0.0217 - val_loss: 0.1252 - val_mae: 0.1157 - val_mse: 0.0241\n",
      "Epoch 17/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 0.1136 - mae: 0.1040 - mse: 0.0214 - val_loss: 0.1243 - val_mae: 0.1148 - val_mse: 0.0242\n",
      "Epoch 18/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 0.1132 - mae: 0.1037 - mse: 0.0213 - val_loss: 0.1232 - val_mae: 0.1139 - val_mse: 0.0241\n",
      "Epoch 19/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 728us/step - loss: 0.1138 - mae: 0.1045 - mse: 0.0215 - val_loss: 0.1181 - val_mae: 0.1088 - val_mse: 0.0228\n",
      "Epoch 20/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 0.1130 - mae: 0.1039 - mse: 0.0215 - val_loss: 0.1185 - val_mae: 0.1094 - val_mse: 0.0232\n",
      "Epoch 21/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1127 - mae: 0.1036 - mse: 0.0214 - val_loss: 0.1211 - val_mae: 0.1121 - val_mse: 0.0234\n",
      "Epoch 22/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 796us/step - loss: 0.1111 - mae: 0.1022 - mse: 0.0210 - val_loss: 0.1181 - val_mae: 0.1092 - val_mse: 0.0224\n",
      "Epoch 23/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 874us/step - loss: 0.1099 - mae: 0.1010 - mse: 0.0207 - val_loss: 0.1227 - val_mae: 0.1139 - val_mse: 0.0243\n",
      "Epoch 24/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 875us/step - loss: 0.1109 - mae: 0.1021 - mse: 0.0211 - val_loss: 0.1200 - val_mae: 0.1113 - val_mse: 0.0235\n",
      "Epoch 25/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 799us/step - loss: 0.1095 - mae: 0.1007 - mse: 0.0207 - val_loss: 0.1188 - val_mae: 0.1100 - val_mse: 0.0231\n",
      "Epoch 26/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 0.1087 - mae: 0.0999 - mse: 0.0202 - val_loss: 0.1170 - val_mae: 0.1084 - val_mse: 0.0228\n",
      "Epoch 27/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - loss: 0.1090 - mae: 0.1003 - mse: 0.0205 - val_loss: 0.1165 - val_mae: 0.1079 - val_mse: 0.0225\n",
      "Epoch 28/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757us/step - loss: 0.1092 - mae: 0.1006 - mse: 0.0207 - val_loss: 0.1207 - val_mae: 0.1122 - val_mse: 0.0241\n",
      "Epoch 29/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 753us/step - loss: 0.1091 - mae: 0.1005 - mse: 0.0206 - val_loss: 0.1201 - val_mae: 0.1115 - val_mse: 0.0244\n",
      "Epoch 30/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 0.1086 - mae: 0.1001 - mse: 0.0204 - val_loss: 0.1198 - val_mae: 0.1113 - val_mse: 0.0239\n",
      "Epoch 31/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 904us/step - loss: 0.1073 - mae: 0.0988 - mse: 0.0201 - val_loss: 0.1265 - val_mae: 0.1181 - val_mse: 0.0266\n",
      "Epoch 32/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755us/step - loss: 0.1079 - mae: 0.0995 - mse: 0.0202 - val_loss: 0.1205 - val_mae: 0.1121 - val_mse: 0.0245\n",
      "Epoch 33/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - loss: 0.1073 - mae: 0.0989 - mse: 0.0202 - val_loss: 0.1195 - val_mae: 0.1111 - val_mse: 0.0244\n",
      "Epoch 34/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 906us/step - loss: 0.1067 - mae: 0.0983 - mse: 0.0200 - val_loss: 0.1216 - val_mae: 0.1132 - val_mse: 0.0251\n",
      "Epoch 35/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 927us/step - loss: 0.1069 - mae: 0.0985 - mse: 0.0199 - val_loss: 0.1146 - val_mae: 0.1062 - val_mse: 0.0219\n",
      "Epoch 36/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 922us/step - loss: 0.1049 - mae: 0.0965 - mse: 0.0195 - val_loss: 0.1144 - val_mae: 0.1061 - val_mse: 0.0220\n",
      "Epoch 37/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 722us/step - loss: 0.1048 - mae: 0.0965 - mse: 0.0193 - val_loss: 0.1158 - val_mae: 0.1075 - val_mse: 0.0227\n",
      "Epoch 38/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 0.1049 - mae: 0.0966 - mse: 0.0193 - val_loss: 0.1183 - val_mae: 0.1100 - val_mse: 0.0237\n",
      "Epoch 39/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 728us/step - loss: 0.1044 - mae: 0.0961 - mse: 0.0193 - val_loss: 0.1231 - val_mae: 0.1149 - val_mse: 0.0245\n",
      "Epoch 40/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 0.1036 - mae: 0.0953 - mse: 0.0189 - val_loss: 0.1147 - val_mae: 0.1064 - val_mse: 0.0221\n",
      "Epoch 41/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 0.1041 - mae: 0.0958 - mse: 0.0193 - val_loss: 0.1187 - val_mae: 0.1105 - val_mse: 0.0234\n",
      "Epoch 42/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 736us/step - loss: 0.1042 - mae: 0.0959 - mse: 0.0192 - val_loss: 0.1164 - val_mae: 0.1081 - val_mse: 0.0226\n",
      "Epoch 43/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - loss: 0.1027 - mae: 0.0944 - mse: 0.0187 - val_loss: 0.1263 - val_mae: 0.1180 - val_mse: 0.0267\n",
      "Epoch 44/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 859us/step - loss: 0.1033 - mae: 0.0950 - mse: 0.0189 - val_loss: 0.1158 - val_mae: 0.1075 - val_mse: 0.0226\n",
      "Epoch 45/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 766us/step - loss: 0.1026 - mae: 0.0944 - mse: 0.0189 - val_loss: 0.1195 - val_mae: 0.1113 - val_mse: 0.0236\n",
      "Epoch 46/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 809us/step - loss: 0.1031 - mae: 0.0948 - mse: 0.0189 - val_loss: 0.1171 - val_mae: 0.1089 - val_mse: 0.0231\n",
      "Fold 1 - MAE: 0.1061, MSE: 0.0220, R2: 0.9337\n",
      "\n",
      "Fold 2/5\n",
      "Epoch 1/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3414 - mae: 0.3241 - mse: 0.2752 - val_loss: 0.1579 - val_mae: 0.1414 - val_mse: 0.0351\n",
      "Epoch 2/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 901us/step - loss: 0.1581 - mae: 0.1417 - mse: 0.0355 - val_loss: 0.1459 - val_mae: 0.1301 - val_mse: 0.0284\n",
      "Epoch 3/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 788us/step - loss: 0.1421 - mae: 0.1266 - mse: 0.0285 - val_loss: 0.1443 - val_mae: 0.1293 - val_mse: 0.0283\n",
      "Epoch 4/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - loss: 0.1392 - mae: 0.1243 - mse: 0.0276 - val_loss: 0.1520 - val_mae: 0.1376 - val_mse: 0.0309\n",
      "Epoch 5/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 789us/step - loss: 0.1328 - mae: 0.1185 - mse: 0.0255 - val_loss: 0.1347 - val_mae: 0.1209 - val_mse: 0.0257\n",
      "Epoch 6/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 737us/step - loss: 0.1293 - mae: 0.1156 - mse: 0.0249 - val_loss: 0.1351 - val_mae: 0.1218 - val_mse: 0.0264\n",
      "Epoch 7/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 753us/step - loss: 0.1263 - mae: 0.1131 - mse: 0.0242 - val_loss: 0.1276 - val_mae: 0.1148 - val_mse: 0.0242\n",
      "Epoch 8/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 0.1273 - mae: 0.1145 - mse: 0.0248 - val_loss: 0.1296 - val_mae: 0.1172 - val_mse: 0.0246\n",
      "Epoch 9/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 870us/step - loss: 0.1220 - mae: 0.1097 - mse: 0.0233 - val_loss: 0.1271 - val_mae: 0.1152 - val_mse: 0.0242\n",
      "Epoch 10/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 931us/step - loss: 0.1231 - mae: 0.1113 - mse: 0.0236 - val_loss: 0.1281 - val_mae: 0.1165 - val_mse: 0.0241\n",
      "Epoch 11/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1225 - mae: 0.1110 - mse: 0.0235 - val_loss: 0.1272 - val_mae: 0.1160 - val_mse: 0.0241\n",
      "Epoch 12/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step - loss: 0.1186 - mae: 0.1074 - mse: 0.0226 - val_loss: 0.1286 - val_mae: 0.1176 - val_mse: 0.0244\n",
      "Epoch 13/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 857us/step - loss: 0.1176 - mae: 0.1067 - mse: 0.0223 - val_loss: 0.1202 - val_mae: 0.1095 - val_mse: 0.0228\n",
      "Epoch 14/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 782us/step - loss: 0.1176 - mae: 0.1071 - mse: 0.0224 - val_loss: 0.1236 - val_mae: 0.1132 - val_mse: 0.0237\n",
      "Epoch 15/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 0.1165 - mae: 0.1061 - mse: 0.0221 - val_loss: 0.1202 - val_mae: 0.1100 - val_mse: 0.0231\n",
      "Epoch 16/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 825us/step - loss: 0.1146 - mae: 0.1044 - mse: 0.0217 - val_loss: 0.1276 - val_mae: 0.1175 - val_mse: 0.0253\n",
      "Epoch 17/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 864us/step - loss: 0.1148 - mae: 0.1048 - mse: 0.0219 - val_loss: 0.1234 - val_mae: 0.1135 - val_mse: 0.0235\n",
      "Epoch 18/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 845us/step - loss: 0.1132 - mae: 0.1033 - mse: 0.0212 - val_loss: 0.1184 - val_mae: 0.1087 - val_mse: 0.0225\n",
      "Epoch 19/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 0.1126 - mae: 0.1029 - mse: 0.0213 - val_loss: 0.1172 - val_mae: 0.1076 - val_mse: 0.0219\n",
      "Epoch 20/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - loss: 0.1114 - mae: 0.1019 - mse: 0.0209 - val_loss: 0.1231 - val_mae: 0.1136 - val_mse: 0.0242\n",
      "Epoch 21/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 0.1103 - mae: 0.1009 - mse: 0.0206 - val_loss: 0.1193 - val_mae: 0.1099 - val_mse: 0.0228\n",
      "Epoch 22/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1103 - mae: 0.1010 - mse: 0.0206 - val_loss: 0.1211 - val_mae: 0.1119 - val_mse: 0.0236\n",
      "Epoch 23/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 837us/step - loss: 0.1101 - mae: 0.1008 - mse: 0.0206 - val_loss: 0.1173 - val_mae: 0.1081 - val_mse: 0.0225\n",
      "Epoch 24/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 0.1098 - mae: 0.1007 - mse: 0.0206 - val_loss: 0.1199 - val_mae: 0.1107 - val_mse: 0.0231\n",
      "Epoch 25/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 858us/step - loss: 0.1086 - mae: 0.0995 - mse: 0.0200 - val_loss: 0.1249 - val_mae: 0.1158 - val_mse: 0.0244\n",
      "Epoch 26/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 820us/step - loss: 0.1086 - mae: 0.0996 - mse: 0.0201 - val_loss: 0.1170 - val_mae: 0.1079 - val_mse: 0.0225\n",
      "Epoch 27/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 739us/step - loss: 0.1090 - mae: 0.1000 - mse: 0.0206 - val_loss: 0.1194 - val_mae: 0.1104 - val_mse: 0.0228\n",
      "Epoch 28/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 0.1092 - mae: 0.1002 - mse: 0.0203 - val_loss: 0.1166 - val_mae: 0.1077 - val_mse: 0.0222\n",
      "Epoch 29/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 0.1065 - mae: 0.0975 - mse: 0.0195 - val_loss: 0.1188 - val_mae: 0.1099 - val_mse: 0.0220\n",
      "Epoch 30/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 0.1071 - mae: 0.0982 - mse: 0.0198 - val_loss: 0.1203 - val_mae: 0.1115 - val_mse: 0.0231\n",
      "Epoch 31/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - loss: 0.1052 - mae: 0.0964 - mse: 0.0192 - val_loss: 0.1173 - val_mae: 0.1086 - val_mse: 0.0226\n",
      "Epoch 32/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 675us/step - loss: 0.1062 - mae: 0.0974 - mse: 0.0196 - val_loss: 0.1171 - val_mae: 0.1084 - val_mse: 0.0225\n",
      "Epoch 33/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 848us/step - loss: 0.1058 - mae: 0.0971 - mse: 0.0195 - val_loss: 0.1215 - val_mae: 0.1128 - val_mse: 0.0236\n",
      "Epoch 34/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 764us/step - loss: 0.1056 - mae: 0.0969 - mse: 0.0195 - val_loss: 0.1165 - val_mae: 0.1078 - val_mse: 0.0219\n",
      "Epoch 35/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 764us/step - loss: 0.1044 - mae: 0.0957 - mse: 0.0191 - val_loss: 0.1195 - val_mae: 0.1108 - val_mse: 0.0232\n",
      "Epoch 36/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 0.1041 - mae: 0.0954 - mse: 0.0190 - val_loss: 0.1176 - val_mae: 0.1089 - val_mse: 0.0224\n",
      "Epoch 37/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755us/step - loss: 0.1034 - mae: 0.0947 - mse: 0.0190 - val_loss: 0.1190 - val_mae: 0.1104 - val_mse: 0.0229\n",
      "Epoch 38/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 0.1042 - mae: 0.0956 - mse: 0.0192 - val_loss: 0.1171 - val_mae: 0.1084 - val_mse: 0.0225\n",
      "Fold 2 - MAE: 0.1077, MSE: 0.0222, R2: 0.9343\n",
      "\n",
      "Fold 3/5\n",
      "Epoch 1/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 975us/step - loss: 0.4066 - mae: 0.3897 - mse: 0.5045 - val_loss: 0.1519 - val_mae: 0.1356 - val_mse: 0.0315\n",
      "Epoch 2/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 0.1551 - mae: 0.1390 - mse: 0.0344 - val_loss: 0.1527 - val_mae: 0.1372 - val_mse: 0.0302\n",
      "Epoch 3/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 715us/step - loss: 0.1428 - mae: 0.1276 - mse: 0.0292 - val_loss: 0.1375 - val_mae: 0.1229 - val_mse: 0.0251\n",
      "Epoch 4/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 730us/step - loss: 0.1350 - mae: 0.1205 - mse: 0.0268 - val_loss: 0.1362 - val_mae: 0.1222 - val_mse: 0.0253\n",
      "Epoch 5/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 753us/step - loss: 0.1345 - mae: 0.1207 - mse: 0.0269 - val_loss: 0.1336 - val_mae: 0.1203 - val_mse: 0.0241\n",
      "Epoch 6/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 878us/step - loss: 0.1298 - mae: 0.1166 - mse: 0.0255 - val_loss: 0.1342 - val_mae: 0.1215 - val_mse: 0.0247\n",
      "Epoch 7/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 0.1284 - mae: 0.1158 - mse: 0.0250 - val_loss: 0.1314 - val_mae: 0.1192 - val_mse: 0.0240\n",
      "Epoch 8/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 0.1254 - mae: 0.1133 - mse: 0.0244 - val_loss: 0.1327 - val_mae: 0.1209 - val_mse: 0.0246\n",
      "Epoch 9/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 816us/step - loss: 0.1226 - mae: 0.1109 - mse: 0.0235 - val_loss: 0.1343 - val_mae: 0.1229 - val_mse: 0.0261\n",
      "Epoch 10/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 795us/step - loss: 0.1210 - mae: 0.1097 - mse: 0.0233 - val_loss: 0.1263 - val_mae: 0.1153 - val_mse: 0.0233\n",
      "Epoch 11/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 753us/step - loss: 0.1202 - mae: 0.1093 - mse: 0.0233 - val_loss: 0.1234 - val_mae: 0.1128 - val_mse: 0.0224\n",
      "Epoch 12/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 807us/step - loss: 0.1186 - mae: 0.1081 - mse: 0.0228 - val_loss: 0.1305 - val_mae: 0.1202 - val_mse: 0.0250\n",
      "Epoch 13/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 772us/step - loss: 0.1180 - mae: 0.1078 - mse: 0.0227 - val_loss: 0.1265 - val_mae: 0.1164 - val_mse: 0.0242\n",
      "Epoch 14/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1166 - mae: 0.1066 - mse: 0.0225 - val_loss: 0.1177 - val_mae: 0.1079 - val_mse: 0.0212\n",
      "Epoch 15/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 925us/step - loss: 0.1152 - mae: 0.1054 - mse: 0.0221 - val_loss: 0.1219 - val_mae: 0.1123 - val_mse: 0.0234\n",
      "Epoch 16/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 0.1154 - mae: 0.1058 - mse: 0.0222 - val_loss: 0.1219 - val_mae: 0.1124 - val_mse: 0.0224\n",
      "Epoch 17/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 763us/step - loss: 0.1130 - mae: 0.1036 - mse: 0.0215 - val_loss: 0.1164 - val_mae: 0.1071 - val_mse: 0.0213\n",
      "Epoch 18/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 764us/step - loss: 0.1136 - mae: 0.1043 - mse: 0.0217 - val_loss: 0.1185 - val_mae: 0.1094 - val_mse: 0.0216\n",
      "Epoch 19/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - loss: 0.1123 - mae: 0.1032 - mse: 0.0214 - val_loss: 0.1176 - val_mae: 0.1086 - val_mse: 0.0213\n",
      "Epoch 20/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step - loss: 0.1106 - mae: 0.1016 - mse: 0.0209 - val_loss: 0.1158 - val_mae: 0.1070 - val_mse: 0.0211\n",
      "Epoch 21/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 798us/step - loss: 0.1112 - mae: 0.1023 - mse: 0.0213 - val_loss: 0.1194 - val_mae: 0.1107 - val_mse: 0.0218\n",
      "Epoch 22/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 866us/step - loss: 0.1107 - mae: 0.1019 - mse: 0.0210 - val_loss: 0.1171 - val_mae: 0.1084 - val_mse: 0.0216\n",
      "Epoch 23/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 796us/step - loss: 0.1118 - mae: 0.1032 - mse: 0.0215 - val_loss: 0.1180 - val_mae: 0.1095 - val_mse: 0.0212\n",
      "Epoch 24/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step - loss: 0.1081 - mae: 0.0996 - mse: 0.0202 - val_loss: 0.1182 - val_mae: 0.1097 - val_mse: 0.0213\n",
      "Epoch 25/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 891us/step - loss: 0.1086 - mae: 0.1001 - mse: 0.0206 - val_loss: 0.1214 - val_mae: 0.1130 - val_mse: 0.0233\n",
      "Epoch 26/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1100 - mae: 0.1015 - mse: 0.0209 - val_loss: 0.1215 - val_mae: 0.1131 - val_mse: 0.0226\n",
      "Epoch 27/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 786us/step - loss: 0.1081 - mae: 0.0997 - mse: 0.0203 - val_loss: 0.1207 - val_mae: 0.1124 - val_mse: 0.0223\n",
      "Epoch 28/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 783us/step - loss: 0.1078 - mae: 0.0995 - mse: 0.0204 - val_loss: 0.1167 - val_mae: 0.1084 - val_mse: 0.0211\n",
      "Epoch 29/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 0.1081 - mae: 0.0998 - mse: 0.0205 - val_loss: 0.1194 - val_mae: 0.1112 - val_mse: 0.0222\n",
      "Epoch 30/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 0.1077 - mae: 0.0995 - mse: 0.0203 - val_loss: 0.1160 - val_mae: 0.1078 - val_mse: 0.0212\n",
      "Fold 3 - MAE: 0.1070, MSE: 0.0211, R2: 0.9376\n",
      "\n",
      "Fold 4/5\n",
      "Epoch 1/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.3316 - mae: 0.3149 - mse: 0.2663 - val_loss: 0.1643 - val_mae: 0.1485 - val_mse: 0.0389\n",
      "Epoch 2/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 0.1509 - mae: 0.1354 - mse: 0.0319 - val_loss: 0.1528 - val_mae: 0.1381 - val_mse: 0.0328\n",
      "Epoch 3/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 0.1369 - mae: 0.1225 - mse: 0.0270 - val_loss: 0.1460 - val_mae: 0.1322 - val_mse: 0.0307\n",
      "Epoch 4/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 877us/step - loss: 0.1311 - mae: 0.1175 - mse: 0.0250 - val_loss: 0.1586 - val_mae: 0.1454 - val_mse: 0.0356\n",
      "Epoch 5/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 864us/step - loss: 0.1298 - mae: 0.1168 - mse: 0.0246 - val_loss: 0.1486 - val_mae: 0.1361 - val_mse: 0.0315\n",
      "Epoch 6/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 783us/step - loss: 0.1257 - mae: 0.1133 - mse: 0.0236 - val_loss: 0.1401 - val_mae: 0.1281 - val_mse: 0.0294\n",
      "Epoch 7/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 809us/step - loss: 0.1226 - mae: 0.1107 - mse: 0.0228 - val_loss: 0.1482 - val_mae: 0.1366 - val_mse: 0.0317\n",
      "Epoch 8/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 0.1206 - mae: 0.1091 - mse: 0.0225 - val_loss: 0.1431 - val_mae: 0.1319 - val_mse: 0.0317\n",
      "Epoch 9/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 733us/step - loss: 0.1221 - mae: 0.1111 - mse: 0.0230 - val_loss: 0.1311 - val_mae: 0.1202 - val_mse: 0.0272\n",
      "Epoch 10/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 0.1186 - mae: 0.1079 - mse: 0.0223 - val_loss: 0.1350 - val_mae: 0.1245 - val_mse: 0.0290\n",
      "Epoch 11/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - loss: 0.1170 - mae: 0.1066 - mse: 0.0216 - val_loss: 0.1287 - val_mae: 0.1185 - val_mse: 0.0271\n",
      "Epoch 12/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 738us/step - loss: 0.1156 - mae: 0.1054 - mse: 0.0214 - val_loss: 0.1369 - val_mae: 0.1269 - val_mse: 0.0291\n",
      "Epoch 13/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 799us/step - loss: 0.1147 - mae: 0.1047 - mse: 0.0211 - val_loss: 0.1325 - val_mae: 0.1227 - val_mse: 0.0281\n",
      "Epoch 14/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 0.1135 - mae: 0.1038 - mse: 0.0209 - val_loss: 0.1302 - val_mae: 0.1206 - val_mse: 0.0270\n",
      "Epoch 15/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 0.1132 - mae: 0.1036 - mse: 0.0209 - val_loss: 0.1261 - val_mae: 0.1166 - val_mse: 0.0257\n",
      "Epoch 16/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 732us/step - loss: 0.1117 - mae: 0.1023 - mse: 0.0204 - val_loss: 0.1331 - val_mae: 0.1237 - val_mse: 0.0292\n",
      "Epoch 17/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 0.1139 - mae: 0.1046 - mse: 0.0212 - val_loss: 0.1321 - val_mae: 0.1229 - val_mse: 0.0280\n",
      "Epoch 18/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 0.1115 - mae: 0.1023 - mse: 0.0204 - val_loss: 0.1311 - val_mae: 0.1220 - val_mse: 0.0281\n",
      "Epoch 19/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 726us/step - loss: 0.1101 - mae: 0.1010 - mse: 0.0201 - val_loss: 0.1225 - val_mae: 0.1135 - val_mse: 0.0250\n",
      "Epoch 20/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 0.1085 - mae: 0.0995 - mse: 0.0196 - val_loss: 0.1332 - val_mae: 0.1243 - val_mse: 0.0290\n",
      "Epoch 21/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - loss: 0.1109 - mae: 0.1021 - mse: 0.0204 - val_loss: 0.1365 - val_mae: 0.1277 - val_mse: 0.0311\n",
      "Epoch 22/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 0.1102 - mae: 0.1014 - mse: 0.0204 - val_loss: 0.1267 - val_mae: 0.1180 - val_mse: 0.0276\n",
      "Epoch 23/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 910us/step - loss: 0.1094 - mae: 0.1007 - mse: 0.0200 - val_loss: 0.1302 - val_mae: 0.1215 - val_mse: 0.0281\n",
      "Epoch 24/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step - loss: 0.1091 - mae: 0.1005 - mse: 0.0201 - val_loss: 0.1256 - val_mae: 0.1170 - val_mse: 0.0274\n",
      "Epoch 25/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 871us/step - loss: 0.1087 - mae: 0.1001 - mse: 0.0200 - val_loss: 0.1372 - val_mae: 0.1286 - val_mse: 0.0320\n",
      "Epoch 26/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 887us/step - loss: 0.1078 - mae: 0.0992 - mse: 0.0197 - val_loss: 0.1309 - val_mae: 0.1224 - val_mse: 0.0297\n",
      "Epoch 27/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 844us/step - loss: 0.1092 - mae: 0.1007 - mse: 0.0203 - val_loss: 0.1263 - val_mae: 0.1179 - val_mse: 0.0274\n",
      "Epoch 28/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 0.1066 - mae: 0.0981 - mse: 0.0192 - val_loss: 0.1282 - val_mae: 0.1198 - val_mse: 0.0282\n",
      "Epoch 29/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 884us/step - loss: 0.1068 - mae: 0.0984 - mse: 0.0196 - val_loss: 0.1271 - val_mae: 0.1187 - val_mse: 0.0283\n",
      "Fold 4 - MAE: 0.1135, MSE: 0.0250, R2: 0.9262\n",
      "\n",
      "Fold 5/5\n",
      "Epoch 1/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.2715 - mae: 0.2546 - mse: 0.1409 - val_loss: 0.1710 - val_mae: 0.1546 - val_mse: 0.0380\n",
      "Epoch 2/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 810us/step - loss: 0.1571 - mae: 0.1410 - mse: 0.0353 - val_loss: 0.1408 - val_mae: 0.1252 - val_mse: 0.0278\n",
      "Epoch 3/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 889us/step - loss: 0.1436 - mae: 0.1283 - mse: 0.0296 - val_loss: 0.1410 - val_mae: 0.1262 - val_mse: 0.0280\n",
      "Epoch 4/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 786us/step - loss: 0.1388 - mae: 0.1241 - mse: 0.0281 - val_loss: 0.1414 - val_mae: 0.1272 - val_mse: 0.0271\n",
      "Epoch 5/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 0.1315 - mae: 0.1174 - mse: 0.0254 - val_loss: 0.1311 - val_mae: 0.1175 - val_mse: 0.0248\n",
      "Epoch 6/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 780us/step - loss: 0.1284 - mae: 0.1149 - mse: 0.0248 - val_loss: 0.1398 - val_mae: 0.1267 - val_mse: 0.0271\n",
      "Epoch 7/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 816us/step - loss: 0.1273 - mae: 0.1143 - mse: 0.0243 - val_loss: 0.1265 - val_mae: 0.1138 - val_mse: 0.0240\n",
      "Epoch 8/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 851us/step - loss: 0.1245 - mae: 0.1120 - mse: 0.0240 - val_loss: 0.1325 - val_mae: 0.1203 - val_mse: 0.0263\n",
      "Epoch 9/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 846us/step - loss: 0.1239 - mae: 0.1118 - mse: 0.0238 - val_loss: 0.1312 - val_mae: 0.1194 - val_mse: 0.0251\n",
      "Epoch 10/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 760us/step - loss: 0.1221 - mae: 0.1104 - mse: 0.0234 - val_loss: 0.1283 - val_mae: 0.1168 - val_mse: 0.0262\n",
      "Epoch 11/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 944us/step - loss: 0.1218 - mae: 0.1105 - mse: 0.0235 - val_loss: 0.1264 - val_mae: 0.1153 - val_mse: 0.0247\n",
      "Epoch 12/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 855us/step - loss: 0.1187 - mae: 0.1077 - mse: 0.0224 - val_loss: 0.1200 - val_mae: 0.1092 - val_mse: 0.0232\n",
      "Epoch 13/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 728us/step - loss: 0.1162 - mae: 0.1054 - mse: 0.0220 - val_loss: 0.1226 - val_mae: 0.1120 - val_mse: 0.0242\n",
      "Epoch 14/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 726us/step - loss: 0.1172 - mae: 0.1067 - mse: 0.0223 - val_loss: 0.1221 - val_mae: 0.1118 - val_mse: 0.0237\n",
      "Epoch 15/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 715us/step - loss: 0.1144 - mae: 0.1042 - mse: 0.0214 - val_loss: 0.1244 - val_mae: 0.1143 - val_mse: 0.0242\n",
      "Epoch 16/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 765us/step - loss: 0.1150 - mae: 0.1050 - mse: 0.0217 - val_loss: 0.1297 - val_mae: 0.1198 - val_mse: 0.0269\n",
      "Epoch 17/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 839us/step - loss: 0.1177 - mae: 0.1078 - mse: 0.0226 - val_loss: 0.1293 - val_mae: 0.1196 - val_mse: 0.0266\n",
      "Epoch 18/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 730us/step - loss: 0.1149 - mae: 0.1052 - mse: 0.0217 - val_loss: 0.1232 - val_mae: 0.1137 - val_mse: 0.0244\n",
      "Epoch 19/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 730us/step - loss: 0.1146 - mae: 0.1051 - mse: 0.0218 - val_loss: 0.1311 - val_mae: 0.1217 - val_mse: 0.0277\n",
      "Epoch 20/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 827us/step - loss: 0.1145 - mae: 0.1051 - mse: 0.0218 - val_loss: 0.1202 - val_mae: 0.1109 - val_mse: 0.0239\n",
      "Epoch 21/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 760us/step - loss: 0.1129 - mae: 0.1036 - mse: 0.0214 - val_loss: 0.1234 - val_mae: 0.1143 - val_mse: 0.0251\n",
      "Epoch 22/100\n",
      "\u001b[1m308/308\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 793us/step - loss: 0.1114 - mae: 0.1022 - mse: 0.0209 - val_loss: 0.1237 - val_mae: 0.1146 - val_mse: 0.0249\n",
      "Fold 5 - MAE: 0.1092, MSE: 0.0232, R2: 0.9315\n",
      "\n",
      "Cross-Validation Results:\n",
      "--------------------------------------------------\n",
      "Average MAE: 0.1087 ± 0.0026\n",
      "Average MSE: 0.0227 ± 0.0013\n",
      "Average R2: 0.9327 ± 0.0038\n"
     ]
    }
   ],
   "source": [
    "cv_results = cv_saved_model_architecture(\n",
    "    saved_model_path='model.h5',\n",
    "    df=train_scaled_df,\n",
    "    target_column='r_value',\n",
    "    n_splits=5,\n",
    "    epochs=100,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Print CV results\n",
    "print(\"\\nCross-Validation Results:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Average MAE: {cv_results['avg_mae']:.4f} ± {cv_results['std_mae']:.4f}\")\n",
    "print(f\"Average MSE: {cv_results['avg_mse']:.4f} ± {cv_results['std_mse']:.4f}\")\n",
    "print(f\"Average R2: {cv_results['avg_r2']:.4f} ± {cv_results['std_r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing GridSearch...\n",
      "\n",
      "Best parameters: {'C': 1, 'epsilon': 0.01}\n",
      "\n",
      "Performing cross-validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 - MAE: 0.1044, MSE: 0.0216, R2: 0.9362: 100%|██████████| 5/5 [00:28<00:00,  5.68s/it]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "svr_param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'epsilon': [0.001, 0.01, 0.1, 0.5, 1, 2]\n",
    "}\n",
    "\n",
    "svr_results = train_model_with_cv_gridsearch(\n",
    "    df=train_scaled_df,\n",
    "    model=SVR(),\n",
    "    param_grid=svr_param_grid\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing GridSearch...\n",
      "\n",
      "Best parameters: {'alpha': 0.001}\n",
      "\n",
      "Performing cross-validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 - MAE: 0.1217, MSE: 0.0266, R2: 0.9216: 100%|██████████| 5/5 [00:00<00:00, 17.77it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge_param_grid = {\n",
    "    'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "}\n",
    "\n",
    "ridge_results = train_model_with_cv_gridsearch(\n",
    "    df=train_scaled_df,\n",
    "    model=Ridge(random_state=42),\n",
    "    param_grid=ridge_param_grid\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_evaluate_model(data_df, features_dict, model_path, n_splits=5, batch_size=32):\n",
    "    \"\"\"\n",
    "    Load saved model and perform cross validation\n",
    "    \n",
    "    Args:\n",
    "        data_df: Pandas DataFrame containing the data\n",
    "        features_dict: Dictionary of features by category\n",
    "        model_path: Path to saved model file\n",
    "        n_splits: Number of CV folds\n",
    "        batch_size: Batch size for evaluation\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of metrics averaged across folds\n",
    "    \"\"\"\n",
    "    # Load the saved model\n",
    "    model = torch.load(model_path)\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize feature arrays and dimensions\n",
    "    feature_arrays = {}\n",
    "    feature_dims = {}\n",
    "    \n",
    "    # Process each feature category\n",
    "    for category in ['chemical', 'time', 'process', 'model']:\n",
    "        available_features = [col for col in features_dict[category] \n",
    "                            if col in data_df.columns]\n",
    "        \n",
    "        if available_features:\n",
    "            feature_arrays[category] = data_df[available_features].values.astype(np.float32)\n",
    "            feature_dims[category] = len(available_features)\n",
    "        else:\n",
    "            feature_arrays[category] = np.zeros((len(data_df), 0), dtype=np.float32)\n",
    "            feature_dims[category] = 0\n",
    "    \n",
    "    # Prepare targets\n",
    "    targets = data_df['r_value'].values\n",
    "    \n",
    "    # Initialize KFold\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Lists to store metrics for each fold\n",
    "    fold_metrics = {\n",
    "        'mae': [],\n",
    "        'mse': [],\n",
    "        'rmse': [],\n",
    "        'r2': []\n",
    "    }\n",
    "    \n",
    "    # Cross validation loop\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(targets)):\n",
    "        # Prepare validation tensors for this fold\n",
    "        val_tensors = {\n",
    "            'chemical': torch.FloatTensor(feature_arrays['chemical'][val_idx]),\n",
    "            'time': torch.FloatTensor(feature_arrays['time'][val_idx]),\n",
    "            'process': torch.FloatTensor(feature_arrays['process'][val_idx]),\n",
    "            'model': torch.FloatTensor(feature_arrays['model'][val_idx])\n",
    "        }\n",
    "        \n",
    "        val_targets = torch.FloatTensor(targets[val_idx])\n",
    "        \n",
    "        # Create validation DataLoader\n",
    "        val_dataset = TensorDataset(\n",
    "            val_tensors['chemical'],\n",
    "            val_tensors['time'],\n",
    "            val_tensors['process'],\n",
    "            val_tensors['model'],\n",
    "            val_targets\n",
    "        )\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "        \n",
    "        # Evaluation for this fold\n",
    "        predictions = []\n",
    "        actuals = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_chem, batch_time, batch_proc, batch_model, batch_targets in val_loader:\n",
    "                outputs = model(batch_chem, batch_time, batch_proc, batch_model)\n",
    "                predictions.extend(outputs.numpy().flatten())\n",
    "                actuals.extend(batch_targets.numpy().flatten())\n",
    "        \n",
    "        # Calculate metrics for this fold\n",
    "        predictions = np.array(predictions)\n",
    "        actuals = np.array(actuals)\n",
    "        \n",
    "        mae = mean_absolute_error(actuals, predictions)\n",
    "        mse = mean_squared_error(actuals, predictions)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(actuals, predictions)\n",
    "        \n",
    "        fold_metrics['mae'].append(mae)\n",
    "        fold_metrics['mse'].append(mse)\n",
    "        fold_metrics['rmse'].append(rmse)\n",
    "        fold_metrics['r2'].append(r2)\n",
    "        \n",
    "        print(f\"Fold {fold+1} - MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}, R2: {r2:.4f}\")\n",
    "    \n",
    "    # Calculate and return average metrics\n",
    "    avg_metrics = {\n",
    "        'mae': np.mean(fold_metrics['mae']),\n",
    "        'mae_std': np.std(fold_metrics['mae']),\n",
    "        'mse': np.mean(fold_metrics['mse']),\n",
    "        'mse_std': np.std(fold_metrics['mse']), \n",
    "        'rmse': np.mean(fold_metrics['rmse']),\n",
    "        'rmse_std': np.std(fold_metrics['rmse']),\n",
    "        'r2': np.mean(fold_metrics['r2']),\n",
    "        'r2_std': np.std(fold_metrics['r2'])\n",
    "    }\n",
    "    \n",
    "    print(\"\\nAverage Metrics across folds:\")\n",
    "    print(f\"MAE: {avg_metrics['mae']:.4f} ± {avg_metrics['mae_std']:.4f}\")\n",
    "    print(f\"MSE: {avg_metrics['mse']:.4f} ± {avg_metrics['mse_std']:.4f}\")\n",
    "    print(f\"RMSE: {avg_metrics['rmse']:.4f} ± {avg_metrics['rmse_std']:.4f}\")\n",
    "    print(f\"R2: {avg_metrics['r2']:.4f} ± {avg_metrics['r2_std']:.4f}\")\n",
    "    \n",
    "    return avg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labeling the features for each branch\n",
    "features = [col for col in df.columns if col not in ['r_value', 'steel_family', 'steel_grade']]\n",
    "features_dict = {\n",
    "   'time': [col for col in features if 'time' in col.lower()], \n",
    "   'chemical': ['pct_al', 'pct_b', 'pct_c', 'pct_cr', 'pct_mn', 'pct_n', 'pct_nb', 'pct_si', 'pct_ti', 'pct_v', 'mfia_coil_frac_fer', 'mfia_et1_frac_fer', 'mfia_et2_frac_fer'],\n",
    "   'model': [\"rm\", \"ag\", \"a80\", \"n_value\"]\n",
    "}\n",
    "features_dict['process'] = [col for col in features if col not in features_dict['time'] and col not in features_dict['chemical']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiBranchSteelRegressor(nn.Module):\n",
    "    def __init__(self, chemical_dim, time_dim, process_dim, model_dim, hidden_units=64, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        # Track which branches are active\n",
    "        self.has_chemical = chemical_dim > 0\n",
    "        self.has_time = time_dim > 0\n",
    "        self.has_process = process_dim > 0\n",
    "        self.has_model = model_dim > 0\n",
    "        \n",
    "        # Count active branches\n",
    "        self.active_branches = sum([self.has_chemical, self.has_time, self.has_process, self.has_model])\n",
    "        \n",
    "        # Adjust hidden units for each branch\n",
    "        self.branch_hidden = min(hidden_units, max(16, hidden_units // 2))\n",
    "        \n",
    "        # Creating branch\n",
    "        def create_branch(input_dim):\n",
    "            return nn.Sequential(\n",
    "                nn.Linear(input_dim, self.branch_hidden),\n",
    "                nn.BatchNorm1d(self.branch_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            )\n",
    "        \n",
    "        # Only create branches that have features\n",
    "        if self.has_chemical:\n",
    "            self.chemical_branch = create_branch(chemical_dim)\n",
    "        if self.has_time:\n",
    "            self.time_branch = create_branch(time_dim)\n",
    "        if self.has_process:\n",
    "            self.process_branch = create_branch(process_dim)\n",
    "        if self.has_model:\n",
    "            self.model_branch = create_branch(model_dim)\n",
    "        \n",
    "        # Combined input dimension based on active branches only\n",
    "        combined_dim = self.branch_hidden * self.active_branches\n",
    "        \n",
    "        # Final layers after concatenation\n",
    "        self.final_layers = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, chemical, time, process, model):\n",
    "        features = []\n",
    "        # Only process branches that have features\n",
    "        if self.has_chemical:\n",
    "            if chemical.dim() == 1:\n",
    "                chemical = chemical.unsqueeze(0)\n",
    "            features.append(self.chemical_branch(chemical))\n",
    "        \n",
    "        if self.has_time:\n",
    "            if time.dim() == 1:\n",
    "                time = time.unsqueeze(0)\n",
    "            features.append(self.time_branch(time))\n",
    "        \n",
    "        if self.has_process:\n",
    "            if process.dim() == 1:\n",
    "                process = process.unsqueeze(0)\n",
    "            features.append(self.process_branch(process))\n",
    "        \n",
    "        if self.has_model:\n",
    "            if model.dim() == 1:\n",
    "                model = model.unsqueeze(0)\n",
    "            features.append(self.model_branch(model))\n",
    "        \n",
    "        # Concatenate only active features\n",
    "        combined = torch.cat(features, dim=1) if len(features) > 1 else features[0]\n",
    "        return self.final_layers(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_regular(df, features_dict, num_epochs, hyperparameters, use_l2=False):\n",
    "    batch_size = hyperparameters['batch_size']\n",
    "    \n",
    "    # Initialize feature arrays and dimensions\n",
    "    feature_arrays = {}\n",
    "    feature_dims = {}\n",
    "    \n",
    "    # Process each feature category\n",
    "    for category in ['chemical', 'time', 'process', 'model']:\n",
    "        available_features = [col for col in features_dict[category] \n",
    "                            if col in df.columns]\n",
    "        \n",
    "        if available_features:\n",
    "            feature_arrays[category] = df[available_features].values.astype(np.float32)\n",
    "            feature_dims[category] = len(available_features)\n",
    "        else:\n",
    "            feature_arrays[category] = np.zeros((len(df), 0), dtype=np.float32)\n",
    "            feature_dims[category] = 0\n",
    "    \n",
    "    # Prepare targets\n",
    "    targets = df['r_value'].values\n",
    "    \n",
    "    # Split data\n",
    "    split_data = train_test_split(\n",
    "        feature_arrays['chemical'],\n",
    "        feature_arrays['time'],\n",
    "        feature_arrays['process'],\n",
    "        feature_arrays['model'],\n",
    "        targets,\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    (X_train_chem, X_test_chem, X_train_time, X_test_time, \n",
    "     X_train_proc, X_test_proc, X_train_model, X_test_model, \n",
    "     y_train, y_test) = split_data\n",
    "    \n",
    "    # Convert to tensors\n",
    "    train_tensors = {\n",
    "        'chemical': torch.FloatTensor(X_train_chem),\n",
    "        'time': torch.FloatTensor(X_train_time),\n",
    "        'process': torch.FloatTensor(X_train_proc),\n",
    "        'model': torch.FloatTensor(X_train_model)\n",
    "    }\n",
    "    \n",
    "    test_tensors = {\n",
    "        'chemical': torch.FloatTensor(X_test_chem),\n",
    "        'time': torch.FloatTensor(X_test_time),\n",
    "        'process': torch.FloatTensor(X_test_proc),\n",
    "        'model': torch.FloatTensor(X_test_model)\n",
    "    }\n",
    "    \n",
    "    y_train_tensor = torch.FloatTensor(y_train)\n",
    "    y_test_tensor = torch.FloatTensor(y_test)\n",
    "    \n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(\n",
    "        train_tensors['chemical'],\n",
    "        train_tensors['time'],\n",
    "        train_tensors['process'],\n",
    "        train_tensors['model'],\n",
    "        y_train_tensor\n",
    "    )\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = MultiBranchSteelRegressor(\n",
    "        chemical_dim=feature_dims['chemical'],\n",
    "        time_dim=feature_dims['time'],\n",
    "        process_dim=feature_dims['process'],\n",
    "        model_dim=feature_dims['model'],\n",
    "        hidden_units=hyperparameters['hidden_units'],\n",
    "        dropout_rate=hyperparameters['dropout_rate']\n",
    "    )\n",
    "    \n",
    "    if use_l2:\n",
    "        weight_decay = 0.001\n",
    "    else:\n",
    "        weight_decay = 0.0\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), weight_decay=weight_decay)\n",
    "    criterion = nn.L1Loss()\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch_chem, batch_time, batch_proc, batch_model, batch_targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_chem, batch_time, batch_proc, batch_model)\n",
    "            loss = criterion(outputs, batch_targets.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(\n",
    "            test_tensors['chemical'],\n",
    "            test_tensors['time'],\n",
    "            test_tensors['process'],\n",
    "            test_tensors['model']\n",
    "        )\n",
    "        test_loss = criterion(y_pred, y_test_tensor.unsqueeze(1)).item()\n",
    "        y_pred_np = y_pred.numpy().flatten()\n",
    "        r2 = r2_score(y_test, y_pred_np)\n",
    "        mae = mean_absolute_error(y_test, y_pred_np)\n",
    "        mse = mean_squared_error(y_test, y_pred_np)\n",
    "        \n",
    "        metrics = {\n",
    "            'r2_score': r2,\n",
    "            'mae': mae,\n",
    "            'mse': mse,\n",
    "            'test_loss': test_loss\n",
    "        }\n",
    "        print(f\"Evaluation - Test Loss: {test_loss:.4f}, R2: {r2:.4f}\")\n",
    "    \n",
    "    return model, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.01, 1e-3],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'hidden_units': [64, 128, 256],\n",
    "    'dropout_rate': [0, 0.2]\n",
    "}\n",
    "grid = ParameterGrid(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:   0%|          | 0/54 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating hyperparameters: {'batch_size': 16, 'dropout_rate': 0, 'hidden_units': 64, 'learning_rate': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:   0%|          | 0/54 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m params \u001b[38;5;129;01min\u001b[39;00m tqdm(grid, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGrid Search Progress\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating hyperparameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m     model, metrics \u001b[38;5;241m=\u001b[39m train_model_regular(train_scaled_df, features_dict, num_epochs, params)\n\u001b[1;32m      9\u001b[0m     mae \u001b[38;5;241m=\u001b[39m metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m best_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m mae \u001b[38;5;241m<\u001b[39m best_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "Cell \u001b[0;32mIn[29], line 93\u001b[0m, in \u001b[0;36mtrain_model_regular\u001b[0;34m(df, features_dict, num_epochs, hyperparameters, use_l2)\u001b[0m\n\u001b[1;32m     91\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_targets\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     92\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 93\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     94\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m             )\n\u001b[0;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/optim/adamw.py:188\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    175\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    177\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    178\u001b[0m         group,\n\u001b[1;32m    179\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    185\u001b[0m         state_steps,\n\u001b[1;32m    186\u001b[0m     )\n\u001b[0;32m--> 188\u001b[0m     adamw(\n\u001b[1;32m    189\u001b[0m         params_with_grad,\n\u001b[1;32m    190\u001b[0m         grads,\n\u001b[1;32m    191\u001b[0m         exp_avgs,\n\u001b[1;32m    192\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    193\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    194\u001b[0m         state_steps,\n\u001b[1;32m    195\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[1;32m    196\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    197\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    198\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    199\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    200\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    201\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    202\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    203\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    204\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    205\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    206\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    207\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    208\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    209\u001b[0m     )\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/optim/adamw.py:340\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 340\u001b[0m func(\n\u001b[1;32m    341\u001b[0m     params,\n\u001b[1;32m    342\u001b[0m     grads,\n\u001b[1;32m    343\u001b[0m     exp_avgs,\n\u001b[1;32m    344\u001b[0m     exp_avg_sqs,\n\u001b[1;32m    345\u001b[0m     max_exp_avg_sqs,\n\u001b[1;32m    346\u001b[0m     state_steps,\n\u001b[1;32m    347\u001b[0m     amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[1;32m    348\u001b[0m     beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    349\u001b[0m     beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    350\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m    351\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[1;32m    352\u001b[0m     eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m    353\u001b[0m     maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[1;32m    354\u001b[0m     capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[1;32m    355\u001b[0m     differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[1;32m    356\u001b[0m     grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[1;32m    357\u001b[0m     found_inf\u001b[38;5;241m=\u001b[39mfound_inf,\n\u001b[1;32m    358\u001b[0m     has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    359\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/optim/adamw.py:420\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    419\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mlerp_(grad, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[0;32m--> 420\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[1;32m    423\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "best_params = None\n",
    "best_results = {'mae': float('inf')}\n",
    "\n",
    "for params in tqdm(grid, desc=\"Grid Search Progress\", leave=True):\n",
    "    print(f\"Evaluating hyperparameters: {params}\")\n",
    "    \n",
    "    model, metrics = train_model_regular(train_scaled_df, features_dict, num_epochs, params)\n",
    "    mae = metrics['mae']\n",
    "    \n",
    "    if best_params is None or mae < best_results['mae']:\n",
    "        best_results = {\n",
    "            'mae': mae,\n",
    "            'metrics': metrics\n",
    "        }\n",
    "        best_params = params\n",
    "\n",
    "print(f\"Best parameters found: {best_params}\")\n",
    "print(f\"Best MAE: {best_results['mae']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - MAE: 1.5456, MSE: 2.5139, RMSE: 1.5855, R2: -6.5826\n",
      "Fold 2 - MAE: 1.5541, MSE: 2.5343, RMSE: 1.5919, R2: -6.5082\n",
      "Fold 3 - MAE: 1.5625, MSE: 2.5602, RMSE: 1.6001, R2: -6.5794\n",
      "Fold 4 - MAE: 1.5420, MSE: 2.5037, RMSE: 1.5823, R2: -6.3996\n",
      "Fold 5 - MAE: 1.5491, MSE: 2.5186, RMSE: 1.5870, R2: -6.4250\n",
      "\n",
      "Average Metrics across folds:\n",
      "MAE: 1.5506 ± 0.0071\n",
      "MSE: 2.5261 ± 0.0197\n",
      "RMSE: 1.5894 ± 0.0062\n",
      "R2: -6.4990 ± 0.0760\n"
     ]
    }
   ],
   "source": [
    "# # Perform cross validation\n",
    "# metrics = load_and_evaluate_model(\n",
    "#     data_df=train_scaled_df,\n",
    "#     features_dict=features_dict,\n",
    "#     model_path='branchmlp.pth',\n",
    "#     n_splits=5,\n",
    "#     batch_size=32\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
