{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from xgboost import XGBRegressor\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import process_steel_data\n",
    "\n",
    "full_path = 'data/'\n",
    "path = 'data/MDC_Data_Descriptions_MeCoMeP-r-value.xlsx'\n",
    "correlation_rate = 0.2\n",
    "dvl_line = 1\n",
    "\n",
    "df = process_steel_data(full_path, path, correlation_rate, dvl_line, model_output=True)\n",
    "df = pd.get_dummies(df, columns=['steel_family'], prefix='steel').drop(['steel_grade'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(df, binary_prefix='steel_'):\n",
    "\n",
    "    # Identify binary columns\n",
    "    binary_columns = [col for col in df.columns if col.startswith(binary_prefix)]\n",
    "    \n",
    "    # Identify columns to scale (non-binary columns)\n",
    "    columns_to_scale = [col for col in df.columns if col not in binary_columns + ['r_value']]\n",
    "    \n",
    "    # Scale numerical features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(df[columns_to_scale])\n",
    "    \n",
    "    # Create new dataframe with scaled data\n",
    "    scaled_df = pd.DataFrame(scaled_data, columns=columns_to_scale)\n",
    "    \n",
    "    # Add back binary columns\n",
    "    for col in binary_columns:\n",
    "        scaled_df[col] = df[col].values\n",
    "    \n",
    "    # Add target variable if present\n",
    "    if 'r_value' in df.columns:\n",
    "        scaled_df['r_value'] = df['r_value'].values\n",
    "    \n",
    "    return scaled_df, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_scaled_df, scaler = scale_data(train_df)\n",
    "binary_columns = [col for col in test_df.columns if col.startswith('steel_')]\n",
    "columns_to_scale = [col for col in test_df.columns if col not in binary_columns + ['r_value']]\n",
    "scaled_test_data = scaler.transform(test_df[columns_to_scale])\n",
    "test_scaled_df = pd.DataFrame(scaled_test_data, columns=columns_to_scale)\n",
    "for col in binary_columns:\n",
    "    test_scaled_df[col] = test_df[col].values\n",
    "if 'r_value' in test_df.columns:\n",
    "    test_scaled_df['r_value'] = test_df['r_value'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_cols = train_scaled_df.select_dtypes(include='bool').columns\n",
    "\n",
    "filtered_dfs_train = {col: train_scaled_df[train_scaled_df[col]].drop(columns=bool_cols) for col in bool_cols}\n",
    "filtered_dfs_test = {col: test_scaled_df[test_scaled_df[col]].drop(columns=bool_cols) for col in bool_cols}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_cv_gridsearch(df, model, param_grid=None, n_splits=5, random_state=42, use_grid_search=True, model_params=None):\n",
    "    \"\"\"\n",
    "    Train a model with optional grid search and cross-validation\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Input dataframe\n",
    "    model : estimator object\n",
    "        Machine learning model to train\n",
    "    param_grid : dict, optional\n",
    "        Parameter grid for grid search (used if use_grid_search=True)\n",
    "    n_splits : int, optional\n",
    "        Number of cross-validation splits (default: 5)\n",
    "    random_state : int, optional\n",
    "        Random state for reproducibility (default: 42)\n",
    "    use_grid_search : bool, optional\n",
    "        Whether to perform grid search (default: True)\n",
    "    model_params : dict, optional\n",
    "        Direct model parameters to use if use_grid_search=False\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict containing model results and performance metrics including tol90\n",
    "    \"\"\"\n",
    "    # Prepare X and y\n",
    "    X = df.drop(['r_value'], axis=1)\n",
    "    y = df['r_value']\n",
    "    \n",
    "    # Initialize cross-validation\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # Initialize metrics storage\n",
    "    cv_scores = {\n",
    "        'mae': [],\n",
    "        'mse': [],\n",
    "        'r2': [],\n",
    "        'tol90': []  # Add tol90 metric\n",
    "    }\n",
    "    \n",
    "    # Determine model parameters\n",
    "    if use_grid_search:\n",
    "        if param_grid is None:\n",
    "            raise ValueError(\"param_grid must be provided when use_grid_search is True\")\n",
    "        \n",
    "        # Initialize GridSearchCV\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=model,\n",
    "            param_grid=param_grid,\n",
    "            cv=n_splits,\n",
    "            scoring='neg_mean_absolute_error',\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Fit GridSearchCV\n",
    "        print(\"Performing GridSearch...\")\n",
    "        grid_search.fit(X, y)\n",
    "        print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
    "        best_model = grid_search.best_estimator_\n",
    "    else:\n",
    "        # Use directly specified parameters or default model\n",
    "        if model_params:\n",
    "            best_model = type(model)(**model_params)\n",
    "        else:\n",
    "            best_model = model\n",
    "        \n",
    "        grid_search = None\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    print(\"\\nPerforming cross-validation...\")\n",
    "    pbar = tqdm(enumerate(kf.split(X), 1),\n",
    "                total=n_splits,\n",
    "                desc=\"Cross-validation\",\n",
    "                leave=True)\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in pbar:\n",
    "        # Split data\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        # Train model\n",
    "        best_model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = best_model.predict(X_val)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "        mse = mean_squared_error(y_val, y_pred)\n",
    "        r2 = r2_score(y_val, y_pred)\n",
    "        \n",
    "        # Calculate tol90 (90th percentile of absolute errors)\n",
    "        abs_errors = np.abs(y_val - y_pred)\n",
    "        tol90 = np.percentile(abs_errors, 90)\n",
    "        \n",
    "        cv_scores['mae'].append(mae)\n",
    "        cv_scores['mse'].append(mse)\n",
    "        cv_scores['r2'].append(r2)\n",
    "        cv_scores['tol90'].append(tol90)\n",
    "        \n",
    "        # Update progress bar description\n",
    "        pbar.set_description(\n",
    "            f\"Fold {fold} - MAE: {mae:.4f}, MSE: {mse:.4f}, R2: {r2:.4f}, TOL90: {tol90:.4f}\"\n",
    "        )\n",
    "    \n",
    "    # Prepare results\n",
    "    results = {\n",
    "        'model': best_model,\n",
    "        'best_params': grid_search.best_params_ if use_grid_search else model_params or {},\n",
    "        'avg_mae': np.mean(cv_scores['mae']),\n",
    "        'std_mae': np.std(cv_scores['mae']),\n",
    "        'avg_mse': np.mean(cv_scores['mse']),\n",
    "        'std_mse': np.std(cv_scores['mse']),\n",
    "        'avg_r2': np.mean(cv_scores['r2']),\n",
    "        'std_r2': np.std(cv_scores['r2']),\n",
    "        'avg_tol90': np.mean(cv_scores['tol90']),  # Add average tol90\n",
    "        'std_tol90': np.std(cv_scores['tol90']),   # Add std of tol90\n",
    "        'cv_scores': cv_scores,\n",
    "        'grid_search_results': grid_search.cv_results_ if use_grid_search else None\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def report_cv_results(results_dict, test_dfs):\n",
    "    \"\"\"\n",
    "    Report cross-validation results and evaluate model performance on test datasets.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    results_dict : dict\n",
    "        Dictionary containing model training results for different steel families.\n",
    "    test_dfs : dict\n",
    "        Dictionary containing test datasets for each steel family.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "    for steel_family, results in results_dict.items():\n",
    "        print(f\"\\nCross-Validation and Test Results for {steel_family}:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Best Parameters: {results['best_params']}\")\n",
    "        print(f\"Average MAE (CV): {results['avg_mae']:.4f} Â± {results['std_mae']:.4f}\")\n",
    "        print(f\"Average MSE (CV): {results['avg_mse']:.4f} Â± {results['std_mse']:.4f}\")\n",
    "        print(f\"Average R2 (CV): {results['avg_r2']:.4f} Â± {results['std_r2']:.4f}\")\n",
    "        print(f\"Average TOL90 (CV): {results['avg_tol90']:.4f} Â± {results['std_tol90']:.4f}\")\n",
    "\n",
    "        # Ensure test data exists for this steel family\n",
    "        if steel_family not in test_dfs:\n",
    "            print(f\"Warning: No test data found for {steel_family}. Skipping test evaluation.\")\n",
    "            continue\n",
    "\n",
    "        test_df = test_dfs[steel_family]\n",
    "        \n",
    "        # Prepare X_test and y_test\n",
    "        X_test = test_df.drop(['r_value'], axis=1)\n",
    "        y_test = test_df['r_value']\n",
    "\n",
    "        # Make predictions on test set\n",
    "        y_pred_test = results['model'].predict(X_test)\n",
    "\n",
    "        # Compute test set performance metrics\n",
    "        test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "        test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "        test_r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "        # Compute tol90 for test set\n",
    "        abs_errors_test = np.abs(y_test - y_pred_test)\n",
    "        test_tol90 = np.percentile(abs_errors_test, 90)\n",
    "\n",
    "        print(f\"Test MAE: {test_mae:.4f}\")\n",
    "        print(f\"Test MSE: {test_mse:.4f}\")\n",
    "        print(f\"Test R2: {test_r2:.4f}\")\n",
    "        print(f\"Test TOL90: {test_tol90:.4f}\")\n",
    "        print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor(random_state=42)\n",
    "rfr_param_grid = {\n",
    "    'n_estimators': [350]\n",
    "}\n",
    "\n",
    "rfr_results_dict = {}\n",
    "\n",
    "for steel_family in filtered_dfs_train.keys():\n",
    "    print(f\"Training model for steel family: {steel_family}\")\n",
    "    rfr_results_dict[steel_family] = train_model_with_cv_gridsearch(\n",
    "        df=filtered_dfs_train[steel_family].select_dtypes(exclude='bool'),\n",
    "        model=rfr,\n",
    "        param_grid=rfr_param_grid,\n",
    "        n_splits=5\n",
    "    )\n",
    "report_cv_results(rfr_results_dict, filtered_dfs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBRegressor(random_state=42)\n",
    "\n",
    "xgb_param_grid = {\n",
    "    'eta': [0.01, 0.05, 0.1, 0.2, 0.3, 0.4],\n",
    "    'lambda': [0, 0.01, 0.1, 1, 10, 50],\n",
    "    'max_depth': [3, 4, 5, 6, 7, 8]\n",
    "}\n",
    "\n",
    "xgb_results_dict = {}\n",
    "\n",
    "for steel_family in filtered_dfs_train.keys():\n",
    "    print(f\"Training model for steel family: {steel_family}\")\n",
    "    xgb_results_dict[steel_family] = train_model_with_cv_gridsearch(\n",
    "        df=filtered_dfs_train[steel_family],\n",
    "        model=xgb_model,\n",
    "        param_grid=xgb_param_grid,\n",
    "        n_splits=5\n",
    "    )\n",
    "report_cv_results(xgb_results_dict, filtered_dfs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiBranchSteelRegressor(nn.Module):\n",
    "    def __init__(self, chemical_dim, time_dim, process_dim, model_dim, hidden_units=64, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        # Track which branches are active\n",
    "        self.has_chemical = chemical_dim > 0\n",
    "        self.has_time = time_dim > 0\n",
    "        self.has_process = process_dim > 0\n",
    "        self.has_model = model_dim > 0\n",
    "        \n",
    "        # Count active branches\n",
    "        self.active_branches = sum([self.has_chemical, self.has_time, self.has_process, self.has_model])\n",
    "        \n",
    "        # Adjust hidden units for each branch\n",
    "        self.branch_hidden = min(hidden_units, max(16, hidden_units // 2))\n",
    "        \n",
    "        # Creating branch\n",
    "        def create_branch(input_dim):\n",
    "            return nn.Sequential(\n",
    "                nn.Linear(input_dim, self.branch_hidden),\n",
    "                nn.BatchNorm1d(self.branch_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            )\n",
    "        \n",
    "        # Only create branches that have features\n",
    "        if self.has_chemical:\n",
    "            self.chemical_branch = create_branch(chemical_dim)\n",
    "        if self.has_time:\n",
    "            self.time_branch = create_branch(time_dim)\n",
    "        if self.has_process:\n",
    "            self.process_branch = create_branch(process_dim)\n",
    "        if self.has_model:\n",
    "            self.model_branch = create_branch(model_dim)\n",
    "        \n",
    "        # Combined input dimension based on active branches only\n",
    "        combined_dim = self.branch_hidden * self.active_branches\n",
    "        \n",
    "        # Final layers after concatenation\n",
    "        self.final_layers = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, chemical, time, process, model):\n",
    "        features = []\n",
    "        # Only process branches that have features\n",
    "        if self.has_chemical:\n",
    "            if chemical.dim() == 1:\n",
    "                chemical = chemical.unsqueeze(0)\n",
    "            features.append(self.chemical_branch(chemical))\n",
    "        \n",
    "        if self.has_time:\n",
    "            if time.dim() == 1:\n",
    "                time = time.unsqueeze(0)\n",
    "            features.append(self.time_branch(time))\n",
    "        \n",
    "        if self.has_process:\n",
    "            if process.dim() == 1:\n",
    "                process = process.unsqueeze(0)\n",
    "            features.append(self.process_branch(process))\n",
    "        \n",
    "        if self.has_model:\n",
    "            if model.dim() == 1:\n",
    "                model = model.unsqueeze(0)\n",
    "            features.append(self.model_branch(model))\n",
    "        \n",
    "        # Concatenate only active features\n",
    "        combined = torch.cat(features, dim=1) if len(features) > 1 else features[0]\n",
    "        return self.final_layers(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labeling the features for each branch\n",
    "features = [col for col in df.columns if col not in ['r_value', 'steel_family', 'steel_grade']]\n",
    "features_dict = {\n",
    "   'time': [col for col in features if 'time' in col.lower()], \n",
    "   'chemical': ['pct_al', 'pct_b', 'pct_c', 'pct_cr', 'pct_mn', 'pct_n', 'pct_nb', 'pct_si', 'pct_ti', 'pct_v', 'mfia_coil_frac_fer', 'mfia_et1_frac_fer', 'mfia_et2_frac_fer'],\n",
    "   'model': [\"rm\", \"ag\", \"a80\", \"n_value\"]\n",
    "}\n",
    "features_dict['process'] = [col for col in features if col not in features_dict['time'] and col not in features_dict['chemical']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "def train_and_evaluate_models_gridsearch(filtered_dfs_train, filtered_dfs_test, features_dict, num_epochs, param_grid, use_l2=False):\n",
    "    \"\"\"\n",
    "    Train and evaluate models for multiple steel families using filtered training and test datasets with Grid Search.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    filtered_dfs_train : dict\n",
    "        Dictionary containing training datasets for each steel family.\n",
    "    filtered_dfs_test : dict\n",
    "        Dictionary containing test datasets for each steel family.\n",
    "    features_dict : dict\n",
    "        Dictionary defining feature categories (chemical, time, process, model).\n",
    "    num_epochs : int\n",
    "        Number of epochs for training.\n",
    "    param_grid : dict\n",
    "        Dictionary of hyperparameter values for grid search.\n",
    "    use_l2 : bool, optional\n",
    "        Whether to use L2 regularization (default: False).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    results_dict : dict\n",
    "        Dictionary with best trained models and evaluation metrics for each steel family.\n",
    "    \"\"\"\n",
    "    \n",
    "    results_dict = {}\n",
    "    grid = list(ParameterGrid(param_grid))  # Convert ParameterGrid to list of dicts\n",
    "\n",
    "    for steel_family in filtered_dfs_train.keys():\n",
    "        print(f\"\\nTraining model for steel family: {steel_family}\")\n",
    "\n",
    "        # Get train and test data\n",
    "        train_df = filtered_dfs_train[steel_family]\n",
    "        test_df = filtered_dfs_test.get(steel_family, None)\n",
    "\n",
    "        # Initialize best metrics\n",
    "        best_model = None\n",
    "        best_metrics = None\n",
    "        best_params = None\n",
    "        best_r2 = -float(\"inf\")  # Higher is better for R2 score\n",
    "\n",
    "        for hyperparameters in grid:\n",
    "            print(f\"\\nTrying hyperparameters: {hyperparameters}\")\n",
    "\n",
    "            batch_size = hyperparameters['batch_size']\n",
    "            \n",
    "            # Initialize feature arrays and dimensions\n",
    "            feature_arrays = {}\n",
    "            feature_dims = {}\n",
    "\n",
    "            # Process each feature category\n",
    "            for category in ['chemical', 'time', 'process', 'model']:\n",
    "                available_features = [col for col in features_dict[category] if col in train_df.columns]\n",
    "\n",
    "                if available_features:\n",
    "                    feature_arrays[category] = train_df[available_features].values.astype(np.float32)\n",
    "                    feature_dims[category] = len(available_features)\n",
    "                else:\n",
    "                    feature_arrays[category] = np.zeros((len(train_df), 0), dtype=np.float32)\n",
    "                    feature_dims[category] = 0\n",
    "\n",
    "            # Prepare targets\n",
    "            targets = train_df['r_value'].values\n",
    "\n",
    "            # Split data\n",
    "            split_data = train_test_split(\n",
    "                feature_arrays['chemical'],\n",
    "                feature_arrays['time'],\n",
    "                feature_arrays['process'],\n",
    "                feature_arrays['model'],\n",
    "                targets,\n",
    "                test_size=0.2,\n",
    "                random_state=42\n",
    "            )\n",
    "\n",
    "            (X_train_chem, X_test_chem, X_train_time, X_test_time, \n",
    "             X_train_proc, X_test_proc, X_train_model, X_test_model, \n",
    "             y_train, y_test) = split_data\n",
    "\n",
    "            # Convert to tensors\n",
    "            train_tensors = {\n",
    "                'chemical': torch.FloatTensor(X_train_chem),\n",
    "                'time': torch.FloatTensor(X_train_time),\n",
    "                'process': torch.FloatTensor(X_train_proc),\n",
    "                'model': torch.FloatTensor(X_train_model)\n",
    "            }\n",
    "\n",
    "            test_tensors = {\n",
    "                'chemical': torch.FloatTensor(X_test_chem),\n",
    "                'time': torch.FloatTensor(X_test_time),\n",
    "                'process': torch.FloatTensor(X_test_proc),\n",
    "                'model': torch.FloatTensor(X_test_model)\n",
    "            }\n",
    "\n",
    "            y_train_tensor = torch.FloatTensor(y_train)\n",
    "            y_test_tensor = torch.FloatTensor(y_test)\n",
    "\n",
    "            # Create DataLoader\n",
    "            train_dataset = TensorDataset(\n",
    "                train_tensors['chemical'],\n",
    "                train_tensors['time'],\n",
    "                train_tensors['process'],\n",
    "                train_tensors['model'],\n",
    "                y_train_tensor\n",
    "            )\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "            # Initialize model\n",
    "            model = MultiBranchSteelRegressor(\n",
    "                chemical_dim=feature_dims['chemical'],\n",
    "                time_dim=feature_dims['time'],\n",
    "                process_dim=feature_dims['process'],\n",
    "                model_dim=feature_dims['model'],\n",
    "                hidden_units=hyperparameters['hidden_units'],\n",
    "                dropout_rate=hyperparameters['dropout_rate']\n",
    "            )\n",
    "\n",
    "            if use_l2:\n",
    "                weight_decay = 0.001\n",
    "            else:\n",
    "                weight_decay = 0.0\n",
    "\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=hyperparameters['learning_rate'], weight_decay=weight_decay)\n",
    "            criterion = nn.L1Loss()\n",
    "\n",
    "            # Training loop\n",
    "            model.train()\n",
    "            for epoch in range(num_epochs):\n",
    "                running_loss = 0.0\n",
    "                for batch_chem, batch_time, batch_proc, batch_model, batch_targets in train_loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(batch_chem, batch_time, batch_proc, batch_model)\n",
    "                    loss = criterion(outputs, batch_targets.unsqueeze(1))\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    running_loss += loss.item()\n",
    "\n",
    "                if (epoch + 1) % 10 == 0:\n",
    "                    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "            # Evaluation\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                y_pred = model(\n",
    "                    test_tensors['chemical'],\n",
    "                    test_tensors['time'],\n",
    "                    test_tensors['process'],\n",
    "                    test_tensors['model']\n",
    "                )\n",
    "                test_loss = criterion(y_pred, y_test_tensor.unsqueeze(1)).item()\n",
    "                y_pred_np = y_pred.numpy().flatten()\n",
    "                r2 = r2_score(y_test, y_pred_np)\n",
    "                mae = mean_absolute_error(y_test, y_pred_np)\n",
    "                mse = mean_squared_error(y_test, y_pred_np)\n",
    "\n",
    "                abs_errors = np.abs(y_test - y_pred_np)\n",
    "                tol90 = np.percentile(abs_errors, 90)\n",
    "\n",
    "                metrics = {\n",
    "                    'model': model,\n",
    "                    'best_params': hyperparameters,\n",
    "                    'avg_mae': mae,\n",
    "                    'std_mae': 0.0,\n",
    "                    'avg_mse': mse,\n",
    "                    'std_mse': 0.0,\n",
    "                    'avg_r2': r2,\n",
    "                    'std_r2': 0.0,\n",
    "                    'avg_tol90': tol90,\n",
    "                    'std_tol90': 0.0,\n",
    "                    'grid_search_results': None\n",
    "                }\n",
    "\n",
    "                print(f\"\\nEvaluation - {steel_family}: R2: {r2:.4f}, MAE: {mae:.4f}, Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "                # Save best model based on R2 score\n",
    "                if r2 > best_r2:\n",
    "                    best_r2 = r2\n",
    "                    best_model = model\n",
    "                    best_metrics = metrics\n",
    "                    best_params = hyperparameters\n",
    "\n",
    "        # Store best results for this steel family\n",
    "        if best_model:\n",
    "            results_dict[steel_family] = best_metrics\n",
    "\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'learning_rate': [0.01, 1e-3],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'hidden_units': [64, 128, 256],\n",
    "    'dropout_rate': [0, 0.2]\n",
    "}\n",
    "num_epochs = 100\n",
    "\n",
    "results_dict = train_and_evaluate_models_gridsearch(filtered_dfs_train, filtered_dfs_test, features_dict, num_epochs, param_grid)\n",
    "\n",
    "report_cv_results(results_dict, filtered_dfs_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
